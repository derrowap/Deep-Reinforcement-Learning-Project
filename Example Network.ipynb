{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "from IPython import display\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class gameOb():\n",
    "    def __init__(self,coordinates,size,intensity,channel,reward,name):\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        self.reward = reward\n",
    "        self.name = name\n",
    "        \n",
    "class gameEnv():\n",
    "    def __init__(self,partial,size):\n",
    "        self.sizeX = size\n",
    "        self.sizeY = size\n",
    "        self.actions = 4\n",
    "        self.objects = []\n",
    "        self.partial = partial\n",
    "        self.a = self.reset()\n",
    "#         plt.imshow(self.a,interpolation=\"nearest\")\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.objects = []\n",
    "        hero = gameOb(self.newPosition(),1,1,2,None,'hero')\n",
    "        self.objects.append(hero)\n",
    "        bug = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(bug)\n",
    "        hole = gameOb(self.newPosition(),1,1,0,0,'fire')\n",
    "        self.objects.append(hole)\n",
    "        bug2 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(bug2)\n",
    "        hole2 = gameOb(self.newPosition(),1,1,0,0,'fire')\n",
    "        self.objects.append(hole2)\n",
    "        bug3 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(bug3)\n",
    "        bug4 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(bug4)\n",
    "        state = self.renderEnv()\n",
    "        self.state = state\n",
    "        return state\n",
    "\n",
    "    def moveChar(self,direction):\n",
    "        # 0 - up, 1 - down, 2 - left, 3 - right\n",
    "        hero = self.objects[0]\n",
    "        heroX = hero.x\n",
    "        heroY = hero.y\n",
    "        penalize = 0.00\n",
    "        if direction == 0 and hero.y >= 1:\n",
    "            hero.y -= 1\n",
    "        if direction == 1 and hero.y <= self.sizeY-2:\n",
    "            hero.y += 1\n",
    "        if direction == 2 and hero.x >= 1:\n",
    "            hero.x -= 1\n",
    "        if direction == 3 and hero.x <= self.sizeX-2:\n",
    "            hero.x += 1     \n",
    "        if hero.x == heroX and hero.y == heroY:\n",
    "            penalize = 0\n",
    "        self.objects[0] = hero\n",
    "        return penalize\n",
    "    \n",
    "    def newPosition(self):\n",
    "        iterables = [ range(self.sizeX), range(self.sizeY)]\n",
    "        points = []\n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        currentPositions = []\n",
    "        for objectA in self.objects:\n",
    "            if (objectA.x,objectA.y) not in currentPositions:\n",
    "                currentPositions.append((objectA.x,objectA.y))\n",
    "        for pos in currentPositions:\n",
    "            points.remove(pos)\n",
    "        location = np.random.choice(range(len(points)),replace=False)\n",
    "        return points[location]\n",
    "\n",
    "    def checkGoal(self):\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'hero':\n",
    "                hero = obj\n",
    "            else:\n",
    "                others.append(obj)\n",
    "        ended = False\n",
    "        for other in others:\n",
    "            if hero.x == other.x and hero.y == other.y:\n",
    "                self.objects.remove(other)\n",
    "                if other.reward == 1:\n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,1,1,'goal'))\n",
    "                else: \n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,0,0,'fire'))\n",
    "                return other.reward,True\n",
    "        if ended == False:\n",
    "            return 0.0,False\n",
    "\n",
    "    def renderEnv(self):\n",
    "        #a = np.zeros([self.sizeY,self.sizeX,3])\n",
    "        self.a = np.ones([self.sizeY+2,self.sizeX+2,3])\n",
    "        self.a[1:-1,1:-1,:] = 0\n",
    "        hero = None\n",
    "        for item in self.objects:\n",
    "            self.a[item.y+1:item.y+item.size+1,item.x+1:item.x+item.size+1,item.channel] = item.intensity\n",
    "            if item.name == 'hero':\n",
    "                hero = item\n",
    "        if self.partial == True:\n",
    "            self.a = self.a[hero.y:hero.y+3,hero.x:hero.x+3,:]\n",
    "        b = scipy.misc.imresize(self.a[:,:,0],[84,84,1],interp='nearest')\n",
    "        c = scipy.misc.imresize(self.a[:,:,1],[84,84,1],interp='nearest')\n",
    "        d = scipy.misc.imresize(self.a[:,:,2],[84,84,1],interp='nearest')\n",
    "        self.a = np.stack([b,c,d],axis=2)\n",
    "        return self.a\n",
    "\n",
    "    def step(self,action):\n",
    "        penalty = self.moveChar(action)\n",
    "        reward,done = self.checkGoal()\n",
    "        state = self.renderEnv()\n",
    "        return state,(reward-penalty),done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gameEnv(partial=False,size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        self.conv1 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv3,num_outputs=512,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        #We take the output from the final convolutional layer and split it into separate advantage and value streams.\n",
    "        self.streamAC,self.streamVC = tf.split(3,2,self.conv4)\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2,env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.sub(self.Advantage,tf.reduce_mean(self.Advantage,reduction_indices=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.mul(self.Qout, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[21168])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    print(type(total_vars))\n",
    "    op_holder = []\n",
    "    print(\"tfVars length: \", total_vars)\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "anneling_steps = 50000 #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 30003 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
    "pre_train_steps_from_Q = False #If true, initialize buffer with steps from Q instead of random actions\n",
    "max_epLength = 100 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./dqn/awjuliani/one_reward_extended\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "tfVars length:  12\n",
      "100 768 0.64 7.68\n",
      "200 1584 0.64 8.16\n",
      "300 2358 0.65 7.74\n",
      "400 3168 0.61 8.1\n",
      "500 3899 0.72 7.31\n",
      "600 4698 0.72 7.99\n",
      "700 5338 0.72 6.4\n",
      "800 6028 0.67 6.9\n",
      "900 6761 0.69 7.33\n",
      "\n",
      "last 1000 -- avg steps  7.561  -- %complete  0.67\n",
      "\n",
      "1000 7561 0.64 8.0\n",
      "1100 8263 0.69 7.02\n",
      "1200 9075 0.7 8.12\n",
      "1300 10237 0.62 11.62\n",
      "1400 12276 0.5 20.39\n",
      "1500 14105 0.53 18.29\n",
      "1600 15486 0.59 13.81\n",
      "1700 16852 0.48 13.66\n",
      "1800 18149 0.63 12.97\n",
      "1900 19273 0.57 11.24\n",
      "\n",
      "last 1000 -- avg steps  12.828  -- %complete  0.591\n",
      "\n",
      "2000 20389 0.6 11.16\n",
      "2100 21501 0.53 11.12\n",
      "2200 22741 0.5 12.4\n",
      "2300 24022 0.64 12.81\n",
      "2400 24966 0.58 9.44\n",
      "2500 26301 0.56 13.35\n",
      "2600 27538 0.65 12.37\n",
      "2700 28818 0.62 12.8\n",
      "2800 30072 0.61 12.54\n",
      "2900 31351 0.62 12.79\n",
      "\n",
      "last 1000 -- avg steps  12.303  -- %complete  0.596\n",
      "\n",
      "3000 32692 0.65 13.41\n",
      "3100 34405 0.59 17.13\n",
      "3200 35748 0.68 13.43\n",
      "3300 37530 0.53 17.82\n",
      "3400 39196 0.7 16.66\n",
      "3500 41101 0.58 19.05\n",
      "3600 43439 0.51 23.38\n",
      "3700 45682 0.62 22.43\n",
      "3800 47797 0.62 21.15\n",
      "3900 50151 0.57 23.54\n",
      "\n",
      "last 1000 -- avg steps  20.474  -- %complete  0.584\n",
      "\n",
      "4000 53166 0.44 30.15\n",
      "4100 55969 0.47 28.03\n",
      "4200 58487 0.38 25.18\n",
      "4300 61222 0.44 27.35\n",
      "4400 63978 0.42 27.56\n",
      "4500 66977 0.39 29.99\n",
      "4600 69521 0.5 25.44\n",
      "4700 72328 0.51 28.07\n",
      "4800 75528 0.36 32.0\n",
      "4900 78383 0.42 28.55\n",
      "\n",
      "last 1000 -- avg steps  28.359  -- %complete  0.433\n",
      "\n",
      "5000 81525 0.44 31.42\n",
      "5100 84509 0.44 29.84\n",
      "5200 87888 0.4 33.79\n",
      "5300 91162 0.32 32.74\n",
      "5400 94547 0.36 33.85\n",
      "5500 97606 0.41 30.59\n",
      "5600 100865 0.33 32.59\n",
      "5700 103542 0.44 26.77\n",
      "5800 106772 0.37 32.3\n",
      "5900 109719 0.36 29.47\n",
      "\n",
      "last 1000 -- avg steps  31.266  -- %complete  0.383\n",
      "\n",
      "6000 112791 0.4 30.72\n",
      "6100 116463 0.28 36.72\n",
      "6200 119750 0.36 32.87\n",
      "6300 122951 0.39 32.01\n",
      "6400 126164 0.37 32.13\n",
      "6500 129329 0.35 31.65\n",
      "6600 132775 0.24 34.46\n",
      "6700 135987 0.31 32.12\n",
      "6800 139413 0.43 34.26\n",
      "6900 142432 0.42 30.19\n",
      "\n",
      "last 1000 -- avg steps  32.907  -- %complete  0.354\n",
      "\n",
      "7000 145698 0.39 32.66\n",
      "7100 149179 0.4 34.81\n",
      "7200 152555 0.4 33.76\n",
      "7300 155735 0.37 31.8\n",
      "7400 158948 0.42 32.13\n",
      "7500 162771 0.26 38.23\n",
      "7600 166428 0.33 36.57\n",
      "7700 169828 0.39 34.0\n",
      "7800 173285 0.34 34.57\n",
      "7900 176390 0.38 31.05\n",
      "\n",
      "last 1000 -- avg steps  34.096  -- %complete  0.364\n",
      "\n",
      "8000 179794 0.35 34.04\n",
      "8100 183131 0.32 33.37\n",
      "8200 186590 0.32 34.59\n",
      "8300 190071 0.33 34.81\n",
      "8400 193324 0.48 32.53\n",
      "8500 196503 0.35 31.79\n",
      "8600 200051 0.32 35.48\n",
      "8700 203350 0.36 32.99\n",
      "8800 206798 0.37 34.48\n",
      "8900 210059 0.35 32.61\n",
      "\n",
      "last 1000 -- avg steps  33.392  -- %complete  0.353\n",
      "\n",
      "9000 213186 0.33 31.27\n",
      "9100 216702 0.34 35.16\n",
      "9200 220167 0.31 34.65\n",
      "9300 223889 0.35 37.22\n",
      "9400 227416 0.31 35.27\n",
      "9500 230811 0.4 33.95\n",
      "9600 234144 0.33 33.33\n",
      "9700 237870 0.3 37.26\n",
      "9800 241324 0.4 34.54\n",
      "9900 244722 0.31 33.98\n",
      "Saved Model\n",
      "\n",
      "last 1000 -- avg steps  34.814  -- %complete  0.346\n",
      "\n",
      "10000 248000 0.41 32.78\n",
      "10100 250857 0.51 28.57\n",
      "10200 254388 0.32 35.31\n",
      "10300 257651 0.34 32.63\n",
      "10400 261164 0.38 35.13\n",
      "10500 264327 0.38 31.63\n",
      "10600 267747 0.36 34.2\n",
      "10700 271063 0.39 33.16\n",
      "10800 274360 0.4 32.97\n",
      "10900 277629 0.36 32.69\n",
      "\n",
      "last 1000 -- avg steps  32.831  -- %complete  0.376\n",
      "\n",
      "11000 280831 0.32 32.02\n",
      "11100 284145 0.42 33.14\n",
      "11200 287128 0.33 29.83\n",
      "11300 290557 0.37 34.29\n",
      "11400 294094 0.34 35.37\n",
      "11500 297229 0.36 31.35\n",
      "11600 300593 0.33 33.64\n",
      "11700 304030 0.41 34.37\n",
      "11800 307397 0.3 33.67\n",
      "11900 310738 0.38 33.41\n",
      "\n",
      "last 1000 -- avg steps  33.166  -- %complete  0.365\n",
      "\n",
      "12000 313997 0.41 32.59\n",
      "12100 316919 0.39 29.22\n",
      "12200 320210 0.35 32.91\n",
      "12300 323123 0.39 29.13\n",
      "12400 326508 0.36 33.85\n",
      "12500 329732 0.41 32.24\n",
      "12600 332950 0.45 32.18\n",
      "12700 335727 0.51 27.77\n",
      "12800 338973 0.26 32.46\n",
      "12900 342444 0.32 34.71\n",
      "\n",
      "last 1000 -- avg steps  32.068  -- %complete  0.373\n",
      "\n",
      "13000 346065 0.29 36.21\n",
      "13100 349369 0.44 33.04\n",
      "13200 353074 0.34 37.05\n",
      "13300 356227 0.38 31.53\n",
      "13400 359685 0.34 34.58\n",
      "13500 362893 0.35 32.08\n",
      "13600 366097 0.42 32.04\n",
      "13700 369140 0.44 30.43\n",
      "13800 372590 0.29 34.5\n",
      "13900 375566 0.45 29.76\n",
      "\n",
      "last 1000 -- avg steps  32.812  -- %complete  0.383\n",
      "\n",
      "14000 378877 0.38 33.11\n",
      "14100 381913 0.44 30.36\n",
      "14200 385020 0.35 31.07\n",
      "14300 388015 0.38 29.95\n",
      "14400 391182 0.37 31.67\n",
      "14500 394219 0.4 30.37\n",
      "14600 397672 0.35 34.53\n",
      "14700 400762 0.42 30.9\n",
      "14800 404070 0.36 33.08\n",
      "14900 407097 0.49 30.27\n",
      "\n",
      "last 1000 -- avg steps  31.011  -- %complete  0.405\n",
      "\n",
      "15000 409888 0.49 27.91\n",
      "15100 413023 0.36 31.35\n",
      "15200 415992 0.4 29.69\n",
      "15300 419043 0.38 30.51\n",
      "15400 422011 0.4 29.68\n",
      "15500 425374 0.35 33.63\n",
      "15600 428406 0.31 30.32\n",
      "15700 431422 0.31 30.16\n",
      "15800 434745 0.28 33.23\n",
      "15900 437968 0.4 32.23\n",
      "\n",
      "last 1000 -- avg steps  31.006  -- %complete  0.36\n",
      "\n",
      "16000 440894 0.41 29.26\n",
      "16100 444074 0.4 31.8\n",
      "16200 447093 0.33 30.19\n",
      "16300 450053 0.46 29.6\n",
      "16400 452869 0.46 28.16\n",
      "16500 455760 0.42 28.91\n",
      "16600 458653 0.46 28.93\n",
      "16700 461640 0.36 29.87\n",
      "16800 464497 0.47 28.57\n",
      "16900 467665 0.37 31.68\n",
      "\n",
      "last 1000 -- avg steps  29.496  -- %complete  0.414\n",
      "\n",
      "17000 470390 0.41 27.25\n",
      "17100 473200 0.51 28.1\n",
      "17200 476293 0.44 30.93\n",
      "17300 479416 0.38 31.23\n",
      "17400 482414 0.45 29.98\n",
      "17500 485453 0.39 30.39\n",
      "17600 488418 0.45 29.65\n",
      "17700 491247 0.46 28.29\n",
      "17800 494083 0.4 28.36\n",
      "17900 497003 0.37 29.2\n",
      "\n",
      "last 1000 -- avg steps  29.668  -- %complete  0.423\n",
      "\n",
      "18000 500058 0.38 30.55\n",
      "18100 502728 0.45 26.7\n",
      "18200 505612 0.42 28.84\n",
      "18300 508215 0.53 26.03\n",
      "18400 511153 0.43 29.38\n",
      "18500 514013 0.47 28.6\n",
      "18600 517226 0.4 32.13\n",
      "18700 519676 0.49 24.5\n",
      "18800 522391 0.45 27.15\n",
      "18900 524928 0.51 25.37\n",
      "\n",
      "last 1000 -- avg steps  27.79  -- %complete  0.451\n",
      "\n",
      "19000 527848 0.36 29.2\n",
      "19100 530633 0.39 27.85\n",
      "19200 533466 0.48 28.33\n",
      "19300 536045 0.44 25.79\n",
      "19400 538866 0.5 28.21\n",
      "19500 541492 0.52 26.26\n",
      "19600 543836 0.46 23.44\n",
      "19700 546687 0.46 28.51\n",
      "19800 549239 0.5 25.52\n",
      "19900 552064 0.51 28.25\n",
      "Saved Model\n",
      "\n",
      "last 1000 -- avg steps  26.59  -- %complete  0.486\n",
      "\n",
      "20000 554438 0.6 23.74\n",
      "20100 556414 0.58 19.76\n",
      "20200 559091 0.52 26.77\n",
      "20300 561786 0.57 26.95\n",
      "20400 563698 0.57 19.12\n",
      "20500 566348 0.51 26.5\n",
      "20600 568712 0.61 23.64\n",
      "20700 571119 0.49 24.07\n",
      "20800 573492 0.61 23.73\n",
      "20900 575677 0.58 21.85\n",
      "\n",
      "last 1000 -- avg steps  22.992  -- %complete  0.573\n",
      "\n",
      "21000 577430 0.69 17.53\n",
      "21100 579515 0.56 20.85\n",
      "21200 581883 0.6 23.68\n",
      "21300 584054 0.67 21.71\n",
      "21400 586251 0.62 21.97\n",
      "21500 588379 0.63 21.28\n",
      "21600 590722 0.65 23.43\n",
      "21700 592977 0.58 22.55\n",
      "21800 595031 0.65 20.54\n",
      "21900 597251 0.65 22.2\n",
      "\n",
      "last 1000 -- avg steps  21.511  -- %complete  0.632\n",
      "\n",
      "22000 598941 0.71 16.9\n",
      "22100 601111 0.65 21.7\n",
      "22200 602865 0.72 17.54\n",
      "22300 604713 0.7 18.48\n",
      "22400 606437 0.67 17.24\n",
      "22500 608392 0.62 19.55\n",
      "22600 610520 0.69 21.28\n",
      "22700 612376 0.69 18.56\n",
      "22800 614157 0.63 17.81\n",
      "22900 616022 0.63 18.65\n",
      "\n",
      "last 1000 -- avg steps  18.808  -- %complete  0.671\n",
      "\n",
      "23000 617749 0.71 17.27\n",
      "23100 619451 0.75 17.02\n",
      "23200 621343 0.7 18.92\n",
      "23300 623379 0.65 20.36\n",
      "23400 624944 0.72 15.65\n",
      "23500 627223 0.6 22.79\n",
      "23600 629027 0.74 18.04\n",
      "23700 630709 0.76 16.82\n",
      "23800 632374 0.68 16.65\n",
      "23900 633932 0.76 15.58\n",
      "\n",
      "last 1000 -- avg steps  17.867  -- %complete  0.707\n",
      "\n",
      "24000 635616 0.71 16.84\n",
      "24100 636918 0.82 13.02\n",
      "24200 638135 0.79 12.17\n",
      "24300 639822 0.75 16.87\n",
      "24400 641210 0.77 13.88\n",
      "24500 642519 0.79 13.09\n",
      "24600 643914 0.77 13.95\n",
      "24700 645583 0.72 16.69\n",
      "24800 646820 0.75 12.37\n",
      "24900 648289 0.77 14.69\n",
      "\n",
      "last 1000 -- avg steps  13.928  -- %complete  0.777\n",
      "\n",
      "25000 649544 0.84 12.55\n",
      "25100 650980 0.77 14.36\n",
      "25200 652293 0.76 13.13\n",
      "25300 653585 0.76 12.92\n",
      "25400 654981 0.76 13.96\n",
      "25500 656419 0.72 14.38\n",
      "25600 657787 0.76 13.68\n",
      "25700 659133 0.78 13.46\n",
      "25800 660647 0.8 15.14\n",
      "25900 661638 0.85 9.91\n",
      "\n",
      "last 1000 -- avg steps  13.444  -- %complete  0.774\n",
      "\n",
      "26000 662988 0.78 13.5\n",
      "26100 664178 0.82 11.9\n",
      "26200 665211 0.8 10.33\n",
      "26300 666211 0.88 10.0\n",
      "26400 667318 0.82 11.07\n",
      "26500 668423 0.81 11.05\n",
      "26600 669528 0.8 11.05\n",
      "26700 670813 0.8 12.85\n",
      "26800 671710 0.89 8.97\n",
      "26900 672883 0.84 11.73\n",
      "\n",
      "last 1000 -- avg steps  10.897  -- %complete  0.833\n",
      "\n",
      "27000 673885 0.87 10.02\n",
      "27100 675152 0.82 12.67\n",
      "27200 676219 0.84 10.67\n",
      "27300 677327 0.85 11.08\n",
      "27400 678351 0.82 10.24\n",
      "27500 679747 0.76 13.96\n",
      "27600 680869 0.81 11.22\n",
      "27700 681956 0.81 10.87\n",
      "27800 683230 0.77 12.74\n",
      "27900 684251 0.82 10.21\n",
      "\n",
      "last 1000 -- avg steps  11.613  -- %complete  0.813\n",
      "\n",
      "28000 685498 0.83 12.47\n",
      "28100 686840 0.78 13.42\n",
      "28200 687905 0.83 10.65\n",
      "28300 688807 0.87 9.02\n",
      "28400 690235 0.76 14.28\n",
      "28500 691293 0.78 10.58\n",
      "28600 692368 0.84 10.75\n",
      "28700 693384 0.84 10.16\n",
      "28800 694096 0.92 7.12\n",
      "28900 695491 0.77 13.95\n",
      "\n",
      "last 1000 -- avg steps  11.049  -- %complete  0.82\n",
      "\n",
      "29000 696547 0.81 10.56\n",
      "29100 697538 0.82 9.91\n",
      "29200 698335 0.89 7.97\n",
      "29300 699280 0.82 9.45\n",
      "29400 700101 0.9 8.21\n",
      "29500 701244 0.8 11.43\n",
      "29600 702141 0.86 8.97\n",
      "29700 703227 0.82 10.86\n",
      "29800 704347 0.84 11.2\n",
      "29900 705361 0.83 10.14\n",
      "Saved Model\n",
      "\n",
      "last 1000 -- avg steps  9.923  -- %complete  0.843\n",
      "\n",
      "30000 706470 0.85 11.09\n",
      "Percent of succesful episodes: 0.5384307712819145%\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        load = './dqn/awjuliani/one_reward/model-10000.cptk'\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,load)\n",
    "    sess.run(init)\n",
    "    updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "    for i in range(1, num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if pre_train_steps_from_Q:\n",
    "                action = sess.run(mainQN.predict, feed_dict={mainQN.scalarInput:np.reshape(processState(env.a), [1, 21168])})\n",
    "            elif np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            s1,r,d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) #Save the experience to our episode buffer.\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    #Update the network with our target values.\n",
    "                    _ = sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
    "                    \n",
    "                    updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "        \n",
    "        #Get all experiences from this episode and discount their rewards.\n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        #Periodically save the model. \n",
    "        if i % 10000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")\n",
    "        if i % 1000 == 0:\n",
    "            print()\n",
    "            print(\"last 1000 -- avg steps \",np.mean(jList[-1000:]), \" -- %complete \", sum(rList[-1000:])/1000)\n",
    "            print()\n",
    "        if len(rList) % 100 == 0:\n",
    "            print(i, total_steps,np.mean(rList[-100:]),np.mean(jList[-100:]))\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Complete\n",
      "reward:  1.0\n",
      "2\n",
      "Complete\n",
      "reward:  1.0\n",
      "22\n",
      "Complete\n",
      "reward:  0.0\n",
      "22\n",
      "Complete\n",
      "reward:  0.0\n",
      "22\n",
      "Complete\n",
      "reward:  0.0\n",
      "1\n",
      "Complete\n",
      "reward:  0.0\n",
      "3\n",
      "Complete\n",
      "reward:  0.0\n",
      "1\n",
      "Complete\n",
      "reward:  1.0\n",
      "22\n",
      "Complete\n",
      "reward:  0.0\n",
      "3\n",
      "Complete\n",
      "reward:  1.0\n",
      "22\n",
      "Complete\n",
      "reward:  0.0\n",
      "22\n",
      "Complete\n",
      "reward:  0.0\n",
      "22\n",
      "Complete\n",
      "reward:  0.0\n",
      "22\n",
      "Complete\n",
      "reward:  0.0\n",
      "22\n",
      "Complete\n",
      "reward:  0.0\n",
      "22\n",
      "Complete\n",
      "reward:  0.0\n",
      "22\n",
      "Complete\n",
      "reward:  0.0\n",
      "1\n",
      "Complete\n",
      "reward:  1.0\n",
      "5\n",
      "Complete\n",
      "reward:  1.0\n",
      "1\n",
      "Complete\n",
      "reward:  0.0\n"
     ]
    }
   ],
   "source": [
    "directions = ['up', 'down', 'left', 'right']\n",
    "with tf.Session() as sess:\n",
    "    load = './dqn/awjuliani/one_reward/model-10000.cptk'\n",
    "    saver.restore(sess, load)\n",
    "    sess.run(init)\n",
    "    for i in range(20):\n",
    "        complete = False\n",
    "        iter = 0\n",
    "        env = gameEnv(partial=False,size=5)\n",
    "#         plt.imshow(env.a)\n",
    "        time.sleep(2)\n",
    "        r_all = 0\n",
    "        while(complete == False):\n",
    "            action = sess.run(mainQN.predict, feed_dict={mainQN.scalarInput:np.reshape(processState(env.a), [1, 21168])})\n",
    "            observation, reward, done = env.step(action)\n",
    "#             plt.imshow(env.a)\n",
    "#             display.clear_output(wait=True)\n",
    "#             display.display(plt.gcf())\n",
    "            r_all += reward\n",
    "            if(iter > 20):\n",
    "                done = True\n",
    "            iter += 1\n",
    "            if(done):\n",
    "                complete = True\n",
    "                print(iter)\n",
    "                print(\"Complete\")\n",
    "                print(\"reward: \", r_all)\n",
    "#             time.sleep(.5)\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./dqn/awjuliani/one_reward/model-0.cptk'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt.model_checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
