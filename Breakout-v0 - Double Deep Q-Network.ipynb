{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout-v0 - Double Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Breakout-v0 Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-28 14:15:44,910] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self, h_size, frameShape, batch_size):\n",
    "        # The network recieves a frame from the game, flattened into an array.\n",
    "        # It then resizes it and processes it through four convolutional layers.\n",
    "        \n",
    "        # raw pixel data (grayscale, so only 1 channel)\n",
    "        self.rgb_array = tf.placeholder(shape=[None, frameShape[0], frameShape[1], 1], dtype=tf.float32)\n",
    "        \n",
    "        # tf input: a 4-D tensor [batch_size, height, width, channels]\n",
    "        self.imageIn = tf.reshape(self.rgb_array,shape=[-1, frameShape[0], frameShape[1], 1])\n",
    "        \n",
    "        self.conv1 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.imageIn,\n",
    "            num_outputs = 8,\n",
    "            kernel_size = [16, 16],\n",
    "            stride = [8, 8],\n",
    "            padding = 'VALID',\n",
    "            activation_fn = tf.nn.relu)\n",
    "        \n",
    "        self.conv2 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv1,\n",
    "            num_outputs = 32,\n",
    "            kernel_size = [4, 4],\n",
    "            stride = [2, 2],\n",
    "            padding = 'VALID',\n",
    "            activation_fn = tf.nn.relu)\n",
    "        \n",
    "        self.conv3 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv2,\n",
    "            num_outputs = 16,\n",
    "            kernel_size = [3, 3],\n",
    "            stride = [1, 1],\n",
    "            padding = 'VALID',\n",
    "            activation_fn = tf.nn.relu)\n",
    "        \n",
    "        self.conv4 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv3,\n",
    "            num_outputs = 64,\n",
    "            kernel_size = [3, 3],\n",
    "            stride = [1, 1],\n",
    "            padding = 'VALID',\n",
    "            activation_fn = tf.nn.relu)\n",
    "        \n",
    "        # We take the output from the final convolutional layer and split it\n",
    "        # into separate advantage and value streams.\n",
    "        \n",
    "        # TODO: figure out what shape of self.conv4 is\n",
    "        # split on the 3rd dimension into 2 different parts\n",
    "        self.streamAC, self.streamVC = tf.split(3, 2, self.conv4)\n",
    "        \n",
    "        # flatten to [batch_size, k]\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        \n",
    "        # weights for advantage and value stream layer\n",
    "        self.AW = tf.Variable(tf.random_normal([896 , env.action_space.n]))\n",
    "        self.VW = tf.Variable(tf.random_normal([896 , 1]))\n",
    "        \n",
    "        # output of advantage and value layer\n",
    "        self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "        self.Value = tf.matmul(self.streamV, self.VW)\n",
    "        \n",
    "        # Then combine them together to get our final Q-values.\n",
    "        # Q(s, a) = V(s) + A(a)\n",
    "        self.Qout = self.Value + tf.sub(\n",
    "            self.Advantage,\n",
    "            tf.reduce_mean( # TODO: understand how this A(a) is calculated\n",
    "                self.Advantage,\n",
    "                reduction_indices=1,\n",
    "                keep_dims=True))\n",
    "        \n",
    "        # index of max value across 1st dimension\n",
    "        self.predict = tf.argmax(self.Qout, 1)\n",
    "        \n",
    "        # Below we obtain the loss by taking the sum of squares difference between\n",
    "        # the target and prediction Q values.\n",
    "        \n",
    "        # target Q value\n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        \n",
    "        # possible actions\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, env.action_space.n, dtype=tf.float32)\n",
    "        \n",
    "        # predicted Q values\n",
    "        self.Q = tf.reduce_sum(tf.mul(self.Qout, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        # error = sum( (target - actual)^2 ) / batch_size\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        \n",
    "        # define trainer\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        \n",
    "        # define optimizer\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    \"\"\"Used to store experiences and samples randomly to train the network.\"\"\"\n",
    "    def __init__(self, buffer_size=1000):\n",
    "        self.buffer_size = buffer_size\n",
    "        # state, action, reward, state1, done\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.states_ = []\n",
    "        self.dones = []\n",
    "        \n",
    "    def add(self, states, actions, rewards, states_, dones):\n",
    "        if len(self.states) + len(states) >= self.buffer_size:\n",
    "            self.states = self.states[:(self.buffer_size - len(states))]\n",
    "            self.actions = self.actions[:(self.buffer_size - len(actions))]\n",
    "            self.rewards = self.rewards[:(self.buffer_size - len(rewards))]\n",
    "            self.states_ = self.states_[:(self.buffer_size - len(states_))]\n",
    "            self.dones = self.dones[:(self.buffer_size - len(dones))]\n",
    "\n",
    "        self.states.extend(states)\n",
    "        self.actions.extend(actions)\n",
    "        self.rewards.extend(rewards)\n",
    "        self.states_.extend(states_)\n",
    "        self.dones.extend(dones)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        samples = random.sample(range(len(self.actions)), size)\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        states_ = []\n",
    "        dones = []\n",
    "        for i in samples:\n",
    "            states.append(self.states[i])\n",
    "            actions.append(self.actions[i])\n",
    "            rewards.append(self.rewards[i])\n",
    "            states_.append(self.states_[i])\n",
    "            dones.append(self.dones[i])\n",
    "                \n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTarget(tfVars, sess):\n",
    "    \"\"\"Updates the parameters of our target network with those of the primary network.\"\"\"\n",
    "    total_vars = len(tfVars)\n",
    "    for idx, var in enumerate(tfVars[:int(total_vars / 2)]):\n",
    "        sess.run(tfVars[int(idx + total_vars / 2)].assign(var.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\"Take 1D float array of rewards and compute discounted reward.\n",
    "    \n",
    "    Discounts rewards for a given episode.\n",
    "    This is the Monte-Carlo method since we apply it to all rewards\n",
    "    in a given episode.\n",
    "    \n",
    "    Provides more robust reward signal to DQN.\n",
    "    \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of experiences to use for each training step\n",
    "batch_size = 100\n",
    "\n",
    "# how often to execute training step\n",
    "update_freq = 10\n",
    "\n",
    "# discount factor on target Q-values\n",
    "y = 0.99\n",
    "\n",
    "# starting chance of random action\n",
    "startE = 1\n",
    "\n",
    "# final chance of random action\n",
    "endE = 0.1\n",
    "\n",
    "# how many training steps required to fully reduce startE to endE\n",
    "anneling_steps = 3000\n",
    "\n",
    "# number of episodes of env to train network with\n",
    "num_episodes = 100\n",
    "\n",
    "# number of random actions before training begins\n",
    "pre_train_steps = 500\n",
    "\n",
    "# load saved model?\n",
    "load_model = False\n",
    "\n",
    "# path to save model to\n",
    "path = \"./dqn\"\n",
    "\n",
    "# size of final convolutional layer before\n",
    "# splitting it into Advantage and Value streams\n",
    "h_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get frame shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frameShape = env.observation_space.shape\n",
    "frameShape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "0 207 1.0 1\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n",
      "(1, 25, 19, 8)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-82be7f105740>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    101\u001b[0m                                      feed_dict={mainQN.rgb_array: np.array(states_t),\n\u001b[0;32m    102\u001b[0m                                                \u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargetQ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                                                mainQN.actions: actions_t})\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                         \u001b[0mupdateTarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 372\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    373\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 636\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    637\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 708\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    709\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    713\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 715\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    716\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size, frameShape, batch_size)\n",
    "targetQN = Qnetwork(h_size, frameShape, batch_size)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "# set rate of random action decrease\n",
    "e = startE\n",
    "stepDrop = (startE - endE) / anneling_steps\n",
    "\n",
    "# create lists to contain total rewards and steps per episode\n",
    "movesList = []\n",
    "rewardList = []\n",
    "total_steps = 0\n",
    "\n",
    "# make path for model to be saved in\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print(\"Loading Model...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "\n",
    "    # set target network to be equal to primary network\n",
    "    updateTarget(trainables, sess)\n",
    "\n",
    "    try:\n",
    "        for i in range(num_episodes):\n",
    "            episodeBuffer = experience_buffer()\n",
    "\n",
    "            # reset environment and get first new observation\n",
    "            env.reset()\n",
    "            state = env.ale.getScreenGrayscale()\n",
    "            done = False\n",
    "            rewardAll = 0\n",
    "            numMoves = 0\n",
    "\n",
    "            # the Q-Network\n",
    "            while numMoves < 500:\n",
    "                numMoves += 1\n",
    "\n",
    "                # choose action with probability e of being a random action\n",
    "                if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                    action = np.random.randint(0, env.action_space.n) # 2 = num different actions\n",
    "                else:\n",
    "                    # select [0] position because returns in the form [action]\n",
    "                    action, a, b, c, d = sess.run([mainQN.predict, mainQN.conv1, mainQN.conv2, mainQN.conv3, mainQN.conv4],\n",
    "                                                  feed_dict={mainQN.rgb_array: [state]})\n",
    "                    action = action[0]\n",
    "                    print(np.array(a).shape)\n",
    "#                     print(b.shape)\n",
    "#                     print(c.shape)\n",
    "#                     print(d.shape)\n",
    "                observation, reward, done, _ = env.step(action)\n",
    "                if i % 100 == 0 and i != 0:\n",
    "                    env.render()\n",
    "                state1 = env.ale.getScreenGrayscale()\n",
    "                total_steps += 1\n",
    "\n",
    "                # save experience to episode buffer\n",
    "                episodeBuffer.add([state], [action], [reward], [state1], [done])\n",
    "\n",
    "                if total_steps > pre_train_steps:\n",
    "                    if e > endE:\n",
    "                        e -= stepDrop\n",
    "\n",
    "                    if total_steps % update_freq == 0:                        \n",
    "\n",
    "#                     if total_steps % (update_freq * 100) == 0:\n",
    "                        # random sample of experiences\n",
    "                        states_t, actions_t, rewards_t, state1_t, dones_t = myBuffer.sample(batch_size)\n",
    "\n",
    "                        # Double-DQN update to the target Q-values\n",
    "                        Q1 = sess.run(mainQN.predict, feed_dict={\n",
    "                                mainQN.rgb_array: state1_t})\n",
    "\n",
    "                        Q2 = sess.run(targetQN.Qout, feed_dict={\n",
    "                                targetQN.rgb_array: state1_t})\n",
    "                        \n",
    "                        # If resulting state is DONE, Q-Target = r\n",
    "                        # If True: 0. If False: 1.\n",
    "                        end_multiplier = -(np.array(dones_t) - 1)\n",
    "\n",
    "                        # The Q values for predicted actions\n",
    "                        doubleQ = np.array([Q2[i, j] for i, j in zip(range(len(Q1)), Q1)])\n",
    "                        \n",
    "                        targetQ = np.array(rewards_t) + (y * doubleQ * end_multiplier)\n",
    "                        \n",
    "                        # update network with target values\n",
    "                        _  = sess.run(mainQN.updateModel,\n",
    "                                     feed_dict={mainQN.rgb_array: np.array(states_t),\n",
    "                                               mainQN.targetQ: targetQ,\n",
    "                                               mainQN.actions: actions_t})\n",
    "                        \n",
    "                        updateTarget(trainables, sess)\n",
    "\n",
    "                rewardAll += reward\n",
    "                state = state1\n",
    "\n",
    "                if done == True:\n",
    "                    break\n",
    "\n",
    "            # get all experiences from this episode\n",
    "            episodeRewards = np.array(episodeBuffer.rewards)\n",
    "\n",
    "            # discount all rewards\n",
    "            discountRewards = discount_rewards(episodeRewards)\n",
    "            episodeBuffer.rewards = discountRewards\n",
    "\n",
    "            # add discounted experiences to our experience buffer\n",
    "            # state, action, reward, state1, done\n",
    "            myBuffer.add(episodeBuffer.states,\n",
    "                         episodeBuffer.actions,\n",
    "                         episodeBuffer.rewards,\n",
    "                         episodeBuffer.states_,\n",
    "                         episodeBuffer.dones)\n",
    "\n",
    "            movesList.append(numMoves)\n",
    "            rewardList.append(rewardAll)\n",
    "\n",
    "            # periodically save model\n",
    "            if i % 1000 == 0:\n",
    "                saver.save(sess, path+'/model-'+str(i)+'.cptk')\n",
    "                print(\"Saved Model\")\n",
    "            if i % 10 == 0:\n",
    "                print(i, total_steps, rewardAll, e)\n",
    "\n",
    "        saver.save(sess, path+'/model-'+str(i)+'.cptk')\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        # if frames are still rendering, stop it\n",
    "        if env.viewer is not None:\n",
    "            env.viewer.close()\n",
    "            env.viewer = None\n",
    "        \n",
    "        saver.save(sess, path+'/model-'+str(i)+'.cptk')\n",
    "        print(\"Saved Model\")\n",
    "\n",
    "print(\"Reward of last episode: \" + str(rewardList[len(rewardList)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
