{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout-v0 - Double Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8bfc4c72475a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Breakout-v0 Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-28 14:15:44,910] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self, h_size, frameShape, batch_size):\n",
    "        # The network recieves a frame from the game, flattened into an array.\n",
    "        # It then resizes it and processes it through four convolutional layers.\n",
    "        \n",
    "        # raw pixel data (grayscale, so only 1 channel)\n",
    "        self.rgb_array = tf.placeholder(shape=[None, frameShape[0], frameShape[1], 1], dtype=tf.float32)\n",
    "        \n",
    "        resizeShape = [84, 84]\n",
    "        \n",
    "        # tf input: a 4-D tensor [batch_size, height, width, channels]\n",
    "        self.imageIn = tf.reshape(tf.image.resize_images(self.rgb_array, resizeShape),\n",
    "                                  shape=[-1, resizeShape[0], resizeShape[1], 1])\n",
    "        \n",
    "        self.conv1 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.imageIn,\n",
    "            num_outputs = 32,\n",
    "            kernel_size = [8, 8],\n",
    "            stride = [4, 4],\n",
    "            padding = 'VALID',\n",
    "            activation_fn = tf.nn.relu)\n",
    "        \n",
    "        self.conv2 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv1,\n",
    "            num_outputs = 64,\n",
    "            kernel_size = [4, 4],\n",
    "            stride = [2, 2],\n",
    "            padding = 'VALID',\n",
    "            activation_fn = tf.nn.relu)\n",
    "        \n",
    "        self.conv3 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv2,\n",
    "            num_outputs = 64,\n",
    "            kernel_size = [3, 3],\n",
    "            stride = [1, 1],\n",
    "            padding = 'VALID',\n",
    "            activation_fn = tf.nn.relu)\n",
    "        \n",
    "        self.conv4 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv3,\n",
    "            num_outputs = 512,\n",
    "            kernel_size = [7, 7],\n",
    "            stride = [1, 1],\n",
    "            padding = 'VALID',\n",
    "            activation_fn = tf.nn.relu)\n",
    "        \n",
    "        # We take the output from the final convolutional layer and split it\n",
    "        # into separate advantage and value streams.\n",
    "        \n",
    "        # TODO: figure out what shape of self.conv4 is\n",
    "        # split on the 3rd dimension into 2 different parts\n",
    "        self.streamAC, self.streamVC = tf.split(3, 2, self.conv4)\n",
    "        \n",
    "        # flatten to [batch_size, k]\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        \n",
    "        # weights for advantage and value stream layer\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size/2 , env.action_space.n]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size/2 , 1]))\n",
    "        \n",
    "        # output of advantage and value layer\n",
    "        self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "        self.Value = tf.matmul(self.streamV, self.VW)\n",
    "        \n",
    "        # Then combine them together to get our final Q-values.\n",
    "        # Q(s, a) = V(s) + A(a)\n",
    "        self.Qout = self.Value + tf.sub(\n",
    "            self.Advantage,\n",
    "            tf.reduce_mean( # TODO: understand how this A(a) is calculated\n",
    "                self.Advantage,\n",
    "                reduction_indices=1,\n",
    "                keep_dims=True))\n",
    "        \n",
    "        # index of max value across 1st dimension\n",
    "        self.predict = tf.argmax(self.Qout, 1)\n",
    "        \n",
    "        # Below we obtain the loss by taking the sum of squares difference between\n",
    "        # the target and prediction Q values.\n",
    "        \n",
    "        # target Q value\n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        \n",
    "        # possible actions\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, env.action_space.n, dtype=tf.float32)\n",
    "        \n",
    "        # predicted Q values\n",
    "        self.Q = tf.reduce_sum(tf.mul(self.Qout, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        # error = sum( (target - actual)^2 ) / batch_size\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        \n",
    "        # define trainer\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        \n",
    "        # define optimizer\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    \"\"\"Used to store experiences and samples randomly to train the network.\"\"\"\n",
    "    def __init__(self, buffer_size=10000):\n",
    "        self.buffer_size = buffer_size\n",
    "        # state, action, reward, state1, done\n",
    "        self.states = np.array([])\n",
    "        self.actions = np.array([])\n",
    "        self.rewards = np.array([])\n",
    "        self.states_ = np.array([])\n",
    "        self.dones = np.array([])\n",
    "        \n",
    "    def add(self, states, actions, rewards, states_, dones):\n",
    "        if len(self.states) + len(states) >= self.buffer_size:\n",
    "            self.states = self.states[:(self.buffer_size - len(states))]\n",
    "            self.actions = self.actions[:(self.buffer_size - len(actions))]\n",
    "            self.rewards = self.rewards[:(self.buffer_size - len(rewards))]\n",
    "            self.states_ = self.states_[:(self.buffer_size - len(states_))]\n",
    "            self.dones = self.dones[:(self.buffer_size - len(dones))]\n",
    "\n",
    "        self.states.append(states)\n",
    "        self.actions.append(actions)\n",
    "        self.rewards.append(rewards)\n",
    "        self.states_.append(states_)\n",
    "        self.dones.append(dones)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        samples = random.sample(range(self.actions.size), size)\n",
    "\n",
    "        states = np.array([])\n",
    "        actions = np.array([])\n",
    "        rewards = np.array([])\n",
    "        states_ = np.array([])\n",
    "        dones = np.array([])\n",
    "        for i in samples:\n",
    "            states.append(self.states[i])\n",
    "            actions.append(self.actions[i])\n",
    "            rewards.append(self.rewards[i])\n",
    "            states_.append(self.states_[i])\n",
    "            dones.append(self.dones[i])\n",
    "                \n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTarget(tfVars, sess):\n",
    "    \"\"\"Updates the parameters of our target network with those of the primary network.\"\"\"\n",
    "    total_vars = len(tfVars)\n",
    "    for idx, var in enumerate(tfVars[:int(total_vars / 2)]):\n",
    "        sess.run(tfVars[int(idx + total_vars / 2)].assign(var.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\"Take 1D float array of rewards and compute discounted reward.\n",
    "    \n",
    "    Discounts rewards for a given episode.\n",
    "    This is the Monte-Carlo method since we apply it to all rewards\n",
    "    in a given episode.\n",
    "    \n",
    "    Provides more robust reward signal to DQN.\n",
    "    \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of experiences to use for each training step\n",
    "batch_size = 32\n",
    "\n",
    "# how often to execute training step\n",
    "update_freq = 4\n",
    "\n",
    "# discount factor on target Q-values\n",
    "y = 0.99\n",
    "\n",
    "# starting chance of random action\n",
    "startE = 1\n",
    "\n",
    "# final chance of random action\n",
    "endE = 0.1\n",
    "\n",
    "# how many training steps required to fully reduce startE to endE\n",
    "anneling_steps = 10000\n",
    "\n",
    "# number of episodes of env to train network with\n",
    "num_episodes = 100\n",
    "\n",
    "# number of random actions before training begins\n",
    "pre_train_steps = 500\n",
    "\n",
    "# load saved model?\n",
    "load_model = False\n",
    "\n",
    "# path to save model to\n",
    "path = \"./dqn\"\n",
    "\n",
    "# size of final convolutional layer before\n",
    "# splitting it into Advantage and Value streams\n",
    "h_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get frame shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frameShape = env.observation_space.shape\n",
    "frameShape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1e32e93aba4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmainQN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframeShape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtargetQN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframeShape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size, frameShape, batch_size)\n",
    "targetQN = Qnetwork(h_size, frameShape, batch_size)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "# set rate of random action decrease\n",
    "e = startE\n",
    "stepDrop = (startE - endE) / anneling_steps\n",
    "\n",
    "# create lists to contain total rewards and steps per episode\n",
    "# movesList = []\n",
    "lastReward = 0\n",
    "total_steps = 0\n",
    "\n",
    "# make path for model to be saved in\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print(\"Loading Model...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "\n",
    "    # set target network to be equal to primary network\n",
    "    updateTarget(trainables, sess)\n",
    "\n",
    "    try:\n",
    "        for i in range(num_episodes):\n",
    "            episodeBuffer = experience_buffer()\n",
    "\n",
    "            # reset environment and get first new observation\n",
    "            env.reset()\n",
    "            state = env.ale.getScreenGrayscale()\n",
    "            done = False\n",
    "            rewardAll = 0\n",
    "            numMoves = 0\n",
    "\n",
    "            # the Q-Network\n",
    "            while numMoves < 500:\n",
    "                numMoves += 1\n",
    "\n",
    "                # choose action with probability e of being a random action\n",
    "                if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                    action = np.random.randint(0, env.action_space.n) # 2 = num different actions\n",
    "                else:\n",
    "                    # select [0] position because returns in the form [action]\n",
    "                    action, a, b, c, d = sess.run([mainQN.predict, mainQN.conv1, mainQN.conv2, mainQN.conv3, mainQN.conv4],\n",
    "                                                  feed_dict={mainQN.rgb_array: [state]})\n",
    "                    action = action[0]\n",
    "                    print(np.array(a).shape)\n",
    "#                     print(b.shape)\n",
    "#                     print(c.shape)\n",
    "#                     print(d.shape)\n",
    "                observation, reward, done, _ = env.step(action)\n",
    "                if i % 100 == 0 and i != 0:\n",
    "                    env.render()\n",
    "                state1 = env.ale.getScreenGrayscale()\n",
    "                total_steps += 1\n",
    "\n",
    "                # save experience to episode buffer\n",
    "                episodeBuffer.add([state], [action], [reward], [state1], [done])\n",
    "\n",
    "                if total_steps > pre_train_steps:\n",
    "                    if e > endE:\n",
    "                        e -= stepDrop\n",
    "\n",
    "                    if total_steps % update_freq == 0:                        \n",
    "\n",
    "#                     if total_steps % (update_freq * 100) == 0:\n",
    "                        # random sample of experiences\n",
    "                        states_t, actions_t, rewards_t, state1_t, dones_t = myBuffer.sample(batch_size)\n",
    "\n",
    "                        # Double-DQN update to the target Q-values\n",
    "                        Q1 = sess.run(mainQN.predict, feed_dict={\n",
    "                                mainQN.rgb_array: state1_t})\n",
    "\n",
    "                        Q2 = sess.run(targetQN.Qout, feed_dict={\n",
    "                                targetQN.rgb_array: state1_t})\n",
    "                        \n",
    "                        # If resulting state is DONE, Q-Target = r\n",
    "                        # If True: 0. If False: 1.\n",
    "                        end_multiplier = -(np.array(dones_t) - 1)\n",
    "\n",
    "                        # The Q values for predicted actions\n",
    "                        doubleQ = np.array([Q2[i, j] for i, j in zip(range(len(Q1)), Q1)])\n",
    "                        \n",
    "                        targetQ = np.array(rewards_t) + (y * doubleQ * end_multiplier)\n",
    "                        \n",
    "                        # update network with target values\n",
    "                        _  = sess.run(mainQN.updateModel,\n",
    "                                     feed_dict={mainQN.rgb_array: np.array(states_t),\n",
    "                                               mainQN.targetQ: targetQ,\n",
    "                                               mainQN.actions: actions_t})\n",
    "                        \n",
    "                        updateTarget(trainables, sess)\n",
    "\n",
    "                rewardAll += reward\n",
    "                state = state1\n",
    "\n",
    "                if done == True:\n",
    "                    break\n",
    "\n",
    "            # get all experiences from this episode\n",
    "            episodeRewards = np.array(episodeBuffer.rewards)\n",
    "\n",
    "            # discount all rewards\n",
    "            discountRewards = discount_rewards(episodeRewards)\n",
    "            episodeBuffer.rewards = discountRewards\n",
    "\n",
    "            # add discounted experiences to our experience buffer\n",
    "            # state, action, reward, state1, done\n",
    "            myBuffer.add(episodeBuffer.states,\n",
    "                         episodeBuffer.actions,\n",
    "                         episodeBuffer.rewards,\n",
    "                         episodeBuffer.states_,\n",
    "                         episodeBuffer.dones)\n",
    "\n",
    "            movesList.append(numMoves)\n",
    "            lastReward = rewardAll\n",
    "\n",
    "            # periodically save model\n",
    "            if i % 1000 == 0:\n",
    "                saver.save(sess, path+'/model-'+str(i)+'.cptk')\n",
    "                print(\"Saved Model\")\n",
    "            if i % 10 == 0:\n",
    "                print(i, total_steps, rewardAll, e)\n",
    "\n",
    "        saver.save(sess, path+'/model-'+str(i)+'.cptk')\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        # if frames are still rendering, stop it\n",
    "        if env.viewer is not None:\n",
    "            env.viewer.close()\n",
    "            env.viewer = None\n",
    "        \n",
    "        saver.save(sess, path+'/model-'+str(i)+'.cptk')\n",
    "        print(\"Saved Model\")\n",
    "\n",
    "print(\"Reward of last episode: \" + str(lastReward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
