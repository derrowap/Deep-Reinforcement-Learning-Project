{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "from IPython import display\n",
    "import time\n",
    "import math\n",
    "import cProfile\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class gameOb():\n",
    "    def __init__(self,coordinates,size,intensity,channel,reward,name):\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        self.reward = reward\n",
    "        self.name = name\n",
    "        \n",
    "class gameEnv():\n",
    "    def __init__(self,partial,size):\n",
    "        self.sizeX = size\n",
    "        self.sizeY = size\n",
    "        self.actions = 4\n",
    "        self.objects = []\n",
    "        self.partial = partial\n",
    "        self.a = self.reset()\n",
    "#         plt.imshow(self.a,interpolation=\"nearest\")\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.objects = []\n",
    "        hero = gameOb(self.newPosition(),1,1,2,None,'hero')\n",
    "        self.objects.append(hero)\n",
    "        bug = gameOb(self.newPosition(),1,1,1,10,'goal')\n",
    "        self.objects.append(bug)\n",
    "        hole = gameOb(self.newPosition(),1,1,0,-2,'fire')\n",
    "        self.objects.append(hole)\n",
    "#         bug2 = gameOb(self.newPosition(),1,1,1,10,'goal')\n",
    "#         self.objects.append(bug2)\n",
    "        hole2 = gameOb(self.newPosition(),1,1,0,-2,'fire')\n",
    "        self.objects.append(hole2)\n",
    "#         bug3 = gameOb(self.newPosition(),1,1,1,10,'goal')\n",
    "#         self.objects.append(bug3)\n",
    "#         bug4 = gameOb(self.newPosition(),1,1,1,10,'goal')\n",
    "#         self.objects.append(bug4)\n",
    "        self.a = self.renderEnv()\n",
    "        return self.a\n",
    "\n",
    "    def moveChar(self,direction):\n",
    "        # 0 - up, 1 - down, 2 - left, 3 - right\n",
    "        hero = self.objects[0]\n",
    "        heroX = hero.x\n",
    "        heroY = hero.y\n",
    "        penalize = 0.00\n",
    "        if direction == 0 and hero.y >= 1:\n",
    "            hero.y -= 1\n",
    "        if direction == 1 and hero.y <= self.sizeY-2:\n",
    "            hero.y += 1\n",
    "        if direction == 2 and hero.x >= 1:\n",
    "            hero.x -= 1\n",
    "        if direction == 3 and hero.x <= self.sizeX-2:\n",
    "            hero.x += 1     \n",
    "        if hero.x == heroX and hero.y == heroY:\n",
    "            penalize = 1\n",
    "        self.objects[0] = hero\n",
    "        return penalize\n",
    "    \n",
    "    def newPosition(self):\n",
    "        iterables = [ range(self.sizeX), range(self.sizeY)]\n",
    "        points = []\n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        currentPositions = []\n",
    "        for objectA in self.objects:\n",
    "            if (objectA.x,objectA.y) not in currentPositions:\n",
    "                currentPositions.append((objectA.x,objectA.y))\n",
    "        for pos in currentPositions:\n",
    "            points.remove(pos)\n",
    "        location = np.random.choice(range(len(points)),replace=False)\n",
    "        return points[location]\n",
    "\n",
    "    def checkGoal(self):\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'hero':\n",
    "                hero = obj\n",
    "            else:\n",
    "                others.append(obj)\n",
    "        ended = False\n",
    "        for other in others:\n",
    "            if hero.x == other.x and hero.y == other.y:\n",
    "                self.objects.remove(other)\n",
    "                if other.reward == 1:\n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,1,10,'goal'))\n",
    "                else: \n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,0,-2,'fire'))\n",
    "                return other.reward,True\n",
    "        if ended == False:\n",
    "            return 0.0,False\n",
    "\n",
    "    def renderEnv(self):\n",
    "        #a = np.zeros([self.sizeY,self.sizeX,3])\n",
    "        self.a = np.ones([self.sizeY+2,self.sizeX+2,3])\n",
    "        self.a[1:-1,1:-1,:] = 0\n",
    "        hero = None\n",
    "        for item in self.objects:\n",
    "            self.a[item.y+1:item.y+item.size+1,item.x+1:item.x+item.size+1,item.channel] = item.intensity\n",
    "            if item.name == 'hero':\n",
    "                hero = item\n",
    "        if self.partial == True:\n",
    "            self.a = self.a[hero.y:hero.y+3,hero.x:hero.x+3,:]\n",
    "        b = scipy.misc.imresize(self.a[:,:,0],[84,84,1],interp='nearest')\n",
    "        c = scipy.misc.imresize(self.a[:,:,1],[84,84,1],interp='nearest')\n",
    "        d = scipy.misc.imresize(self.a[:,:,2],[84,84,1],interp='nearest')\n",
    "        self.a = np.stack([b,c,d],axis=2)\n",
    "        return self.a\n",
    "\n",
    "    def step(self,action):\n",
    "        penalty = self.moveChar(action)\n",
    "        reward,done = self.checkGoal()\n",
    "        if(penalty != 0):\n",
    "            done = True\n",
    "        state = self.renderEnv()\n",
    "        return state,(reward-penalty),done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gameEnv(partial=False,size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size,prev_states):\n",
    "        self.image_to_resize = tf.placeholder(shape=[1,84,84,3], dtype=tf.float32,name='image_to_resize')\n",
    "        self.y = tf.identity(self.image_to_resize)\n",
    "        self.resized_image = tf.image.resize_images(self.y, 84, 84)\n",
    "        \n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.imageIn = tf.placeholder(shape=[None,84,84,3],dtype=tf.float32)\n",
    "        self.conv1 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv3,num_outputs=512,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        #We take the output from the final convolutional layer and split it into separate advantage and value streams.\n",
    "        self.streamAC,self.streamVC = tf.split(3,2,self.conv4)\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2,env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.sub(self.Advantage,tf.reduce_mean(self.Advantage,reduction_indices=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.mul(self.Qout, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    \"\"\"Used to store experiences and samples randomly to train the network.\"\"\"\n",
    "    def __init__(self, buffer_size=50000):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        \n",
    "    def add(self, states, actions, rewards, dones):\n",
    "        if len(self.actions) == self.buffer_size:\n",
    "            self.states = self.states[1:]\n",
    "            self.actions = self.actions[1:]\n",
    "            self.rewards = self.rewards[1:]\n",
    "            self.dones = self.dones[1:]\n",
    "\n",
    "        self.states.append(states)\n",
    "        self.actions.append(actions)\n",
    "        self.rewards.append(rewards)\n",
    "        self.dones.append(dones)\n",
    "        \n",
    "    def sample(self, size, previous_states):\n",
    "        samples = np.random.permutation(len(self.actions)-(previous_states-1)) + (previous_states-1)\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        states_ = []\n",
    "        dones = []\n",
    "        for i in samples[:size]:\n",
    "            temp = []\n",
    "            for j in range(previous_states):\n",
    "                temp.append(self.states[i - previous_states + j + 1])\n",
    "            states.append(np.dstack(temp))\n",
    "            actions.append(self.actions[i])\n",
    "            rewards.append(self.rewards[i])\n",
    "            states_.append(self.states[i+1])\n",
    "            dones.append(self.dones[i])\n",
    "                \n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 8 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "anneling_steps = 75000 #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 100003 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 20000 #How many steps of random actions before training begins.\n",
    "pre_train_steps_from_Q = False #If true, initialize buffer with steps from Q instead of random actions\n",
    "max_epLength = 20 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./dqn/save_data/gridWorld/seven_by_seven/\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.0001 #Rate to update target network toward primary network\n",
    "previous_states=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance(x1, y1, x2, y2):\n",
    "    return math.sqrt((x2-x1)**2+(y2-y1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000  -- avg steps  4.229  -- %complete  0.856 , anneling --  1\n",
      "2000  -- avg steps  4.244  -- %complete  0.857 , anneling --  1\n",
      "3000  -- avg steps  4.185  -- %complete  0.848 , anneling --  1\n",
      "4000  -- avg steps  4.21  -- %complete  0.837 , anneling --  1\n",
      "5000  -- avg steps  4.201  -- %complete  0.867 , anneling --  0.9871719999999872\n",
      "6000  -- avg steps  4.272  -- %complete  0.827 , anneling --  0.9359079999999359\n",
      "7000  -- avg steps  4.604  -- %complete  0.81 , anneling --  0.8806599999998806\n",
      "8000  -- avg steps  4.593  -- %complete  0.768 , anneling --  0.8255439999998255\n",
      "9000  -- avg steps  4.601  -- %complete  0.737 , anneling --  0.7703319999997703\n",
      "10000  -- avg steps  4.827  -- %complete  0.696 , anneling --  0.7124079999997124\n",
      "Saved Model @  ./dqn/save_data/gridWorld/seven_by_seven/model-10000.cptk\n",
      "11000  -- avg steps  5.025  -- %complete  0.685 , anneling --  0.6521079999996521\n",
      "12000  -- avg steps  5.271  -- %complete  0.632 , anneling --  0.5888559999995888\n",
      "13000  -- avg steps  5.757  -- %complete  0.61 , anneling --  0.5197719999995197\n",
      "14000  -- avg steps  6.446  -- %complete  0.626 , anneling --  0.44241999999944237\n",
      "15000  -- avg steps  7.586  -- %complete  0.571 , anneling --  0.35138799999935133\n",
      "16000  -- avg steps  8.537  -- %complete  0.523 , anneling --  0.24894399999924888\n",
      "17000  -- avg steps  9.831  -- %complete  0.438 , anneling --  0.1309719999991309\n",
      "18000  -- avg steps  11.18  -- %complete  0.361 , anneling --  0.09999999999912884\n",
      "19000  -- avg steps  11.645  -- %complete  0.339 , anneling --  0.09999999999912884\n",
      "20000  -- avg steps  10.984  -- %complete  0.35 , anneling --  0.09999999999912884\n",
      "Saved Model @  ./dqn/save_data/gridWorld/seven_by_seven/model-20000.cptk\n",
      "21000  -- avg steps  10.811  -- %complete  0.356 , anneling --  0.09999999999912884\n",
      "22000  -- avg steps  10.899  -- %complete  0.387 , anneling --  0.09999999999912884\n",
      "23000  -- avg steps  10.496  -- %complete  0.4 , anneling --  0.09999999999912884\n",
      "24000  -- avg steps  9.78  -- %complete  0.431 , anneling --  0.09999999999912884\n",
      "25000  -- avg steps  9.801  -- %complete  0.438 , anneling --  0.09999999999912884\n",
      "26000  -- avg steps  9.321  -- %complete  0.482 , anneling --  0.09999999999912884\n",
      "27000  -- avg steps  8.264  -- %complete  0.542 , anneling --  0.09999999999912884\n",
      "28000  -- avg steps  7.571  -- %complete  0.533 , anneling --  0.09999999999912884\n",
      "29000  -- avg steps  7.623  -- %complete  0.576 , anneling --  0.09999999999912884\n",
      "30000  -- avg steps  7.442  -- %complete  0.552 , anneling --  0.09999999999912884\n",
      "Saved Model @  ./dqn/save_data/gridWorld/seven_by_seven/model-30000.cptk\n",
      "31000  -- avg steps  7.703  -- %complete  0.548 , anneling --  0.09999999999912884\n",
      "32000  -- avg steps  6.997  -- %complete  0.577 , anneling --  0.09999999999912884\n",
      "33000  -- avg steps  6.991  -- %complete  0.631 , anneling --  0.09999999999912884\n",
      "34000  -- avg steps  7.058  -- %complete  0.589 , anneling --  0.09999999999912884\n",
      "35000  -- avg steps  6.518  -- %complete  0.616 , anneling --  0.09999999999912884\n",
      "36000  -- avg steps  6.608  -- %complete  0.655 , anneling --  0.09999999999912884\n",
      "37000  -- avg steps  6.484  -- %complete  0.65 , anneling --  0.09999999999912884\n",
      "38000  -- avg steps  6.103  -- %complete  0.697 , anneling --  0.09999999999912884\n",
      "39000  -- avg steps  6.148  -- %complete  0.672 , anneling --  0.09999999999912884\n",
      "40000  -- avg steps  5.851  -- %complete  0.673 , anneling --  0.09999999999912884\n",
      "Saved Model @  ./dqn/save_data/gridWorld/seven_by_seven/model-40000.cptk\n",
      "41000  -- avg steps  5.782  -- %complete  0.684 , anneling --  0.09999999999912884\n",
      "42000  -- avg steps  5.777  -- %complete  0.694 , anneling --  0.09999999999912884\n",
      "43000  -- avg steps  5.707  -- %complete  0.698 , anneling --  0.09999999999912884\n",
      "44000  -- avg steps  5.554  -- %complete  0.7 , anneling --  0.09999999999912884\n",
      "45000  -- avg steps  5.65  -- %complete  0.721 , anneling --  0.09999999999912884\n",
      "46000  -- avg steps  5.335  -- %complete  0.722 , anneling --  0.09999999999912884\n",
      "47000  -- avg steps  5.527  -- %complete  0.731 , anneling --  0.09999999999912884\n",
      "48000  -- avg steps  5.412  -- %complete  0.722 , anneling --  0.09999999999912884\n",
      "49000  -- avg steps  5.793  -- %complete  0.724 , anneling --  0.09999999999912884\n",
      "50000  -- avg steps  5.663  -- %complete  0.762 , anneling --  0.09999999999912884\n",
      "Saved Model @  ./dqn/save_data/gridWorld/seven_by_seven/model-50000.cptk\n",
      "51000  -- avg steps  5.393  -- %complete  0.765 , anneling --  0.09999999999912884\n",
      "52000  -- avg steps  5.529  -- %complete  0.727 , anneling --  0.09999999999912884\n",
      "53000  -- avg steps  5.722  -- %complete  0.767 , anneling --  0.09999999999912884\n",
      "54000  -- avg steps  5.203  -- %complete  0.752 , anneling --  0.09999999999912884\n",
      "55000  -- avg steps  5.4  -- %complete  0.748 , anneling --  0.09999999999912884\n",
      "56000  -- avg steps  5.797  -- %complete  0.768 , anneling --  0.09999999999912884\n",
      "57000  -- avg steps  5.267  -- %complete  0.767 , anneling --  0.09999999999912884\n",
      "58000  -- avg steps  5.15  -- %complete  0.783 , anneling --  0.09999999999912884\n",
      "59000  -- avg steps  5.497  -- %complete  0.781 , anneling --  0.09999999999912884\n",
      "60000  -- avg steps  5.277  -- %complete  0.755 , anneling --  0.09999999999912884\n",
      "Saved Model @  ./dqn/save_data/gridWorld/seven_by_seven/model-60000.cptk\n",
      "61000  -- avg steps  5.279  -- %complete  0.801 , anneling --  0.09999999999912884\n",
      "62000  -- avg steps  5.051  -- %complete  0.779 , anneling --  0.09999999999912884\n",
      "63000  -- avg steps  5.176  -- %complete  0.765 , anneling --  0.09999999999912884\n",
      "64000  -- avg steps  4.916  -- %complete  0.794 , anneling --  0.09999999999912884\n",
      "65000  -- avg steps  4.99  -- %complete  0.802 , anneling --  0.09999999999912884\n",
      "66000  -- avg steps  4.942  -- %complete  0.811 , anneling --  0.09999999999912884\n",
      "67000  -- avg steps  5.199  -- %complete  0.808 , anneling --  0.09999999999912884\n",
      "68000  -- avg steps  5.03  -- %complete  0.819 , anneling --  0.09999999999912884\n",
      "69000  -- avg steps  4.988  -- %complete  0.805 , anneling --  0.09999999999912884\n",
      "70000  -- avg steps  5.079  -- %complete  0.799 , anneling --  0.09999999999912884\n",
      "Saved Model @  ./dqn/save_data/gridWorld/seven_by_seven/model-70000.cptk\n",
      "71000  -- avg steps  5.091  -- %complete  0.826 , anneling --  0.09999999999912884\n",
      "72000  -- avg steps  5.264  -- %complete  0.828 , anneling --  0.09999999999912884\n",
      "73000  -- avg steps  5.168  -- %complete  0.825 , anneling --  0.09999999999912884\n",
      "74000  -- avg steps  5.034  -- %complete  0.827 , anneling --  0.09999999999912884\n",
      "75000  -- avg steps  5.351  -- %complete  0.822 , anneling --  0.09999999999912884\n",
      "76000  -- avg steps  4.793  -- %complete  0.827 , anneling --  0.09999999999912884\n",
      "77000  -- avg steps  5.013  -- %complete  0.821 , anneling --  0.09999999999912884\n",
      "78000  -- avg steps  5.196  -- %complete  0.82 , anneling --  0.09999999999912884\n",
      "79000  -- avg steps  5.044  -- %complete  0.856 , anneling --  0.09999999999912884\n",
      "80000  -- avg steps  4.848  -- %complete  0.828 , anneling --  0.09999999999912884\n",
      "Saved Model @  ./dqn/save_data/gridWorld/seven_by_seven/model-80000.cptk\n",
      "81000  -- avg steps  4.953  -- %complete  0.857 , anneling --  0.09999999999912884\n",
      "82000  -- avg steps  4.927  -- %complete  0.831 , anneling --  0.09999999999912884\n",
      "83000  -- avg steps  4.789  -- %complete  0.85 , anneling --  0.09999999999912884\n",
      "84000  -- avg steps  5.005  -- %complete  0.853 , anneling --  0.09999999999912884\n",
      "85000  -- avg steps  5.002  -- %complete  0.844 , anneling --  0.09999999999912884\n",
      "86000  -- avg steps  4.828  -- %complete  0.846 , anneling --  0.09999999999912884\n",
      "87000  -- avg steps  4.833  -- %complete  0.859 , anneling --  0.09999999999912884\n",
      "88000  -- avg steps  5.052  -- %complete  0.828 , anneling --  0.09999999999912884\n",
      "89000  -- avg steps  4.576  -- %complete  0.863 , anneling --  0.09999999999912884\n",
      "90000  -- avg steps  5.013  -- %complete  0.865 , anneling --  0.09999999999912884\n",
      "Saved Model @  ./dqn/save_data/gridWorld/seven_by_seven/model-90000.cptk\n",
      "91000  -- avg steps  4.953  -- %complete  0.846 , anneling --  0.09999999999912884\n",
      "92000  -- avg steps  5.158  -- %complete  0.863 , anneling --  0.09999999999912884\n",
      "93000  -- avg steps  4.825  -- %complete  0.85 , anneling --  0.09999999999912884\n",
      "94000  -- avg steps  4.938  -- %complete  0.871 , anneling --  0.09999999999912884\n",
      "95000  -- avg steps  5.03  -- %complete  0.874 , anneling --  0.09999999999912884\n",
      "96000  -- avg steps  4.804  -- %complete  0.876 , anneling --  0.09999999999912884\n",
      "97000  -- avg steps  4.916  -- %complete  0.888 , anneling --  0.09999999999912884\n",
      "98000  -- avg steps  4.872  -- %complete  0.892 , anneling --  0.09999999999912884\n",
      "99000  -- avg steps  4.889  -- %complete  0.894 , anneling --  0.09999999999912884\n",
      "100000  -- avg steps  4.627  -- %complete  0.896 , anneling --  0.09999999999912884\n",
      "Saved Model @  ./dqn/save_data/gridWorld/seven_by_seven/model-100000.cptk\n",
      "Percent of succesful episodes: 6.983940481785546%\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size,previous_states)\n",
    "targetQN = Qnetwork(h_size,previous_states)\n",
    "t_start = time.time()\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "#arrays to save\n",
    "eps_arr = []\n",
    "time_arr = []\n",
    "err_arr = []\n",
    "annel_arr = []\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "step_list = []\n",
    "reward_list = []\n",
    "total_steps = 0\n",
    "hero_x = 0\n",
    "hero_y = 0\n",
    "least_distance = 100\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        load = './dqn/awjuliani/algorithm_train1/model-70000.cptk'\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,load)\n",
    "    sess.run(init)\n",
    "    updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "    for i in range(1, num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s = np.reshape(s, [-1, 84, 84, 3])\n",
    "        s = sess.run(mainQN.resized_image, feed_dict={mainQN.image_to_resize:s})[0]\n",
    "        if(i==1):\n",
    "            myBuffer.states.append(s)\n",
    "        d = False\n",
    "        reward_sum = 0\n",
    "        step = 0\n",
    "        #The Q-Network\n",
    "        while step < max_epLength: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "            step+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:       \n",
    "                goal_x = 0\n",
    "                goal_y = 0\n",
    "                for obj in env.objects:\n",
    "                    if(obj.name == 'hero'):\n",
    "                        hero_x = obj.x\n",
    "                        hero_y = obj.y\n",
    "                    if(obj.name == 'goal'):\n",
    "                        dist = distance(hero_x,hero_y,obj.x,obj.y)\n",
    "                        if(dist<least_distance):\n",
    "                            lesast_distance = dist\n",
    "                            goal_x = obj.x\n",
    "                            goal_y = obj.y\n",
    "\n",
    "                            x_dir = hero_x - goal_x\n",
    "                            y_dir = hero_y - goal_y\n",
    "\n",
    "                            if(y_dir > 0):\n",
    "                                a = 0\n",
    "                            elif(y_dir < 0):\n",
    "                                a = 1\n",
    "                            elif(x_dir > 0):\n",
    "                                a = 2\n",
    "                            elif(x_dir < 0):\n",
    "                                a = 3\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.imageIn:[s]})[0]\n",
    "            s1,r,d = env.step(a)\n",
    "            s1 = np.reshape(s1, [-1, 84, 84, 3])\n",
    "            s1 = sess.run(mainQN.resized_image, feed_dict={mainQN.image_to_resize:s1})[0]\n",
    "            total_steps += 1\n",
    "            myBuffer.add(s1,a,r,d) #Save the experience to our episode buffer.\n",
    "\n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    states, actions, rewards, state_, done = myBuffer.sample(batch_size,previous_states) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.imageIn:state_})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.imageIn:state_})\n",
    "                    end_multiplier = -(np.array(done) - 1)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    targetQ = rewards + (y*doubleQ * end_multiplier)\n",
    "                    #Update the network with our target values.\n",
    "                    _ = sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.imageIn:states,mainQN.targetQ:targetQ, mainQN.actions:actions})\n",
    "\n",
    "                    updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "            reward_sum += r\n",
    "            s = s1\n",
    "\n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "\n",
    "        #Get all experiences from this episode and discount their rewards. \n",
    "        step_list.append(step)\n",
    "        reward_list.append(reward_sum)\n",
    "        #Periodically save the model. \n",
    "        if i % 1000 == 0:\n",
    "            print(str(i), \" -- avg steps \",np.mean(step_list[-1000:]), \" -- %complete \", (len([k for k in reward_list[-1000:] if k > 0]) / len(reward_list[-1000:])), ', anneling -- ', e)\n",
    "            eps_arr.append(i)\n",
    "            time_arr.append(time.time() - t_start)\n",
    "            err_arr.append((len([k for k in reward_list[-100:] if k > 0]) / len(reward_list[-100:])))\n",
    "            annel_arr.append(e)\n",
    "        if i % 10000 == 0:\n",
    "            saver.save(sess,path+'model-'+str(i)+'.cptk')\n",
    "            np.savez(path+'data' + str(i)+ '.npz', episode=eps_arr, time=time_arr, error = err_arr, anneling = annel_arr)\n",
    "            print(\"Saved Model @ \", path+'model-'+str(i)+'.cptk')\n",
    "            \n",
    "    saver.save(sess,path+'model-'+str(i)+'.cptk')\n",
    "    np.savez(path+'data' + str(i)+ '.npz', episode=eps_arr, time=time_arr, error = err_arr, anneling = annel_arr)\n",
    "print(\"Percent of succesful episodes: \" + str(sum(reward_list)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "tfVars length:  12\n",
      "100 306 3.06 0.9 0.9923200000000065\n",
      "200 602 2.96 0.89 0.9834400000000141\n",
      "300 896 2.94 0.93 0.9746200000000216\n",
      "400 1221 3.25 0.89 0.9648700000000299\n",
      "500 1557 3.36 0.86 0.9547900000000384\n",
      "600 1876 3.19 0.88 0.9452200000000466\n",
      "700 2193 3.17 0.83 0.9357100000000547\n",
      "800 2562 3.69 0.87 0.9246400000000641\n",
      "900 2887 3.25 0.79 0.9148900000000724\n",
      "\n",
      "last 1000 -- avg steps  3.21  -- %complete  0.875\n",
      "\n",
      "1000 3210 3.23 0.91 0.9052000000000806\n",
      "1100 3536 3.26 0.86 0.8954200000000889\n",
      "1200 3873 3.37 0.84 0.8853100000000975\n",
      "1300 4201 3.28 0.81 0.8754700000001059\n",
      "1400 4544 3.43 0.81 0.8651800000001146\n",
      "1500 4899 3.55 0.86 0.8545300000001237\n",
      "1600 5209 3.1 0.79 0.8452300000001316\n",
      "1700 5574 3.65 0.81 0.8342800000001409\n",
      "1800 5941 3.67 0.82 0.8232700000001503\n",
      "1900 6325 3.84 0.84 0.8117500000001601\n",
      "\n",
      "last 1000 -- avg steps  3.484  -- %complete  0.826\n",
      "\n",
      "2000 6694 3.69 0.82 0.8006800000001695\n",
      "2100 7064 3.7 0.7 0.7895800000001789\n",
      "2200 7409 3.45 0.77 0.7792300000001877\n",
      "2300 7818 4.09 0.81 0.7669600000001981\n",
      "2400 8206 3.88 0.78 0.755320000000208\n",
      "2500 8532 3.26 0.75 0.7455400000002164\n",
      "2600 8886 3.54 0.78 0.7349200000002254\n",
      "2700 9222 3.36 0.77 0.724840000000234\n",
      "2800 9614 3.92 0.77 0.713080000000244\n",
      "2900 10044 4.3 0.77 0.7001800000002549\n",
      "\n",
      "last 1000 -- avg steps  3.744  -- %complete  0.765\n",
      "\n",
      "3000 10438 3.94 0.75 0.688360000000265\n",
      "3100 10774 3.36 0.73 0.6782800000002736\n",
      "3200 11164 3.9 0.77 0.6665800000002835\n",
      "3300 11582 4.18 0.71 0.6540400000002942\n",
      "3400 12076 4.94 0.73 0.6392200000003068\n",
      "3500 12529 4.53 0.68 0.6256300000003183\n",
      "3600 12915 3.86 0.68 0.6140500000003282\n",
      "3700 13290 3.75 0.68 0.6028000000003377\n",
      "3800 13752 4.62 0.71 0.5889400000003495\n",
      "3900 14216 4.64 0.71 0.5750200000003614\n",
      "\n",
      "last 1000 -- avg steps  4.271  -- %complete  0.704\n",
      "\n",
      "4000 14709 4.93 0.64 0.5602300000003739\n",
      "4100 15283 5.74 0.62 0.5430100000003886\n",
      "4200 15731 4.48 0.61 0.5295700000004\n",
      "4300 16256 5.25 0.68 0.5138200000004134\n",
      "4400 16717 4.61 0.57 0.49999000000042515\n",
      "4500 17194 4.77 0.54 0.4856800000004373\n",
      "4600 17603 4.09 0.54 0.47341000000044775\n",
      "4700 18123 5.2 0.61 0.457810000000461\n",
      "4800 18508 3.85 0.57 0.44626000000047084\n",
      "4900 18900 3.92 0.6 0.43450000000048083\n",
      "\n",
      "last 1000 -- avg steps  4.631  -- %complete  0.595\n",
      "\n",
      "5000 19340 4.4 0.61 0.42130000000049206\n",
      "5100 19755 4.15 0.58 0.40885000000050264\n",
      "5200 20125 3.7 0.44 0.3977500000005121\n",
      "5300 20554 4.29 0.54 0.384880000000523\n",
      "5400 21030 4.76 0.5 0.37060000000053517\n",
      "5500 21408 3.78 0.52 0.3592600000005448\n",
      "5600 21839 4.31 0.54 0.3463300000005558\n",
      "5700 22315 4.76 0.47 0.33205000000056795\n",
      "5800 22746 4.31 0.56 0.31912000000057894\n",
      "5900 23172 4.26 0.55 0.3063400000005898\n",
      "\n",
      "last 1000 -- avg steps  4.177  -- %complete  0.525\n",
      "\n",
      "6000 23517 3.45 0.55 0.2959900000005986\n",
      "6100 23866 3.49 0.54 0.2855200000006075\n",
      "6200 24339 4.73 0.5 0.2713300000006196\n",
      "6300 24851 5.12 0.5 0.25597000000063264\n",
      "6400 25312 4.61 0.5 0.24214000000063712\n",
      "6500 25779 4.67 0.51 0.22813000000063607\n",
      "6600 26206 4.27 0.52 0.21532000000063511\n",
      "6700 26577 3.71 0.52 0.20419000000063428\n",
      "6800 27068 4.91 0.54 0.18946000000063318\n",
      "6900 27487 4.19 0.36 0.17689000000063224\n",
      "\n",
      "last 1000 -- avg steps  4.414  -- %complete  0.491\n",
      "\n",
      "7000 27931 4.44 0.42 0.16357000000063124\n",
      "7100 28328 3.97 0.41 0.15166000000063035\n",
      "7200 28811 4.83 0.36 0.13717000000062926\n",
      "7300 29336 5.25 0.42 0.12142000000062808\n",
      "7400 29922 5.86 0.38 0.10384000000062676\n",
      "7500 30315 3.93 0.37 0.09997000000062647\n",
      "7600 30670 3.55 0.33 0.09997000000062647\n",
      "7700 31101 4.31 0.29 0.09997000000062647\n",
      "7800 31518 4.17 0.37 0.09997000000062647\n",
      "7900 31965 4.47 0.32 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  4.497  -- %complete  0.367\n",
      "\n",
      "8000 32428 4.63 0.42 0.09997000000062647\n",
      "8100 32790 3.62 0.32 0.09997000000062647\n",
      "8200 33210 4.2 0.33 0.09997000000062647\n",
      "8300 33572 3.62 0.38 0.09997000000062647\n",
      "8400 34040 4.68 0.32 0.09997000000062647\n",
      "8500 34525 4.85 0.37 0.09997000000062647\n",
      "8600 34967 4.42 0.4 0.09997000000062647\n",
      "8700 35335 3.68 0.4 0.09997000000062647\n",
      "8800 35732 3.97 0.43 0.09997000000062647\n",
      "8900 36134 4.02 0.33 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  4.07  -- %complete  0.359\n",
      "\n",
      "9000 36498 3.64 0.31 0.09997000000062647\n",
      "9100 36959 4.61 0.36 0.09997000000062647\n",
      "9200 37383 4.24 0.4 0.09997000000062647\n",
      "9300 37736 3.53 0.45 0.09997000000062647\n",
      "9400 38050 3.14 0.34 0.09997000000062647\n",
      "9500 38466 4.16 0.41 0.09997000000062647\n",
      "9600 38969 5.03 0.3 0.09997000000062647\n",
      "9700 39434 4.65 0.37 0.09997000000062647\n",
      "9800 39888 4.54 0.49 0.09997000000062647\n",
      "9900 40459 5.71 0.42 0.09997000000062647\n",
      "Saved Model @  ./dqn/save_data/gridWorld/one_go1/model-10000.cptk\n",
      "\n",
      "last 1000 -- avg steps  4.452  -- %complete  0.389\n",
      "\n",
      "10000 40950 4.91 0.35 0.09997000000062647\n",
      "10100 41351 4.01 0.35 0.09997000000062647\n",
      "10200 41810 4.59 0.44 0.09997000000062647\n",
      "10300 42424 6.14 0.35 0.09997000000062647\n",
      "10400 42937 5.13 0.43 0.09997000000062647\n",
      "10500 43374 4.37 0.36 0.09997000000062647\n",
      "10600 43921 5.47 0.37 0.09997000000062647\n",
      "10700 44506 5.85 0.36 0.09997000000062647\n",
      "10800 45044 5.38 0.45 0.09997000000062647\n",
      "10900 45474 4.3 0.29 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  4.978  -- %complete  0.386\n",
      "\n",
      "11000 45928 4.54 0.46 0.09997000000062647\n",
      "11100 46629 7.01 0.31 0.09997000000062647\n",
      "11200 47225 5.96 0.45 0.09997000000062647\n",
      "11300 47787 5.62 0.51 0.09997000000062647\n",
      "11400 48372 5.85 0.44 0.09997000000062647\n",
      "11500 48988 6.16 0.47 0.09997000000062647\n",
      "11600 49660 6.72 0.47 0.09997000000062647\n",
      "11700 50354 6.94 0.43 0.09997000000062647\n",
      "11800 50805 4.51 0.47 0.09997000000062647\n",
      "11900 51408 6.03 0.48 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  6.173  -- %complete  0.443\n",
      "\n",
      "12000 52101 6.93 0.4 0.09997000000062647\n",
      "12100 52588 4.87 0.49 0.09997000000062647\n",
      "12200 53165 5.77 0.48 0.09997000000062647\n",
      "12300 53623 4.58 0.41 0.09997000000062647\n",
      "12400 54293 6.7 0.43 0.09997000000062647\n",
      "12500 54866 5.73 0.48 0.09997000000062647\n",
      "12600 55345 4.79 0.47 0.09997000000062647\n",
      "12700 55835 4.9 0.52 0.09997000000062647\n",
      "12800 56522 6.87 0.46 0.09997000000062647\n",
      "12900 57072 5.5 0.54 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  5.541  -- %complete  0.485\n",
      "\n",
      "13000 57642 5.7 0.57 0.09997000000062647\n",
      "13100 58357 7.15 0.49 0.09997000000062647\n",
      "13200 59075 7.18 0.49 0.09997000000062647\n",
      "13300 59693 6.18 0.49 0.09997000000062647\n",
      "13400 60469 7.76 0.41 0.09997000000062647\n",
      "13500 61190 7.21 0.42 0.09997000000062647\n",
      "13600 61759 5.69 0.51 0.09997000000062647\n",
      "13700 62537 7.78 0.46 0.09997000000062647\n",
      "13800 63201 6.64 0.46 0.09997000000062647\n",
      "13900 63850 6.49 0.47 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  6.963  -- %complete  0.476\n",
      "\n",
      "14000 64605 7.55 0.56 0.09997000000062647\n",
      "14100 65387 7.82 0.51 0.09997000000062647\n",
      "14200 66142 7.55 0.52 0.09997000000062647\n",
      "14300 66820 6.78 0.49 0.09997000000062647\n",
      "14400 67579 7.59 0.49 0.09997000000062647\n",
      "14500 68160 5.81 0.5 0.09997000000062647\n",
      "14600 68837 6.77 0.55 0.09997000000062647\n",
      "14700 69496 6.59 0.55 0.09997000000062647\n",
      "14800 70119 6.23 0.54 0.09997000000062647\n",
      "14900 70828 7.09 0.57 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  7.112  -- %complete  0.518\n",
      "\n",
      "15000 71717 8.89 0.46 0.09997000000062647\n",
      "15100 72338 6.21 0.6 0.09997000000062647\n",
      "15200 73110 7.72 0.46 0.09997000000062647\n",
      "15300 73747 6.37 0.55 0.09997000000062647\n",
      "15400 74417 6.7 0.51 0.09997000000062647\n",
      "15500 75011 5.94 0.59 0.09997000000062647\n",
      "15600 75782 7.71 0.54 0.09997000000062647\n",
      "15700 76486 7.04 0.52 0.09997000000062647\n",
      "15800 77310 8.24 0.46 0.09997000000062647\n",
      "15900 78111 8.01 0.49 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  7.265  -- %complete  0.518\n",
      "\n",
      "16000 78982 8.71 0.46 0.09997000000062647\n",
      "16100 79677 6.95 0.58 0.09997000000062647\n",
      "16200 80356 6.79 0.6 0.09997000000062647\n",
      "16300 81116 7.6 0.53 0.09997000000062647\n",
      "16400 81944 8.28 0.71 0.09997000000062647\n",
      "16500 82810 8.66 0.55 0.09997000000062647\n",
      "16600 83531 7.21 0.53 0.09997000000062647\n",
      "16700 84238 7.07 0.62 0.09997000000062647\n",
      "16800 84903 6.65 0.59 0.09997000000062647\n",
      "16900 85582 6.79 0.54 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  7.246  -- %complete  0.587\n",
      "\n",
      "17000 86228 6.46 0.62 0.09997000000062647\n",
      "17100 87021 7.93 0.59 0.09997000000062647\n",
      "17200 87823 8.02 0.55 0.09997000000062647\n",
      "17300 88578 7.55 0.48 0.09997000000062647\n",
      "17400 89189 6.11 0.64 0.09997000000062647\n",
      "17500 89846 6.57 0.66 0.09997000000062647\n",
      "17600 90636 7.9 0.46 0.09997000000062647\n",
      "17700 91227 5.91 0.53 0.09997000000062647\n",
      "17800 91890 6.63 0.58 0.09997000000062647\n",
      "17900 92702 8.12 0.48 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  7.189  -- %complete  0.556\n",
      "\n",
      "18000 93417 7.15 0.59 0.09997000000062647\n",
      "18100 94139 7.22 0.6 0.09997000000062647\n",
      "18200 94795 6.56 0.55 0.09997000000062647\n",
      "18300 95545 7.5 0.52 0.09997000000062647\n",
      "18400 96173 6.28 0.56 0.09997000000062647\n",
      "18500 96918 7.45 0.55 0.09997000000062647\n",
      "18600 97524 6.06 0.54 0.09997000000062647\n",
      "18700 98227 7.03 0.6 0.09997000000062647\n",
      "18800 98834 6.07 0.74 0.09997000000062647\n",
      "18900 99414 5.8 0.6 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  6.598  -- %complete  0.592\n",
      "\n",
      "19000 100015 6.01 0.66 0.09997000000062647\n",
      "19100 100612 5.97 0.59 0.09997000000062647\n",
      "19200 101293 6.81 0.53 0.09997000000062647\n",
      "19300 101905 6.12 0.61 0.09997000000062647\n",
      "19400 102451 5.46 0.65 0.09997000000062647\n",
      "19500 103001 5.5 0.69 0.09997000000062647\n",
      "19600 103627 6.26 0.5 0.09997000000062647\n",
      "19700 104314 6.87 0.52 0.09997000000062647\n",
      "19800 104980 6.66 0.5 0.09997000000062647\n",
      "19900 105542 5.62 0.48 0.09997000000062647\n",
      "Saved Model @  ./dqn/save_data/gridWorld/one_go1/model-20000.cptk\n",
      "\n",
      "last 1000 -- avg steps  6.195  -- %complete  0.555\n",
      "\n",
      "20000 106210 6.68 0.48 0.09997000000062647\n",
      "20100 106885 6.75 0.58 0.09997000000062647\n",
      "20200 107506 6.21 0.56 0.09997000000062647\n",
      "20300 108110 6.04 0.47 0.09997000000062647\n",
      "20400 108755 6.45 0.58 0.09997000000062647\n",
      "20500 109370 6.15 0.58 0.09997000000062647\n",
      "20600 109972 6.02 0.53 0.09997000000062647\n",
      "20700 110539 5.67 0.55 0.09997000000062647\n",
      "20800 111061 5.22 0.59 0.09997000000062647\n",
      "20900 111626 5.65 0.59 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  6.106  -- %complete  0.553\n",
      "\n",
      "21000 112316 6.9 0.5 0.09997000000062647\n",
      "21100 112909 5.93 0.54 0.09997000000062647\n",
      "21200 113565 6.56 0.52 0.09997000000062647\n",
      "21300 114130 5.65 0.62 0.09997000000062647\n",
      "21400 114677 5.47 0.65 0.09997000000062647\n",
      "21500 115274 5.97 0.57 0.09997000000062647\n",
      "21600 115853 5.79 0.56 0.09997000000062647\n",
      "21700 116428 5.75 0.56 0.09997000000062647\n",
      "21800 117042 6.14 0.57 0.09997000000062647\n",
      "21900 117596 5.54 0.54 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  5.862  -- %complete  0.568\n",
      "\n",
      "22000 118178 5.82 0.55 0.09997000000062647\n",
      "22100 118791 6.13 0.49 0.09997000000062647\n",
      "22200 119400 6.09 0.56 0.09997000000062647\n",
      "22300 120071 6.71 0.62 0.09997000000062647\n",
      "22400 120582 5.11 0.54 0.09997000000062647\n",
      "22500 121140 5.58 0.6 0.09997000000062647\n",
      "22600 121652 5.12 0.56 0.09997000000062647\n",
      "22700 122225 5.73 0.57 0.09997000000062647\n",
      "22800 122739 5.14 0.49 0.09997000000062647\n",
      "22900 123257 5.18 0.61 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  5.542  -- %complete  0.561\n",
      "\n",
      "23000 123720 4.63 0.57 0.09997000000062647\n",
      "23100 124327 6.07 0.55 0.09997000000062647\n",
      "23200 124860 5.33 0.7 0.09997000000062647\n",
      "23300 125450 5.9 0.59 0.09997000000062647\n",
      "23400 126032 5.82 0.62 0.09997000000062647\n",
      "23500 126624 5.92 0.57 0.09997000000062647\n",
      "23600 127215 5.91 0.57 0.09997000000062647\n",
      "23700 127749 5.34 0.6 0.09997000000062647\n",
      "23800 128346 5.97 0.54 0.09997000000062647\n",
      "23900 128841 4.95 0.65 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  5.559  -- %complete  0.597\n",
      "\n",
      "24000 129279 4.38 0.58 0.09997000000062647\n",
      "24100 129773 4.94 0.63 0.09997000000062647\n",
      "24200 130342 5.69 0.59 0.09997000000062647\n",
      "24300 130763 4.21 0.58 0.09997000000062647\n",
      "24400 131323 5.6 0.63 0.09997000000062647\n",
      "24500 131843 5.2 0.64 0.09997000000062647\n",
      "24600 132386 5.43 0.63 0.09997000000062647\n",
      "24700 133007 6.21 0.6 0.09997000000062647\n",
      "24800 133579 5.72 0.63 0.09997000000062647\n",
      "24900 134171 5.92 0.55 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  5.432  -- %complete  0.615\n",
      "\n",
      "25000 134711 5.4 0.67 0.09997000000062647\n",
      "25100 135325 6.14 0.61 0.09997000000062647\n",
      "25200 135810 4.85 0.6 0.09997000000062647\n",
      "25300 136303 4.93 0.7 0.09997000000062647\n",
      "25400 136806 5.03 0.64 0.09997000000062647\n",
      "25500 137302 4.96 0.64 0.09997000000062647\n",
      "25600 137821 5.19 0.64 0.09997000000062647\n",
      "25700 138299 4.78 0.63 0.09997000000062647\n",
      "25800 138806 5.07 0.59 0.09997000000062647\n",
      "25900 139398 5.92 0.61 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  5.171  -- %complete  0.63\n",
      "\n",
      "26000 139882 4.84 0.64 0.09997000000062647\n",
      "26100 140264 3.82 0.61 0.09997000000062647\n",
      "26200 140658 3.94 0.7 0.09997000000062647\n",
      "26300 141106 4.48 0.68 0.09997000000062647\n",
      "26400 141589 4.83 0.65 0.09997000000062647\n",
      "26500 142076 4.87 0.66 0.09997000000062647\n",
      "26600 142471 3.95 0.73 0.09997000000062647\n",
      "26700 142930 4.59 0.64 0.09997000000062647\n",
      "26800 143407 4.77 0.69 0.09997000000062647\n",
      "26900 143833 4.26 0.69 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  4.542  -- %complete  0.662\n",
      "\n",
      "27000 144424 5.91 0.57 0.09997000000062647\n",
      "27100 145044 6.2 0.67 0.09997000000062647\n",
      "27200 145537 4.93 0.63 0.09997000000062647\n",
      "27300 146096 5.59 0.69 0.09997000000062647\n",
      "27400 146576 4.8 0.59 0.09997000000062647\n",
      "27500 147060 4.84 0.67 0.09997000000062647\n",
      "27600 147628 5.68 0.65 0.09997000000062647\n",
      "27700 148205 5.77 0.76 0.09997000000062647\n",
      "27800 148643 4.38 0.78 0.09997000000062647\n",
      "27900 149178 5.35 0.7 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  5.211  -- %complete  0.682\n",
      "\n",
      "28000 149635 4.57 0.68 0.09997000000062647\n",
      "28100 150182 5.47 0.71 0.09997000000062647\n",
      "28200 150567 3.85 0.74 0.09997000000062647\n",
      "28300 151146 5.79 0.62 0.09997000000062647\n",
      "28400 151723 5.77 0.58 0.09997000000062647\n",
      "28500 152377 6.54 0.53 0.09997000000062647\n",
      "28600 152895 5.18 0.74 0.09997000000062647\n",
      "28700 153419 5.24 0.66 0.09997000000062647\n",
      "28800 153951 5.32 0.62 0.09997000000062647\n",
      "28900 154428 4.77 0.78 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  5.333  -- %complete  0.663\n",
      "\n",
      "29000 154968 5.4 0.65 0.09997000000062647\n",
      "29100 155468 5.0 0.66 0.09997000000062647\n",
      "29200 156002 5.34 0.69 0.09997000000062647\n",
      "29300 156402 4.0 0.73 0.09997000000062647\n",
      "29400 156887 4.85 0.69 0.09997000000062647\n",
      "29500 157379 4.92 0.66 0.09997000000062647\n",
      "29600 157792 4.13 0.76 0.09997000000062647\n",
      "29700 158258 4.66 0.7 0.09997000000062647\n",
      "29800 158735 4.77 0.72 0.09997000000062647\n",
      "29900 159116 3.81 0.75 0.09997000000062647\n",
      "Saved Model @  ./dqn/save_data/gridWorld/one_go1/model-30000.cptk\n",
      "\n",
      "last 1000 -- avg steps  4.628  -- %complete  0.703\n",
      "\n",
      "30000 159596 4.8 0.67 0.09997000000062647\n",
      "30100 160092 4.96 0.64 0.09997000000062647\n",
      "30200 160509 4.17 0.65 0.09997000000062647\n",
      "30300 160972 4.63 0.75 0.09997000000062647\n",
      "30400 161426 4.54 0.69 0.09997000000062647\n",
      "30500 161813 3.87 0.67 0.09997000000062647\n",
      "30600 162193 3.8 0.69 0.09997000000062647\n",
      "30700 162554 3.61 0.69 0.09997000000062647\n",
      "30800 162907 3.53 0.72 0.09997000000062647\n",
      "30900 163381 4.74 0.72 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  4.2  -- %complete  0.691\n",
      "\n",
      "31000 163796 4.15 0.69 0.09997000000062647\n",
      "31100 164251 4.55 0.8 0.09997000000062647\n",
      "31200 164678 4.27 0.66 0.09997000000062647\n",
      "31300 165151 4.73 0.66 0.09997000000062647\n",
      "31400 165565 4.14 0.74 0.09997000000062647\n",
      "31500 166082 5.17 0.73 0.09997000000062647\n",
      "31600 166578 4.96 0.66 0.09997000000062647\n",
      "31700 167017 4.39 0.7 0.09997000000062647\n",
      "31800 167428 4.11 0.8 0.09997000000062647\n",
      "31900 167902 4.74 0.71 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  4.64  -- %complete  0.713\n",
      "\n",
      "32000 168436 5.34 0.67 0.09997000000062647\n",
      "32100 168833 3.97 0.64 0.09997000000062647\n",
      "32200 169252 4.19 0.77 0.09997000000062647\n",
      "32300 169717 4.65 0.62 0.09997000000062647\n",
      "32400 170264 5.47 0.58 0.09997000000062647\n",
      "32500 170640 3.76 0.66 0.09997000000062647\n",
      "32600 171093 4.53 0.6 0.09997000000062647\n",
      "32700 171593 5.0 0.73 0.09997000000062647\n",
      "32800 172022 4.29 0.68 0.09997000000062647\n",
      "32900 172486 4.64 0.74 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  4.499  -- %complete  0.676\n",
      "\n",
      "33000 172935 4.49 0.74 0.09997000000062647\n",
      "33100 173361 4.26 0.73 0.09997000000062647\n",
      "33200 173812 4.51 0.67 0.09997000000062647\n",
      "33300 174257 4.45 0.72 0.09997000000062647\n",
      "33400 174651 3.94 0.76 0.09997000000062647\n",
      "33500 175073 4.22 0.71 0.09997000000062647\n",
      "33600 175500 4.27 0.72 0.09997000000062647\n",
      "33700 175901 4.01 0.74 0.09997000000062647\n",
      "33800 176313 4.12 0.73 0.09997000000062647\n",
      "33900 176739 4.26 0.67 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  4.206  -- %complete  0.72\n",
      "\n",
      "34000 177141 4.02 0.75 0.09997000000062647\n",
      "34100 177537 3.96 0.7 0.09997000000062647\n",
      "34200 177989 4.52 0.65 0.09997000000062647\n",
      "34300 178396 4.07 0.76 0.09997000000062647\n",
      "34400 178712 3.16 0.71 0.09997000000062647\n",
      "34500 179185 4.73 0.67 0.09997000000062647\n",
      "34600 179611 4.26 0.66 0.09997000000062647\n",
      "34700 180026 4.15 0.74 0.09997000000062647\n",
      "34800 180522 4.96 0.75 0.09997000000062647\n",
      "34900 180892 3.7 0.7 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  4.156  -- %complete  0.71\n",
      "\n",
      "35000 181297 4.05 0.76 0.09997000000062647\n",
      "35100 181719 4.22 0.77 0.09997000000062647\n",
      "35200 182153 4.34 0.76 0.09997000000062647\n",
      "35300 182486 3.33 0.65 0.09997000000062647\n",
      "35400 182826 3.4 0.72 0.09997000000062647\n",
      "35500 183156 3.3 0.68 0.09997000000062647\n",
      "35600 183599 4.43 0.78 0.09997000000062647\n",
      "35700 183961 3.62 0.74 0.09997000000062647\n",
      "35800 184354 3.93 0.75 0.09997000000062647\n",
      "35900 184748 3.94 0.75 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  3.869  -- %complete  0.729\n",
      "\n",
      "36000 185166 4.18 0.69 0.09997000000062647\n",
      "36100 185535 3.69 0.69 0.09997000000062647\n",
      "36200 185885 3.5 0.72 0.09997000000062647\n",
      "36300 186235 3.5 0.7 0.09997000000062647\n",
      "36400 186568 3.33 0.7 0.09997000000062647\n",
      "36500 186921 3.53 0.72 0.09997000000062647\n",
      "36600 187359 4.38 0.7 0.09997000000062647\n",
      "36700 187766 4.07 0.78 0.09997000000062647\n",
      "36800 188095 3.29 0.72 0.09997000000062647\n",
      "36900 188448 3.53 0.76 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  3.636  -- %complete  0.733\n",
      "\n",
      "37000 188802 3.54 0.84 0.09997000000062647\n",
      "37100 189186 3.84 0.78 0.09997000000062647\n",
      "37200 189592 4.06 0.75 0.09997000000062647\n",
      "37300 190081 4.89 0.74 0.09997000000062647\n",
      "37400 190429 3.48 0.72 0.09997000000062647\n",
      "37500 190878 4.49 0.66 0.09997000000062647\n",
      "37600 191271 3.93 0.74 0.09997000000062647\n",
      "37700 191691 4.2 0.68 0.09997000000062647\n",
      "37800 192087 3.96 0.75 0.09997000000062647\n",
      "37900 192439 3.52 0.71 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  4.005  -- %complete  0.725\n",
      "\n",
      "38000 192807 3.68 0.72 0.09997000000062647\n",
      "38100 193228 4.21 0.75 0.09997000000062647\n",
      "38200 193686 4.58 0.7 0.09997000000062647\n",
      "38300 194157 4.71 0.7 0.09997000000062647\n",
      "38400 194566 4.09 0.69 0.09997000000062647\n",
      "38500 194847 2.81 0.78 0.09997000000062647\n",
      "38600 195239 3.92 0.7 0.09997000000062647\n",
      "38700 195598 3.59 0.71 0.09997000000062647\n",
      "38800 195977 3.79 0.79 0.09997000000062647\n",
      "38900 196362 3.85 0.81 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  3.951  -- %complete  0.737\n",
      "\n",
      "39000 196758 3.96 0.74 0.09997000000062647\n",
      "39100 197103 3.45 0.7 0.09997000000062647\n",
      "39200 197480 3.77 0.75 0.09997000000062647\n",
      "39300 197861 3.81 0.73 0.09997000000062647\n",
      "39400 198232 3.71 0.78 0.09997000000062647\n",
      "39500 198656 4.24 0.72 0.09997000000062647\n",
      "39600 199017 3.61 0.68 0.09997000000062647\n",
      "39700 199411 3.94 0.75 0.09997000000062647\n",
      "39800 199786 3.75 0.69 0.09997000000062647\n",
      "39900 200168 3.82 0.76 0.09997000000062647\n",
      "Saved Model @  ./dqn/save_data/gridWorld/one_go1/model-40000.cptk\n",
      "\n",
      "last 1000 -- avg steps  3.811  -- %complete  0.729\n",
      "\n",
      "40000 200569 4.01 0.73 0.09997000000062647\n",
      "40100 200935 3.66 0.75 0.09997000000062647\n",
      "40200 201283 3.48 0.78 0.09997000000062647\n",
      "40300 201642 3.59 0.68 0.09997000000062647\n",
      "40400 201972 3.3 0.76 0.09997000000062647\n",
      "40500 202342 3.7 0.71 0.09997000000062647\n",
      "40600 202693 3.51 0.7 0.09997000000062647\n",
      "40700 203047 3.54 0.76 0.09997000000062647\n",
      "40800 203437 3.9 0.8 0.09997000000062647\n",
      "40900 203790 3.53 0.74 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  3.54  -- %complete  0.746\n",
      "\n",
      "41000 204109 3.19 0.78 0.09997000000062647\n",
      "41100 204495 3.86 0.66 0.09997000000062647\n",
      "41200 204896 4.01 0.73 0.09997000000062647\n",
      "41300 205235 3.39 0.75 0.09997000000062647\n",
      "41400 205623 3.88 0.76 0.09997000000062647\n",
      "41500 205959 3.36 0.67 0.09997000000062647\n",
      "41600 206287 3.28 0.7 0.09997000000062647\n",
      "41700 206657 3.7 0.73 0.09997000000062647\n",
      "41800 207081 4.24 0.74 0.09997000000062647\n",
      "41900 207479 3.98 0.76 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  3.723  -- %complete  0.726\n",
      "\n",
      "42000 207832 3.53 0.76 0.09997000000062647\n",
      "42100 208237 4.05 0.72 0.09997000000062647\n",
      "42200 208613 3.76 0.66 0.09997000000062647\n",
      "42300 208971 3.58 0.7 0.09997000000062647\n",
      "42400 209446 4.75 0.69 0.09997000000062647\n",
      "42500 209852 4.06 0.77 0.09997000000062647\n",
      "42600 210194 3.42 0.81 0.09997000000062647\n",
      "42700 210593 3.99 0.77 0.09997000000062647\n",
      "42800 210991 3.98 0.74 0.09997000000062647\n",
      "42900 211401 4.1 0.75 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  3.909  -- %complete  0.734\n",
      "\n",
      "43000 211741 3.4 0.73 0.09997000000062647\n",
      "43100 212127 3.86 0.68 0.09997000000062647\n",
      "43200 212509 3.82 0.69 0.09997000000062647\n",
      "43300 212805 2.96 0.72 0.09997000000062647\n",
      "43400 213160 3.55 0.79 0.09997000000062647\n",
      "43500 213511 3.51 0.72 0.09997000000062647\n",
      "43600 213861 3.5 0.67 0.09997000000062647\n",
      "43700 214163 3.02 0.78 0.09997000000062647\n",
      "43800 214633 4.7 0.73 0.09997000000062647\n",
      "43900 215016 3.83 0.79 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  3.669  -- %complete  0.726\n",
      "\n",
      "44000 215410 3.94 0.69 0.09997000000062647\n",
      "44100 215739 3.29 0.75 0.09997000000062647\n",
      "44200 216094 3.55 0.73 0.09997000000062647\n",
      "44300 216381 2.87 0.72 0.09997000000062647\n",
      "44400 216738 3.57 0.76 0.09997000000062647\n",
      "44500 217072 3.34 0.77 0.09997000000062647\n",
      "44600 217431 3.59 0.76 0.09997000000062647\n",
      "44700 217779 3.48 0.78 0.09997000000062647\n",
      "44800 218130 3.51 0.8 0.09997000000062647\n",
      "44900 218514 3.84 0.72 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  3.481  -- %complete  0.759\n",
      "\n",
      "45000 218891 3.77 0.8 0.09997000000062647\n",
      "45100 219187 2.96 0.7 0.09997000000062647\n",
      "45200 219598 4.11 0.8 0.09997000000062647\n",
      "45300 219951 3.53 0.82 0.09997000000062647\n",
      "45400 220320 3.69 0.7 0.09997000000062647\n",
      "45500 220772 4.52 0.66 0.09997000000062647\n",
      "45600 221153 3.81 0.68 0.09997000000062647\n",
      "45700 221452 2.99 0.78 0.09997000000062647\n",
      "45800 221782 3.3 0.78 0.09997000000062647\n",
      "45900 222102 3.2 0.8 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  3.585  -- %complete  0.746\n",
      "\n",
      "46000 222476 3.74 0.74 0.09997000000062647\n",
      "46100 222803 3.27 0.74 0.09997000000062647\n",
      "46200 223134 3.31 0.85 0.09997000000062647\n",
      "46300 223542 4.08 0.8 0.09997000000062647\n",
      "46400 223895 3.53 0.83 0.09997000000062647\n",
      "46500 224266 3.71 0.71 0.09997000000062647\n",
      "46600 224581 3.15 0.75 0.09997000000062647\n",
      "46700 224956 3.75 0.77 0.09997000000062647\n",
      "46800 225266 3.1 0.75 0.09997000000062647\n",
      "46900 225655 3.89 0.81 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  3.543  -- %complete  0.777\n",
      "\n",
      "47000 226019 3.64 0.76 0.09997000000062647\n",
      "47100 226452 4.33 0.87 0.09997000000062647\n",
      "47200 226798 3.46 0.74 0.09997000000062647\n",
      "47300 227158 3.6 0.78 0.09997000000062647\n",
      "47400 227463 3.05 0.71 0.09997000000062647\n",
      "47500 227832 3.69 0.86 0.09997000000062647\n",
      "47600 228169 3.37 0.79 0.09997000000062647\n",
      "47700 228516 3.47 0.8 0.09997000000062647\n",
      "47800 228877 3.61 0.73 0.09997000000062647\n",
      "47900 229196 3.19 0.83 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  3.562  -- %complete  0.787\n",
      "\n",
      "48000 229581 3.85 0.76 0.09997000000062647\n",
      "48100 229918 3.37 0.79 0.09997000000062647\n",
      "48200 230298 3.8 0.78 0.09997000000062647\n",
      "48300 230659 3.61 0.75 0.09997000000062647\n",
      "48400 230991 3.32 0.76 0.09997000000062647\n",
      "48500 231359 3.68 0.79 0.09997000000062647\n",
      "48600 231735 3.76 0.75 0.09997000000062647\n",
      "48700 232072 3.37 0.79 0.09997000000062647\n",
      "48800 232455 3.83 0.78 0.09997000000062647\n",
      "48900 232867 4.12 0.82 0.09997000000062647\n",
      "\n",
      "last 1000 -- avg steps  3.71  -- %complete  0.777\n",
      "\n",
      "49000 233291 4.24 0.76 0.09997000000062647\n",
      "49100 233630 3.39 0.75 0.09997000000062647\n",
      "49200 233995 3.65 0.76 0.09997000000062647\n",
      "49300 234370 3.75 0.81 0.09997000000062647\n",
      "49400 234790 4.2 0.73 0.09997000000062647\n",
      "49500 235125 3.35 0.74 0.09997000000062647\n",
      "49600 235468 3.43 0.83 0.09997000000062647\n",
      "49700 235783 3.15 0.74 0.09997000000062647\n",
      "49800 236114 3.31 0.85 0.09997000000062647\n",
      "49900 236481 3.67 0.79 0.09997000000062647\n",
      "Saved Model @  ./dqn/save_data/gridWorld/one_go1/model-50000.cptk\n",
      "\n",
      "last 1000 -- avg steps  3.548  -- %complete  0.775\n",
      "\n",
      "50000 236839 3.58 0.75 0.09997000000062647\n",
      "Percent of succesful episodes: 6.039737615743055%\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cProfile.run(\"run()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Tensor name \"beta1_power_3\" not found in checkpoint files ./dqn/save_data/gridWorld/one_go1/model-50000.cptk\n\t [[Node: save_4/restore_slice_183 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_4/Const_0, save_4/restore_slice_183/tensor_name, save_4/restore_slice_183/shape_and_slice)]]\nCaused by op 'save_4/restore_slice_183', defined at:\n  File \"/opt/anaconda3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/opt/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/opt/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/opt/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/opt/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/opt/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-20-e07bbc3edf45>\", line 8, in <module>\n    saver = tf.train.Saver()\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 845, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 515, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 271, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 186, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/io_ops.py\", line 202, in _restore_slice\n    preferred_shard, name=name)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 358, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 715\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    716\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    449\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    451\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Tensor name \"beta1_power_3\" not found in checkpoint files ./dqn/save_data/gridWorld/one_go1/model-50000.cptk\n\t [[Node: save_4/restore_slice_183 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_4/Const_0, save_4/restore_slice_183/tensor_name, save_4/restore_slice_183/shape_and_slice)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-e07bbc3edf45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./dqn/save_data/gridWorld/one_go1/model-50000.cptk'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mcomplete\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1103\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restore called with invalid save path %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1105\u001b[1;33m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 372\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    373\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 636\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    637\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 708\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    709\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Tensor name \"beta1_power_3\" not found in checkpoint files ./dqn/save_data/gridWorld/one_go1/model-50000.cptk\n\t [[Node: save_4/restore_slice_183 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_4/Const_0, save_4/restore_slice_183/tensor_name, save_4/restore_slice_183/shape_and_slice)]]\nCaused by op 'save_4/restore_slice_183', defined at:\n  File \"/opt/anaconda3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/opt/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/opt/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/opt/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/opt/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/opt/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-20-e07bbc3edf45>\", line 8, in <module>\n    saver = tf.train.Saver()\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 845, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 515, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 271, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 186, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/io_ops.py\", line 202, in _restore_slice\n    preferred_shard, name=name)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 358, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "directions = ['up', 'down', 'left', 'right']\n",
    "arr = []\n",
    "\n",
    "mainQN = Qnetwork(h_size, previous_states)\n",
    "targetQN = Qnetwork(h_size, previous_states)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    load = './dqn/save_data/gridWorld/one_go1/model-50000.cptk'\n",
    "    saver.restore(sess, load)\n",
    "    for i in range(20):\n",
    "        complete = False\n",
    "        iter = 0\n",
    "        env = gameEnv(partial=False,size=5)\n",
    "        s = env.reset()\n",
    "        s = sess.run(mainQN.resized_image, feed_dict={mainQN.image_to_resize:s})[0]\n",
    "        plt.imshow(env.a)\n",
    "        r_all = 0\n",
    "        while(complete == False):\n",
    "            \n",
    "            action = sess.run(mainQN.predict, feed_dict={mainQN.imageIn:[s]})\n",
    "            s, reward, done = env.step(action)\n",
    "            s = sess.run(mainQN.resized_image, feed_dict={mainQN.image_to_resize:s})[0]\n",
    "            plt.imshow(env.a)\n",
    "#             display.clear_output(wait=True)\n",
    "#             display.display(plt.gcf())\n",
    "            r_all += reward\n",
    "            if(iter > 20):\n",
    "                done = True\n",
    "            iter += 1\n",
    "            if(done):\n",
    "                arr.append(r_all)\n",
    "                complete = True\n",
    "                print(iter)\n",
    "                print(\"Complete\")\n",
    "                print(\"reward: \", r_all)\n",
    "#             time.sleep(.5)\n",
    "            plt.show()\n",
    "\n",
    "(len([k for k in arr[-100:] if k > 0]) / len(arr[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
