{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "from IPython import display\n",
    "import time\n",
    "import math\n",
    "import cProfile\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gameEnv():\n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.actions = 4\n",
    "        self.a = []\n",
    "        self.rendered = []\n",
    "        self.currReward = 0\n",
    "        self.cumReward = 0\n",
    "        self.reset()\n",
    "        self.won = False\n",
    "    \n",
    "    def reset(self):\n",
    "        arr = np.array([0 for i in range(16)])\n",
    "        self.a = arr.reshape([4,4])\n",
    "        self.addRandomOpen()\n",
    "        self.renderEnv()\n",
    "        self.currReward = 0\n",
    "        self.cumReward = 0\n",
    "        return self.a\n",
    "        \n",
    "    def addRandomOpen(self):\n",
    "        indices = np.asarray(np.where(self.a == 0)).T\n",
    "        choice = random.choice(indices)\n",
    "        self.a[choice[0]][choice[1]] = 2\n",
    "    \n",
    "    def renderEnv(self):\n",
    "        arr = np.ones([4,4])\n",
    "        for i in range(16):\n",
    "            arr[i//4][i%4] = self.getColor(self.a[i//4][i%4])\n",
    "        return scipy.misc.imresize(arr[:,:],[84,84,1],interp='nearest')\n",
    "        \n",
    "    def getColor(self, val):\n",
    "        if(val == 0):\n",
    "            return 0\n",
    "        return math.log2(val) * 23\n",
    "    \n",
    "    def checkEnd(self):\n",
    "        if(len(np.where(self.a == 0)[0]) == 0):\n",
    "            return True\n",
    "        if(len(np.where(self.a == 2048)[0]) != 0):\n",
    "            if(self.won == False):\n",
    "                print(\"won the game!\")\n",
    "                self.won = True\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    # 0 - up, 1 - down, 2 - left, 3 - right\n",
    "    def move(self, action):\n",
    "        if(action == 0):\n",
    "            self.a = np.rot90(self.a,1)\n",
    "            self.a = self.mergeLeft(self.a)\n",
    "            self.a = np.rot90(self.a,3)\n",
    "        elif(action ==1):\n",
    "            self.a = np.rot90(self.a,3)\n",
    "            self.a = self.mergeLeft(self.a)\n",
    "            self.a = np.rot90(self.a,1)\n",
    "        elif(action == 2):\n",
    "            self.a = self.mergeLeft(self.a)\n",
    "        elif(action == 3):\n",
    "            self.a = np.rot90(self.a,2)\n",
    "            self.a = self.mergeLeft(self.a)\n",
    "            self.a = np.rot90(self.a,2)        \n",
    "        \n",
    "        self.addRandomOpen()\n",
    "        return self.checkEnd()\n",
    "                        \n",
    "    def step(self, action):\n",
    "        done = self.move(action)\n",
    "        return self.renderEnv(),self.currReward,done\n",
    "        \n",
    "    def mergeLeft(self, arr):\n",
    "        self.currReward = 0\n",
    "        for i in range(4):\n",
    "            for j in range(1, 4):\n",
    "                stop = 0\n",
    "                if arr[i][j] != 0:\n",
    "                    for k in range(stop, j):\n",
    "                        if(arr[i][k] == arr[i][j]):\n",
    "                            arr[i][k] = arr[i][k] * 2\n",
    "                            arr[i][j] = 0\n",
    "                            self.currReward += arr[i][k]\n",
    "                            stop = j\n",
    "                            break\n",
    "                        elif(arr[i][k] == 0):\n",
    "                            arr[i][k] = arr[i][j]\n",
    "                            arr[i][j] = 0\n",
    "                            break\n",
    "                            \n",
    "        self.cumReward += self.currReward\n",
    "        return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size,prev_states):        \n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.imageIn = tf.placeholder(shape=[None,84,84,1],dtype=tf.float32)\n",
    "        self.conv1 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv3,num_outputs=512,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        #We take the output from the final convolutional layer and split it into separate advantage and value streams.\n",
    "        self.streamAC,self.streamVC = tf.split(3,2,self.conv4)\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2,env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.sub(self.Advantage,tf.reduce_mean(self.Advantage,reduction_indices=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.mul(self.Qout, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    \"\"\"Used to store experiences and samples randomly to train the network.\"\"\"\n",
    "    def __init__(self, buffer_size=50000):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        \n",
    "    def add(self, states, actions, rewards, dones):\n",
    "        if len(self.actions) == self.buffer_size:\n",
    "            self.states = self.states[1:]\n",
    "            self.actions = self.actions[1:]\n",
    "            self.rewards = self.rewards[1:]\n",
    "            self.dones = self.dones[1:]\n",
    "\n",
    "        self.states.append(states)\n",
    "        self.actions.append(actions)\n",
    "        self.rewards.append(rewards)\n",
    "        self.dones.append(dones)\n",
    "        \n",
    "    def sample(self, size, previous_states):\n",
    "        samples = np.random.permutation(len(self.actions)-(previous_states-1)) + (previous_states-1)\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        states_ = []\n",
    "        dones = []\n",
    "        for i in samples[:size]:\n",
    "            temp = []\n",
    "            for j in range(previous_states):\n",
    "                temp.append(self.states[i - previous_states + j + 1])\n",
    "            states.append(np.dstack(temp))\n",
    "            actions.append(self.actions[i])\n",
    "            rewards.append(self.rewards[i])\n",
    "            states_.append(self.states[i+1])\n",
    "            dones.append(self.dones[i])\n",
    "                \n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 8 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "anneling_steps = 1 #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 100003 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 20000 #How many steps of random actions before training begins.\n",
    "pre_train_steps_from_Q = False #If true, initialize buffer with steps from Q instead of random actions\n",
    "max_epLength = 5000 #The max allowed length of our episode.\n",
    "load_model = True #Whether to load a saved model.\n",
    "path = \"./dqn/save_data/2048/1/\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.0001 #Rate to update target network toward primary network\n",
    "previous_states=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gameEnv()\n",
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size,previous_states)\n",
    "targetQN = Qnetwork(h_size,previous_states)\n",
    "sess = tf.Session()\n",
    "load = './Deep-Reinforcement-Learning-Project/dqn/save_data/2048/1/model.cptk'\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess,load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3b1c1494a8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD/CAYAAADRymv0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlxpk8KUG/T+y3/ub2bE/BwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3b1c0af1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.renderEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD/CAYAAADRymv0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABPhJREFUeJzt2sFKG10cxuF/QmJipRMrGrAuunfZhXj/l1BX7ootGBAE\ng1qKSee7ge7i5NDvfZ7l2cy7+XGYYUZ93xeQZdx6ALB/wodAwodAwodAwodAwodAwodAwodAwodA\nwodAk6EfMBqN/rf/BJ+fn9eXL1/q8+fPracM4vLysr5+/VqXl5etpwxqPB7XYrGorutqPp+3nvPe\nRn87dONDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFD\nIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFD\nIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDoEnrAf+y2WxW\nXdfV6elp6ymD6LquZrNZ6xmD2263tVqt6sePH7XdblvPeVdXV1d/PRf+DubzeZ2cnNT5+XnrKYP4\n9OlTRPhvb2/18+fPuru7q6enp9Zz3pXwBzCbzWqxWNRyuWw9ZRCLxSIi/O12W/f393Vzc1Or1ar1\nnL3wjg+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+B\nhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+B\nhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BJq0H/Muenp7q\n+/fvrWcMpuu6Ojk5qa7rWk8Z1Ha7rcfHx5pOp3V6etp6zl4IfwePj491e3tbq9Wq9ZRBHBwc1Hw+\nr9ls1nrKoCaTSS2Xy1oul3V8fNx6zl4Ifwfr9brW63XrGezo8PCwrq+v6+LiopbLZes5e+EdHwIJ\nHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJ\nHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJ\nHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwJNWg/4l41GoxqP\nxzUajVpPGUTf9/Xnz5/q+771lMFtNpv6/ft3/fr1q/WUvRD+Dj58+FAfP36so6Oj1lMG8fLyUuv1\nul5fX1tPGdRms6nValXfvn2ru7u71nP2Qvg7ODw8rLOzszo7O2s9ZRAPDw+12Wwiwr+/v6/1el3T\n6bT1nL0Q/g6m02kdHR3V8fFx6ymDeH19jQih7/t6fn6u5+fn1lP2xsc9CCR8CCR8CCR8CCR8CCR8\nCCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8\nCCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8\nCCR8CCR8CCR8CCR8CCR8CCR8CCR8CDTq+771BmDP3PgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQ\nSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQ6D/Yt5BYinPHLwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3adc095588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-667ccc72cde5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0miter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mcomplete\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'arr' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD/CAYAAADRymv0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABPhJREFUeJzt2sFKG10cxuF/QmJipRMrGrAuunfZhXj/l1BX7ootGBAE\ng1qKSee7ge7i5NDvfZ7l2cy7+XGYYUZ93xeQZdx6ALB/wodAwodAwodAwodAwodAwodAwodAwodA\nwodAk6EfMBqN/rf/BJ+fn9eXL1/q8+fPracM4vLysr5+/VqXl5etpwxqPB7XYrGorutqPp+3nvPe\nRn87dONDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFD\nIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFD\nIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDIOFDoEnrAf+y2WxW\nXdfV6elp6ymD6LquZrNZ6xmD2263tVqt6sePH7XdblvPeVdXV1d/PRf+DubzeZ2cnNT5+XnrKYP4\n9OlTRPhvb2/18+fPuru7q6enp9Zz3pXwBzCbzWqxWNRyuWw9ZRCLxSIi/O12W/f393Vzc1Or1ar1\nnL3wjg+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+B\nhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+B\nhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BhA+BJq0H/Muenp7q\n+/fvrWcMpuu6Ojk5qa7rWk8Z1Ha7rcfHx5pOp3V6etp6zl4IfwePj491e3tbq9Wq9ZRBHBwc1Hw+\nr9ls1nrKoCaTSS2Xy1oul3V8fNx6zl4Ifwfr9brW63XrGezo8PCwrq+v6+LiopbLZes5e+EdHwIJ\nHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJ\nHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJ\nHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwIJHwJNWg/4l41GoxqP\nxzUajVpPGUTf9/Xnz5/q+771lMFtNpv6/ft3/fr1q/WUvRD+Dj58+FAfP36so6Oj1lMG8fLyUuv1\nul5fX1tPGdRms6nValXfvn2ru7u71nP2Qvg7ODw8rLOzszo7O2s9ZRAPDw+12Wwiwr+/v6/1el3T\n6bT1nL0Q/g6m02kdHR3V8fFx6ymDeH19jQih7/t6fn6u5+fn1lP2xsc9CCR8CCR8CCR8CCR8CCR8\nCCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8\nCCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8CCR8\nCCR8CCR8CCR8CCR8CCR8CCR8CCR8CDTq+771BmDP3PgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQ\nSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQ6D/Yt5BYinPHLwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3adc095588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    complete = False\n",
    "    iter = 0\n",
    "    env = gameEnv()\n",
    "    plt.imshow(env.renderEnv())\n",
    "    r_all = 0\n",
    "    while(complete == False):\n",
    "        action = sess.run(mainQN.predict, feed_dict={mainQN.imageIn:[env.renderEnv().reshape([84, 84, 1])]})\n",
    "        observation, reward, done = env.step(action)\n",
    "        plt.imshow(env.renderEnv(),cmap='Greys_r')\n",
    "        plt.axis('off')\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        r_all += reward\n",
    "        if(iter > 20):\n",
    "            done = True\n",
    "        iter += 1\n",
    "        if(done):\n",
    "            arr.append(r_all)\n",
    "            complete = True\n",
    "            print(iter)\n",
    "            print(\"Complete\")\n",
    "            print(\"reward: \", r_all)\n",
    "        time.sleep(2)\n",
    "#           plt.show()\n",
    "    print(\"\\n \\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
