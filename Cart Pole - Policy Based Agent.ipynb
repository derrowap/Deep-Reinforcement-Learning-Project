{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole - Policy Based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the CartPole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-15 15:43:13,029] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for this episode was: 50.000000\n",
      "Reward for this episode was: 20.000000\n",
      "Reward for this episode was: 22.000000\n",
      "Reward for this episode was: 26.000000\n",
      "Reward for this episode was: 16.000000\n",
      "Reward for this episode was: 15.000000\n",
      "Reward for this episode was: 17.000000\n",
      "Reward for this episode was: 20.000000\n",
      "Reward for this episode was: 15.000000\n",
      "Reward for this episode was: 28.000000\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "random_episodes = 0\n",
    "reward_sum = 0\n",
    "while random_episodes < 10:\n",
    "    env.render()\n",
    "    observation, reward, done, _ = env.step(np.random.randint(0, 2))\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        random_episodes += 1\n",
    "        print(\"Reward for this episode was: %f\" % reward_sum)\n",
    "        reward_sum = 0\n",
    "        env.reset()\n",
    "        \n",
    "# close window\n",
    "env.viewer.close()\n",
    "\n",
    "# have to do this or next time you try to render, it will throw error\n",
    "env.viewer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 10 # number of hidden layer neurons\n",
    "batch_size = 50 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-2 # feel free to play with this to train faster or more stably.\n",
    "gamma = 0.99 # discount factor for reward\n",
    "\n",
    "D = 4 # input dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#This defines the network as it goes from taking an observation of the environment to \n",
    "#giving a probability of chosing to the action of moving left or right.\n",
    "observations = tf.placeholder(tf.float32, [None, D], name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[D, H],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "score = tf.matmul(layer1,W2)\n",
    "\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "#From here we define the parts of the network needed for learning a good policy.\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32, [None, 1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32, name=\"reward_signal\")\n",
    "\n",
    "# The loss function. This sends the weights in the direction of making actions \n",
    "# that gave good advantage (reward over time) more likely, and actions that didn't less likely.\n",
    "loss = -tf.reduce_mean((tf.log(input_y - probability)) * advantages) \n",
    "newGrads = tf.gradients(loss, tvars)\n",
    "\n",
    "# Once we have collected a series of gradients from multiple episodes, we apply them.\n",
    "# We don't just apply gradeients after every episode in order to account for noise in the reward signal.\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate) # Our optimizer\n",
    "W1Grad = tf.placeholder(tf.float32, name=\"batch_grad1\") # Placeholders to send the final gradients through when we update.\n",
    "W2Grad = tf.placeholder(tf.float32, name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad, W2Grad]\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantage function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Agent on the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 19.260000.  Total average reward 19.260000.\n",
      "Average reward for episode 19.120000.  Total average reward 19.258600.\n",
      "Average reward for episode 28.540000.  Total average reward 19.351414.\n",
      "Average reward for episode 21.320000.  Total average reward 19.371100.\n",
      "Average reward for episode 20.460000.  Total average reward 19.381989.\n",
      "Average reward for episode 20.600000.  Total average reward 19.394169.\n",
      "Average reward for episode 23.440000.  Total average reward 19.434627.\n",
      "Average reward for episode 24.000000.  Total average reward 19.480281.\n",
      "Average reward for episode 23.440000.  Total average reward 19.519878.\n",
      "Average reward for episode 22.480000.  Total average reward 19.549479.\n",
      "Average reward for episode 28.560000.  Total average reward 19.639585.\n",
      "Average reward for episode 22.160000.  Total average reward 19.664789.\n",
      "Average reward for episode 27.240000.  Total average reward 19.740541.\n",
      "Average reward for episode 24.740000.  Total average reward 19.790535.\n",
      "Average reward for episode 22.420000.  Total average reward 19.816830.\n",
      "Average reward for episode 24.340000.  Total average reward 19.862062.\n",
      "Average reward for episode 26.260000.  Total average reward 19.926041.\n",
      "Average reward for episode 26.300000.  Total average reward 19.989781.\n",
      "Average reward for episode 30.340000.  Total average reward 20.093283.\n",
      "Average reward for episode 28.440000.  Total average reward 20.176750.\n",
      "Average reward for episode 28.980000.  Total average reward 20.264783.\n",
      "Average reward for episode 27.560000.  Total average reward 20.337735.\n",
      "Average reward for episode 30.880000.  Total average reward 20.443157.\n",
      "Average reward for episode 29.940000.  Total average reward 20.538126.\n",
      "Average reward for episode 31.320000.  Total average reward 20.645945.\n",
      "Average reward for episode 29.160000.  Total average reward 20.731085.\n",
      "Average reward for episode 31.260000.  Total average reward 20.836374.\n",
      "Average reward for episode 35.220000.  Total average reward 20.980211.\n",
      "Average reward for episode 34.180000.  Total average reward 21.112208.\n",
      "Average reward for episode 34.280000.  Total average reward 21.243886.\n",
      "Average reward for episode 38.400000.  Total average reward 21.415448.\n",
      "Average reward for episode 41.580000.  Total average reward 21.617093.\n",
      "Average reward for episode 42.720000.  Total average reward 21.828122.\n",
      "Average reward for episode 42.300000.  Total average reward 22.032841.\n",
      "Average reward for episode 40.040000.  Total average reward 22.212913.\n",
      "Average reward for episode 42.340000.  Total average reward 22.414183.\n",
      "Average reward for episode 43.680000.  Total average reward 22.626842.\n",
      "Average reward for episode 48.020000.  Total average reward 22.880773.\n",
      "Average reward for episode 53.300000.  Total average reward 23.184965.\n",
      "Average reward for episode 48.740000.  Total average reward 23.440516.\n",
      "Average reward for episode 54.140000.  Total average reward 23.747511.\n",
      "Average reward for episode 49.020000.  Total average reward 24.000235.\n",
      "Average reward for episode 41.200000.  Total average reward 24.172233.\n",
      "Average reward for episode 53.520000.  Total average reward 24.465711.\n",
      "Average reward for episode 57.120000.  Total average reward 24.792254.\n",
      "Average reward for episode 57.340000.  Total average reward 25.117731.\n",
      "Average reward for episode 57.740000.  Total average reward 25.443954.\n",
      "Average reward for episode 59.700000.  Total average reward 25.786514.\n",
      "Average reward for episode 55.420000.  Total average reward 26.082849.\n",
      "Average reward for episode 56.460000.  Total average reward 26.386621.\n",
      "Average reward for episode 63.880000.  Total average reward 26.761554.\n",
      "Average reward for episode 65.040000.  Total average reward 27.144339.\n",
      "Average reward for episode 62.800000.  Total average reward 27.500896.\n",
      "Average reward for episode 54.780000.  Total average reward 27.773687.\n",
      "Average reward for episode 71.260000.  Total average reward 28.208550.\n",
      "Average reward for episode 66.240000.  Total average reward 28.588864.\n",
      "Average reward for episode 73.360000.  Total average reward 29.036576.\n",
      "Average reward for episode 71.300000.  Total average reward 29.459210.\n",
      "Average reward for episode 82.620000.  Total average reward 29.990818.\n",
      "Average reward for episode 79.660000.  Total average reward 30.487510.\n",
      "Average reward for episode 81.500000.  Total average reward 30.997634.\n",
      "Average reward for episode 75.560000.  Total average reward 31.443258.\n",
      "Average reward for episode 83.380000.  Total average reward 31.962626.\n",
      "Average reward for episode 79.820000.  Total average reward 32.441199.\n",
      "Average reward for episode 88.240000.  Total average reward 32.999187.\n",
      "Average reward for episode 82.020000.  Total average reward 33.489395.\n",
      "Average reward for episode 95.780000.  Total average reward 34.112301.\n",
      "Average reward for episode 113.020000.  Total average reward 34.901378.\n",
      "Average reward for episode 112.440000.  Total average reward 35.676765.\n",
      "Average reward for episode 118.180000.  Total average reward 36.501797.\n",
      "Average reward for episode 127.420000.  Total average reward 37.410979.\n",
      "Average reward for episode 153.020000.  Total average reward 38.567069.\n",
      "Average reward for episode 157.340000.  Total average reward 39.754799.\n",
      "Average reward for episode 165.280000.  Total average reward 41.010051.\n",
      "Average reward for episode 171.640000.  Total average reward 42.316350.\n",
      "Average reward for episode 174.060000.  Total average reward 43.633787.\n",
      "Average reward for episode 174.100000.  Total average reward 44.938449.\n",
      "Average reward for episode 170.680000.  Total average reward 46.195864.\n",
      "Average reward for episode 222.740000.  Total average reward 47.961306.\n",
      "Average reward for episode 188.700000.  Total average reward 49.368693.\n",
      "Average reward for episode 220.420000.  Total average reward 51.079206.\n",
      "Average reward for episode 189.660000.  Total average reward 52.465014.\n",
      "Average reward for episode 212.160000.  Total average reward 54.061963.\n",
      "Average reward for episode 201.780000.  Total average reward 55.539144.\n",
      "Average reward for episode 233.120000.  Total average reward 57.314952.\n",
      "Average reward for episode 240.620000.  Total average reward 59.148003.\n",
      "Average reward for episode 252.960000.  Total average reward 61.086123.\n",
      "Average reward for episode 211.040000.  Total average reward 62.585662.\n",
      "Average reward for episode 261.120000.  Total average reward 64.571005.\n",
      "Average reward for episode 263.060000.  Total average reward 66.555895.\n",
      "Average reward for episode 232.300000.  Total average reward 68.213336.\n",
      "Average reward for episode 267.600000.  Total average reward 70.207203.\n",
      "Average reward for episode 273.600000.  Total average reward 72.241131.\n",
      "Average reward for episode 271.200000.  Total average reward 74.230719.\n",
      "Average reward for episode 277.280000.  Total average reward 76.261212.\n",
      "Average reward for episode 294.380000.  Total average reward 78.442400.\n",
      "Average reward for episode 294.860000.  Total average reward 80.606576.\n",
      "Average reward for episode 255.940000.  Total average reward 82.359910.\n",
      "Average reward for episode 304.440000.  Total average reward 84.580711.\n",
      "Average reward for episode 292.280000.  Total average reward 86.657704.\n",
      "Average reward for episode 294.540000.  Total average reward 88.736527.\n",
      "Average reward for episode 331.440000.  Total average reward 91.163562.\n",
      "Average reward for episode 360.120000.  Total average reward 93.853126.\n",
      "Average reward for episode 346.700000.  Total average reward 96.381595.\n",
      "Average reward for episode 349.680000.  Total average reward 98.914579.\n",
      "Average reward for episode 401.020000.  Total average reward 101.935633.\n",
      "Average reward for episode 416.380000.  Total average reward 105.080077.\n",
      "Average reward for episode 403.780000.  Total average reward 108.067076.\n",
      "Average reward for episode 394.420000.  Total average reward 110.930605.\n",
      "Average reward for episode 385.120000.  Total average reward 113.672499.\n",
      "Average reward for episode 457.400000.  Total average reward 117.109774.\n",
      "Average reward for episode 394.880000.  Total average reward 119.887476.\n",
      "Average reward for episode 428.020000.  Total average reward 122.968802.\n",
      "Average reward for episode 465.860000.  Total average reward 126.397714.\n",
      "Average reward for episode 433.180000.  Total average reward 129.465536.\n",
      "Average reward for episode 466.480000.  Total average reward 132.835681.\n",
      "Average reward for episode 458.960000.  Total average reward 136.096924.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-15 15:48:02,568] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.0.6521.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 539.840000.  Total average reward 140.134355.\n",
      "Task solved in 5900 episodes!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-15 15:48:09,961] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.1.6521.video000001.mp4\n",
      "[2016-11-15 15:48:34,148] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.8.6521.video000008.mp4\n",
      "[2016-11-15 15:49:43,197] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.27.6521.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 191.480000.  Total average reward 140.647812.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-15 15:51:51,808] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.64.6521.video000064.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-195e45b55063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#         if reward_sum/batch_size > 180 or rendering == True :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrendering\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Make sure the observation is in a shape the network can handle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/derrowap/anaconda3/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/derrowap/anaconda3/lib/python3.5/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/derrowap/anaconda3/lib/python3.5/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/derrowap/anaconda3/lib/python3.5/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;31m# TODO canvas.flip?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/derrowap/anaconda3/lib/python3.5/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vsync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_vsync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0mglx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXSwapBuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglx_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mXlibContextARB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibContext13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "total_episodes = 50000\n",
    "init = tf.initialize_all_variables()\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    finished = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset() # Obtain an initial observation of the environment\n",
    "\n",
    "    # Reset the gradient placeholder. We will collect gradients in \n",
    "    # gradBuffer until we are ready to update our policy network. \n",
    "    gradBuffer = sess.run(tvars)\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    \n",
    "    while episode_number <= total_episodes:\n",
    "        \n",
    "        # Rendering the environment slows things down, \n",
    "        # so let's only look at it once our agent is doing a good job.\n",
    "#         if reward_sum/batch_size > 180 or rendering == True :\n",
    "        if rendering == True :\n",
    "            env.render()\n",
    "            \n",
    "        # Make sure the observation is in a shape the network can handle.\n",
    "        x = np.reshape(observation, [1, D])\n",
    "        \n",
    "        # Run the policy network and get an action to take. \n",
    "        tfprob = sess.run(probability, feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "        \n",
    "        xs.append(x) # observation\n",
    "        y = 1 if action == 0 else 0 # a \"fake label\"\n",
    "        ys.append(y)\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done: \n",
    "            episode_number += 1\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            tfp = tfps\n",
    "            xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[] # reset array memory\n",
    "\n",
    "            # compute the discounted reward backwards through time\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            # size the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr /= np.std(discounted_epr)\n",
    "            \n",
    "            # Get the gradient for this episode, and save it in the gradBuffer\n",
    "            tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "            for ix,grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "                \n",
    "            # If we have completed enough episodes, then update the policy network with our gradients.\n",
    "            if episode_number % batch_size == 0: \n",
    "                sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                for ix,grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                # Give a summary of how well our network is doing for each batch of episodes.\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                print('Average reward for episode %f.  Total average reward %f.' % (reward_sum/batch_size, running_reward/batch_size))\n",
    "                \n",
    "                if reward_sum/batch_size > 500: \n",
    "                    print(\"Task solved in\", episode_number, 'episodes!')\n",
    "                    if (finished):\n",
    "                        break\n",
    "                    finished = True\n",
    "                    rendering = True\n",
    "                    \n",
    "                reward_sum = 0\n",
    "            \n",
    "            if rendering:\n",
    "                env.monitor.start('/tmp/cartpole-experiment-1', force=False)\n",
    "            \n",
    "            observation = env.reset()\n",
    "        \n",
    "env.monitor.close()\n",
    "env.viewer.close()\n",
    "env.viewer = None\n",
    "print(episode_number, 'Episodes completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
