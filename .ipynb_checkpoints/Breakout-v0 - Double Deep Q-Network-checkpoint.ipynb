{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout-v0 - Double Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Breakout-v0 Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self, h_size, frameShape, batch_size):\n",
    "        # The network recieves a frame from the game, flattened into an array.\n",
    "        # It then resizes it and processes it through four convolutional layers.\n",
    "        \n",
    "        # raw pixel data (grayscale, so only 1 channel)\n",
    "        self.rgb_array = tf.placeholder(shape=[None, frameShape[0], frameShape[1], 1], dtype=tf.float32)\n",
    "        \n",
    "        # tf input: a 4-D tensor [batch_size, height, width, channels]\n",
    "        self.imageIn = tf.div(tf.image.resize_images(self.rgb_array, [84, 84]), 225)\n",
    "        \n",
    "        self.conv1 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.imageIn,\n",
    "            num_outputs = 32,\n",
    "            kernel_size = [8, 8],\n",
    "            stride = [4, 4],\n",
    "            padding = 'VALID',\n",
    "            activation_fn = tf.nn.relu)\n",
    "        \n",
    "        self.conv2 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv1,\n",
    "            num_outputs = 64,\n",
    "            kernel_size = [4, 4],\n",
    "            stride = [2, 2],\n",
    "            padding = 'VALID',\n",
    "            activation_fn = tf.nn.relu)\n",
    "        \n",
    "        self.conv3 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv2,\n",
    "            num_outputs = 64,\n",
    "            kernel_size = [3, 3],\n",
    "            stride = [1, 1],\n",
    "            padding = 'VALID',\n",
    "            activation_fn = tf.nn.relu)\n",
    "        \n",
    "        self.conv4 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv3,\n",
    "            num_outputs = 512,\n",
    "            kernel_size = [7, 7],\n",
    "            stride = [1, 1],\n",
    "            padding = 'VALID',\n",
    "            activation_fn = tf.nn.relu)\n",
    "        \n",
    "        # We take the output from the final convolutional layer and split it\n",
    "        # into separate advantage and value streams.\n",
    "        \n",
    "        # split on the 3rd dimension into 2 different parts\n",
    "        self.streamAC, self.streamVC = tf.split(3, 2, self.conv4)\n",
    "        \n",
    "        # flatten to [batch_size, k]\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        \n",
    "        # weights for advantage and value stream layer\n",
    "        self.AW = tf.Variable(tf.random_normal([int(h_size/2) , env.action_space.n]))\n",
    "        self.VW = tf.Variable(tf.random_normal([int(h_size/2) , 1]))\n",
    "        \n",
    "        # output of advantage and value layer\n",
    "        self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "        self.Value = tf.matmul(self.streamV, self.VW)\n",
    "        \n",
    "        # Then combine them together to get our final Q-values.\n",
    "        # Q(s, a) = V(s) + A(a)\n",
    "        self.Qout = self.Value + tf.sub(\n",
    "            self.Advantage,\n",
    "            tf.reduce_mean( # TODO: understand how this A(a) is calculated\n",
    "                self.Advantage,\n",
    "                reduction_indices=1,\n",
    "                keep_dims=True))\n",
    "        \n",
    "        # index of max value across 1st dimension\n",
    "        self.predict = tf.argmax(self.Qout, 1)\n",
    "        \n",
    "        # Below we obtain the loss by taking the sum of squares difference between\n",
    "        # the target and prediction Q values.\n",
    "        \n",
    "        # target Q value\n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        \n",
    "        # possible actions\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, env.action_space.n, dtype=tf.float32)\n",
    "        \n",
    "        # predicted Q values\n",
    "        self.Q = tf.reduce_sum(tf.mul(self.Qout, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        # error = sum( (target - actual)^2 ) / batch_size\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        \n",
    "        # define trainer\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        \n",
    "        # define optimizer\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    \"\"\"Used to store experiences and samples randomly to train the network.\"\"\"\n",
    "    def __init__(self, buffer_size=1000):\n",
    "        self.buffer_size = buffer_size\n",
    "        # state, action, reward, state1, done\n",
    "#         self.states = np.array([])\n",
    "#         self.actions = np.array([])\n",
    "#         self.rewards = np.array([])\n",
    "#         self.states_ = np.array([])\n",
    "#         self.dones = np.array([])\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.states_ = []\n",
    "        self.dones = []\n",
    "        \n",
    "    def add(self, states, actions, rewards, states_, dones):\n",
    "        if len(self.states) + len(states) >= self.buffer_size:\n",
    "            self.states = self.states[:(self.buffer_size - len(states))]\n",
    "            self.actions = self.actions[:(self.buffer_size - len(actions))]\n",
    "            self.rewards = self.rewards[:(self.buffer_size - len(rewards))]\n",
    "            self.states_ = self.states_[:(self.buffer_size - len(states_))]\n",
    "            self.dones = self.dones[:(self.buffer_size - len(dones))]\n",
    "\n",
    "        self.states.extend(states)\n",
    "        self.actions.extend(actions)\n",
    "        self.rewards.extend(rewards)\n",
    "        self.states_.extend(states_)\n",
    "        self.dones.extend(dones)\n",
    "#         self.states = np.append(self.states, states)\n",
    "#         self.actions = np.append(self.actions, actions)\n",
    "#         self.rewards = np.append(self.rewards, rewards)\n",
    "#         self.states_ = np.append(self.states_, states_)\n",
    "#         self.dones = np.append(self.dones, dones)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        samples = random.sample(range(len(self.actions)), size)\n",
    "\n",
    "#         states = np.array([])\n",
    "#         actions = np.array([])\n",
    "#         rewards = np.array([])\n",
    "#         states_ = np.array([])\n",
    "#         dones = np.array([])\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        states_ = []\n",
    "        dones = []\n",
    "        for i in samples:\n",
    "#             states = np.append(states, self.states[i])\n",
    "#             actions = np.append(actions, self.actions[i])\n",
    "#             rewards = np.append(rewards, self.rewards[i])\n",
    "#             states_ = np.append(states_, self.states_[i])\n",
    "#             dones = np.append(dones, self.dones[i])\n",
    "            \n",
    "            states.append(self.states[i])\n",
    "            actions.append(self.actions[i])\n",
    "            rewards.append(self.rewards[i])\n",
    "            states_.append(self.states_[i])\n",
    "            dones.append(self.dones[i])\n",
    "                \n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[:int(total_vars/2)]):\n",
    "        op_holder.append(tfVars[idx+int(total_vars/2)].assign((var.value()*tau) + ((1-tau)*tfVars[idx+int(total_vars/2)].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\"Take 1D float array of rewards and compute discounted reward.\n",
    "    \n",
    "    Discounts rewards for a given episode.\n",
    "    This is the Monte-Carlo method since we apply it to all rewards\n",
    "    in a given episode.\n",
    "    \n",
    "    Provides more robust reward signal to DQN.\n",
    "    \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of experiences to use for each training step\n",
    "batch_size = 32\n",
    "\n",
    "# how often to execute training step\n",
    "update_freq = 4\n",
    "\n",
    "# discount factor on target Q-values\n",
    "y = 0.99\n",
    "\n",
    "# starting chance of random action\n",
    "startE = 1\n",
    "\n",
    "# final chance of random action\n",
    "endE = 0.1\n",
    "\n",
    "# how many training steps required to fully reduce startE to endE\n",
    "anneling_steps = 100000\n",
    "\n",
    "# number of episodes of env to train network with\n",
    "num_episodes = 100000\n",
    "\n",
    "# number of random actions before training begins\n",
    "pre_train_steps = 1000\n",
    "\n",
    "# Rate to update target network toward primary network\n",
    "tau = 0.001 \n",
    "\n",
    "# load saved model?\n",
    "load_model = True\n",
    "\n",
    "# path to save model to\n",
    "path = \"./dqn\"\n",
    "\n",
    "# size of final convolutional layer before\n",
    "# splitting it into Advantage and Value streams\n",
    "h_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get frame shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frameShape = env.ale.getScreenGrayscale().shape\n",
    "frameShape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size, frameShape, batch_size)\n",
    "targetQN = Qnetwork(h_size, frameShape, batch_size)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "# set rate of random action decrease\n",
    "e = startE\n",
    "stepDrop = (startE - endE) / anneling_steps\n",
    "\n",
    "# create lists to contain total rewards and steps per episode\n",
    "# movesList = []\n",
    "lastReward = 0\n",
    "total_steps = 0\n",
    "\n",
    "# make path for model to be saved in\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print(\"Loading Model...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "\n",
    "    # set target network to be equal to primary network\n",
    "    updateTarget(targetOps, sess)\n",
    "\n",
    "    try:\n",
    "        for i in range(num_episodes):\n",
    "            episodeBuffer = experience_buffer()\n",
    "\n",
    "            # reset environment and get first new observation\n",
    "            env.reset()\n",
    "            state = env.ale.getScreenGrayscale()\n",
    "            done = False\n",
    "            rewardAll = 0\n",
    "            numMoves = 0\n",
    "\n",
    "            # the Q-Network\n",
    "            while numMoves < 500:\n",
    "                numMoves += 1\n",
    "\n",
    "                # choose action with probability e of being a random action\n",
    "                if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                    action = np.random.randint(0, env.action_space.n) # 2 = num different actions\n",
    "                else:\n",
    "                    action = sess.run(mainQN.predict, feed_dict={mainQN.rgb_array: [state]})\n",
    "\n",
    "                observation, reward, done, _ = env.step(action)\n",
    "#                 if i % 100 == 0 and i != 0:\n",
    "                env.render()\n",
    "                state1 = env.ale.getScreenGrayscale()\n",
    "                total_steps += 1\n",
    "\n",
    "                # save experience to episode buffer\n",
    "                episodeBuffer.add([state], [action], [reward], [state1], [done])\n",
    "\n",
    "                if total_steps > pre_train_steps:\n",
    "                    if e > endE:\n",
    "                        e -= stepDrop\n",
    "\n",
    "                    if total_steps % update_freq == 0:\n",
    "                        # random sample of experiences\n",
    "                        states_t, actions_t, rewards_t, state1_t, dones_t = myBuffer.sample(batch_size)\n",
    "\n",
    "                        states_t = np.reshape(states_t, [-1, 210, 160, 1])\n",
    "\n",
    "                        # Double-DQN update to the target Q-values\n",
    "                        Q1 = sess.run(mainQN.predict, feed_dict={\n",
    "                                mainQN.rgb_array: state1_t})\n",
    "\n",
    "                        Q2 = sess.run(targetQN.Qout, feed_dict={\n",
    "                                targetQN.rgb_array: state1_t})\n",
    "                        \n",
    "                        # If resulting state is DONE, Q-Target = r\n",
    "                        # If True: 0. If False: 1.\n",
    "                        end_multiplier = -(np.array(dones_t) - 1)\n",
    "\n",
    "                        # The Q values for predicted actions\n",
    "                        doubleQ = np.array([Q2[i, j] for i, j in zip(range(len(Q1)), Q1)])\n",
    "                        \n",
    "                        targetQ = np.array(rewards_t) + (y * doubleQ * end_multiplier)\n",
    "                        \n",
    "                        # update network with target values\n",
    "                        _  = sess.run(mainQN.updateModel,\n",
    "                                     feed_dict={mainQN.rgb_array: states_t,\n",
    "                                               mainQN.targetQ: targetQ,\n",
    "                                               mainQN.actions: actions_t})\n",
    "                        \n",
    "                        updateTarget(targetOps, sess)\n",
    "\n",
    "                rewardAll += reward\n",
    "                state = state1\n",
    "\n",
    "                if done == True:\n",
    "                    break\n",
    "\n",
    "            # get all experiences from this episode\n",
    "            episodeRewards = np.array(episodeBuffer.rewards)\n",
    "\n",
    "            # discount all rewards\n",
    "            discountRewards = discount_rewards(episodeRewards)\n",
    "            episodeBuffer.rewards = discountRewards\n",
    "\n",
    "            # add discounted experiences to our experience buffer\n",
    "            # state, action, reward, state1, done\n",
    "            myBuffer.add(episodeBuffer.states,\n",
    "                         episodeBuffer.actions,\n",
    "                         episodeBuffer.rewards,\n",
    "                         episodeBuffer.states_,\n",
    "                         episodeBuffer.dones)\n",
    "\n",
    "            lastReward = rewardAll\n",
    "\n",
    "            # periodically save model\n",
    "            if i % 20 == 0:\n",
    "                saver.save(sess, path+'/model-Breakout-'+str(i)+'.cptk')\n",
    "                print(\"Saved Model\")\n",
    "            if i % 10 == 0:\n",
    "                print(i, total_steps, rewardAll, e)\n",
    "\n",
    "        saver.save(sess, path+'/model-Breakout-'+str(i)+'.cptk')\n",
    "    except Exception as e:\n",
    "        # if frames are still rendering, stop it\n",
    "        if env.viewer is not None:\n",
    "            env.viewer.close()\n",
    "            env.viewer = None\n",
    "        \n",
    "        saver.save(sess, path+'/model-Breakout-'+str(i)+'.cptk')\n",
    "        print(\"Saved Model\")\n",
    "        raise e\n",
    "\n",
    "print(\"Reward of last episode: \" + str(lastReward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size, frameShape, batch_size)\n",
    "targetQN = Qnetwork(h_size, frameShape, batch_size)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# create lists to contain total rewards and steps per episode\n",
    "total_steps = 0\n",
    "\n",
    "# make path for model to be saved in\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print(\"Loading Model...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "\n",
    "    try:\n",
    "        for i in range(num_episodes):\n",
    "\n",
    "            # reset environment and get first new observation\n",
    "            env.reset()\n",
    "            state = env.ale.getScreenGrayscale()\n",
    "            done = False\n",
    "            numMoves = 0\n",
    "            rewardSum = 0\n",
    "\n",
    "            # the Q-Network\n",
    "            while numMoves < 500:\n",
    "                numMoves += 1\n",
    "                state = env.ale.getScreenGrayscale()\n",
    "                action = sess.run(mainQN.predict, feed_dict={mainQN.rgb_array: [state]})\n",
    "\n",
    "                observation, reward, done, _ = env.step(action)\n",
    "                \n",
    "                env.render()\n",
    "                rewardSum += reward\n",
    "                total_steps += 1\n",
    "\n",
    "                if done == True:\n",
    "                    break\n",
    "\n",
    "            print(i, total_steps, rewardSum)\n",
    "\n",
    "    except Exception as e:\n",
    "        # if frames are still rendering, stop it\n",
    "        if env.viewer is not None:\n",
    "            env.viewer.close()\n",
    "            env.viewer = None\n",
    "        \n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buffer = experience_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "for i in range(100000):\n",
    "    env.render()\n",
    "    state = env.ale.getScreenGrayscale()\n",
    "    \n",
    "    action = int(input()) # your agent here (this takes random actions)\n",
    "    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    state1 = env.ale.getScreenGrayscale()\n",
    "    \n",
    "    buffer.add([state], [action], [reward], [state1], [done])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if env.viewer is not None:\n",
    "    env.viewer.close()\n",
    "    env.viewer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(buffer.states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"test.txt\", 'w') as file:\n",
    "    file.write(\"[\")\n",
    "    for step in buffer.states:\n",
    "        file.write(\"[\")\n",
    "        for pixel in step:\n",
    "            file.write(\"{}, \".format(pixel))\n",
    "        file.write(\"], \")\n",
    "    file.write(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.asarray(buffer.states)\n",
    "a.tofile(\"./human_data/states.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('./human_data/states.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./human_data/states.csv', sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size, frameShape, batch_size)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.ale.getScreenGrayscale()\n",
    "resized_image = sess.run(mainQN.imageIn, feed_dict={mainQN.rgb_array: [env.ale.getScreenGrayscale()]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resized_image[0].reshape(84,84)\n",
    "# resized_image[0][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(resized_image[0].reshape(84, 84), cmap='Greys_r')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
