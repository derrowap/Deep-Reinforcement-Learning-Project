{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import itertools\n",
    "import scipy.misc\n",
    "from IPython import display\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class gameOb():\n",
    "    def __init__(self,coordinates,size,intensity,channel,reward,name):\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        self.reward = reward\n",
    "        self.name = name\n",
    "        \n",
    "class gameEnv():\n",
    "    def __init__(self,partial,size):\n",
    "        self.sizeX = size\n",
    "        self.sizeY = size\n",
    "        self.actions = 4\n",
    "        self.objects = []\n",
    "        self.partial = partial\n",
    "        self.a = self.reset()\n",
    "#         plt.imshow(self.a,interpolation=\"nearest\")\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.objects = []\n",
    "        hero = gameOb(self.newPosition(),1,1,2,None,'hero')\n",
    "        self.objects.append(hero)\n",
    "        bug = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(bug)\n",
    "        hole = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
    "        self.objects.append(hole)\n",
    "        bug2 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(bug2)\n",
    "        hole2 = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
    "        self.objects.append(hole2)\n",
    "        bug3 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(bug3)\n",
    "        bug4 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(bug4)\n",
    "        state = self.renderEnv()\n",
    "        self.state = state\n",
    "        return state\n",
    "\n",
    "    def moveChar(self,direction):\n",
    "        # 0 - up, 1 - down, 2 - left, 3 - right\n",
    "        hero = self.objects[0]\n",
    "        heroX = hero.x\n",
    "        heroY = hero.y\n",
    "        penalize = 0.05\n",
    "        if direction == 0 and hero.y >= 1:\n",
    "            hero.y -= 1\n",
    "        if direction == 1 and hero.y <= self.sizeY-2:\n",
    "            hero.y += 1\n",
    "        if direction == 2 and hero.x >= 1:\n",
    "            hero.x -= 1\n",
    "        if direction == 3 and hero.x <= self.sizeX-2:\n",
    "            hero.x += 1     \n",
    "        if hero.x == heroX and hero.y == heroY:\n",
    "            penalize = .2\n",
    "        self.objects[0] = hero\n",
    "        return penalize\n",
    "    \n",
    "    def newPosition(self):\n",
    "        iterables = [ range(self.sizeX), range(self.sizeY)]\n",
    "        points = []\n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        currentPositions = []\n",
    "        for objectA in self.objects:\n",
    "            if (objectA.x,objectA.y) not in currentPositions:\n",
    "                currentPositions.append((objectA.x,objectA.y))\n",
    "        for pos in currentPositions:\n",
    "            points.remove(pos)\n",
    "        location = np.random.choice(range(len(points)),replace=False)\n",
    "        return points[location]\n",
    "\n",
    "    def checkGoal(self):\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'hero':\n",
    "                hero = obj\n",
    "            else:\n",
    "                others.append(obj)\n",
    "        ended = False\n",
    "        for other in others:\n",
    "            if hero.x == other.x and hero.y == other.y:\n",
    "                self.objects.remove(other)\n",
    "                if other.reward == 1:\n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,1,1,'goal'))\n",
    "                else: \n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,0,-1,'fire'))\n",
    "                return other.reward,True\n",
    "        if ended == False:\n",
    "            return 0.0,False\n",
    "\n",
    "    def renderEnv(self):\n",
    "        #a = np.zeros([self.sizeY,self.sizeX,3])\n",
    "        self.a = np.ones([self.sizeY+2,self.sizeX+2,3])\n",
    "        self.a[1:-1,1:-1,:] = 0\n",
    "        hero = None\n",
    "        for item in self.objects:\n",
    "            self.a[item.y+1:item.y+item.size+1,item.x+1:item.x+item.size+1,item.channel] = item.intensity\n",
    "            if item.name == 'hero':\n",
    "                hero = item\n",
    "        if self.partial == True:\n",
    "            self.a = self.a[hero.y:hero.y+3,hero.x:hero.x+3,:]\n",
    "        b = scipy.misc.imresize(self.a[:,:,0],[84,84,1],interp='nearest')\n",
    "        c = scipy.misc.imresize(self.a[:,:,1],[84,84,1],interp='nearest')\n",
    "        d = scipy.misc.imresize(self.a[:,:,2],[84,84,1],interp='nearest')\n",
    "        self.a = np.stack([b,c,d],axis=2)\n",
    "        return self.a\n",
    "\n",
    "    def step(self,action):\n",
    "        penalty = self.moveChar(action)\n",
    "        reward,done = self.checkGoal()\n",
    "        state = self.renderEnv()\n",
    "        return state,(reward-penalty),done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gameEnv(partial=False,size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self, h_size, frameShape, batch_size):\n",
    "        # The network recieves a frame from the game, flattened into an array.\n",
    "        # It then resizes it and processes it through four convolutional layers.\n",
    "        \n",
    "        # raw pixel data (grayscale, so only 1 channel)\n",
    "        self.rgb_array = tf.placeholder(shape=[None, frameShape[0], frameShape[1], 3], dtype=tf.float32)\n",
    "        \n",
    "        # tf input: a 4-D tensor [batch_size, height, width, channels]\n",
    "        self.imageIn = tf.image.resize_images(self.rgb_array, 84, 84, 3)\n",
    "        \n",
    "        self.conv1 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.imageIn,\n",
    "            num_outputs = 32,\n",
    "            kernel_size = [8, 8],\n",
    "            stride = [4, 4],\n",
    "            padding = 'VALID',\n",
    "            biases_initializer = None)\n",
    "        \n",
    "        self.conv2 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv1,\n",
    "            num_outputs = 64,\n",
    "            kernel_size = [4, 4],\n",
    "            stride = [2, 2],\n",
    "            padding = 'VALID',\n",
    "            biases_initializer = None)\n",
    "        \n",
    "        self.conv3 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv2,\n",
    "            num_outputs = 64,\n",
    "            kernel_size = [3, 3],\n",
    "            stride = [1, 1],\n",
    "            padding = 'VALID',\n",
    "            biases_initializer = None)\n",
    "        \n",
    "        self.conv4 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv3,\n",
    "            num_outputs = 512,\n",
    "            kernel_size = [7, 7],\n",
    "            stride = [1, 1],\n",
    "            padding = 'VALID',\n",
    "            biases_initializer = None)\n",
    "        \n",
    "        # We take the output from the final convolutional layer and split it\n",
    "        # into separate advantage and value streams.\n",
    "        \n",
    "        # split on the 3rd dimension into 2 different parts\n",
    "        self.streamAC, self.streamVC = tf.split(3, 2, self.conv4)\n",
    "        \n",
    "        # flatten to [batch_size, k]\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        \n",
    "        # weights for advantage and value stream layer\n",
    "        self.AW = tf.Variable(tf.random_normal([int(h_size/2) , env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([int(h_size/2) , 1]))\n",
    "        \n",
    "        # output of advantage and value layer\n",
    "        self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "        self.Value = tf.matmul(self.streamV, self.VW)\n",
    "        \n",
    "        # Then combine them together to get our final Q-values.\n",
    "        # Q(s, a) = V(s) + A(a)\n",
    "        self.Qout = self.Value + tf.sub(\n",
    "            self.Advantage,\n",
    "            tf.reduce_mean( # TODO: understand how this A(a) is calculated\n",
    "                self.Advantage,\n",
    "                reduction_indices=1,\n",
    "                keep_dims=True))\n",
    "        \n",
    "        # index of max value across 1st dimension\n",
    "        self.predict = tf.argmax(self.Qout, 1)\n",
    "        \n",
    "        # Below we obtain the loss by taking the sum of squares difference between\n",
    "        # the target and prediction Q values.\n",
    "        \n",
    "        # target Q value\n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        \n",
    "        # possible actions\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, env.actions, dtype=tf.float32)\n",
    "        \n",
    "        # predicted Q values\n",
    "        self.Q = tf.reduce_sum(tf.mul(self.Qout, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        # error = sum( (target - actual)^2 ) / batch_size\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        \n",
    "        # define trainer\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        \n",
    "        # define optimizer\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    \"\"\"Used to store experiences and samples randomly to train the network.\"\"\"\n",
    "    def __init__(self, buffer_size=50000):\n",
    "        self.buffer_size = buffer_size\n",
    "        # state, action, reward, state1, done\n",
    "#         self.states = np.array([])\n",
    "#         self.actions = np.array([])\n",
    "#         self.rewards = np.array([])\n",
    "#         self.states_ = np.array([])\n",
    "#         self.dones = np.array([])\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.states_ = []\n",
    "        self.dones = []\n",
    "        \n",
    "    def add(self, states, actions, rewards, states_, dones):\n",
    "        if len(self.states) + len(states) >= self.buffer_size:\n",
    "            self.states = self.states[:(self.buffer_size - len(states))]\n",
    "            self.actions = self.actions[:(self.buffer_size - len(actions))]\n",
    "            self.rewards = self.rewards[:(self.buffer_size - len(rewards))]\n",
    "            self.states_ = self.states_[:(self.buffer_size - len(states_))]\n",
    "            self.dones = self.dones[:(self.buffer_size - len(dones))]\n",
    "\n",
    "        self.states.extend(states)\n",
    "        self.actions.extend(actions)\n",
    "        self.rewards.extend(rewards)\n",
    "        self.states_.extend(states_)\n",
    "        self.dones.extend(dones)\n",
    "#         self.states = np.append(self.states, states)\n",
    "#         self.actions = np.append(self.actions, actions)\n",
    "#         self.rewards = np.append(self.rewards, rewards)\n",
    "#         self.states_ = np.append(self.states_, states_)\n",
    "#         self.dones = np.append(self.dones, dones)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        samples = random.sample(range(len(self.actions)), size)\n",
    "\n",
    "#         states = np.array([])\n",
    "#         actions = np.array([])\n",
    "#         rewards = np.array([])\n",
    "#         states_ = np.array([])\n",
    "#         dones = np.array([])\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        states_ = []\n",
    "        dones = []\n",
    "        for i in samples:\n",
    "#             states = np.append(states, self.states[i])\n",
    "#             actions = np.append(actions, self.actions[i])\n",
    "#             rewards = np.append(rewards, self.rewards[i])\n",
    "#             states_ = np.append(states_, self.states_[i])\n",
    "#             dones = np.append(dones, self.dones[i])\n",
    "            \n",
    "            states.append(self.states[i])\n",
    "            actions.append(self.actions[i])\n",
    "            rewards.append(self.rewards[i])\n",
    "            states_.append(self.states_[i])\n",
    "            dones.append(self.dones[i])\n",
    "                \n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[:int(total_vars/2)]):\n",
    "        op_holder.append(tfVars[idx+int(total_vars/2)].assign((var.value()*tau) + ((1-tau)*tfVars[idx+int(total_vars/2)].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\"Take 1D float array of rewards and compute discounted reward.\n",
    "    \n",
    "    Discounts rewards for a given episode.\n",
    "    This is the Monte-Carlo method since we apply it to all rewards\n",
    "    in a given episode.\n",
    "    \n",
    "    Provides more robust reward signal to DQN.\n",
    "    \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of experiences to use for each training step\n",
    "batch_size = 32\n",
    "\n",
    "# how often to execute training step\n",
    "update_freq = 4\n",
    "\n",
    "# discount factor on target Q-values\n",
    "y = 0.99\n",
    "\n",
    "# starting chance of random action\n",
    "startE = 1\n",
    "\n",
    "# final chance of random action\n",
    "endE = 0.1\n",
    "\n",
    "# how many training steps required to fully reduce startE to endE\n",
    "anneling_steps = 20000\n",
    "\n",
    "# number of episodes of env to train network with\n",
    "num_episodes = 10000\n",
    "\n",
    "# number of random actions before training begins\n",
    "pre_train_steps = 10000\n",
    "\n",
    "# maximum length of our episode\n",
    "\n",
    "# Rate to update target network toward primary network\n",
    "tau = 0.001 \n",
    "\n",
    "# load saved model?\n",
    "load_model = False\n",
    "\n",
    "# path to save model to\n",
    "path = \"./dqn/gridWorld\"\n",
    "\n",
    "# size of final convolutional layer before\n",
    "# splitting it into Advantage and Value streams\n",
    "h_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frameShape = (84, 84, 3)\n",
    "frameShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model model-gridWorld-0\n",
      "0 4 -1.2000000000000002 1\n",
      "Saved Model model-gridWorld-100\n",
      "100 676 0.7 1\n",
      "Saved Model model-gridWorld-200\n",
      "200 1342 0.34999999999999987 1\n",
      "Saved Model model-gridWorld-300\n",
      "300 2268 0.8999999999999999 1\n",
      "Saved Model model-gridWorld-400\n",
      "400 3088 -0.3500000000000003 1\n",
      "Saved Model model-gridWorld-500\n",
      "500 3765 -3.1500000000000004 1\n",
      "Saved Model model-gridWorld-600\n",
      "600 4504 -1.1500000000000001 1\n",
      "Saved Model model-gridWorld-700\n",
      "700 5275 0.29999999999999993 1\n",
      "Saved Model model-gridWorld-800\n",
      "800 5934 -1.25 1\n",
      "Saved Model model-gridWorld-900\n",
      "900 6735 -0.8500000000000001 1\n",
      "Saved Model model-gridWorld-1000\n",
      "1000 7395 -2.75 1\n",
      "Saved Model model-gridWorld-1100\n",
      "1100 8119 -1.1 1\n",
      "Saved Model model-gridWorld-1200\n",
      "1200 8715 -1.3 1\n",
      "Saved Model model-gridWorld-1300\n",
      "1300 9575 -1.25 1\n",
      "Saved Model model-gridWorld-1400\n",
      "1400 10224 0.5999999999999999 0.9899200000000086\n",
      "Saved Model model-gridWorld-1500\n",
      "1500 11039 -1.1 0.9532450000000398\n",
      "Saved Model model-gridWorld-1600\n",
      "1600 12030 0.04999999999999982 0.9086500000000777\n",
      "Saved Model model-gridWorld-1700\n",
      "1700 12864 -1.4999999999999998 0.8711200000001096\n",
      "Saved Model model-gridWorld-1800\n",
      "1800 13639 -4.149999999999999 0.8362450000001392\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size, frameShape, batch_size)\n",
    "targetQN = Qnetwork(h_size, frameShape, batch_size)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "# set rate of random action decrease\n",
    "e = startE\n",
    "stepDrop = (startE - endE) / anneling_steps\n",
    "\n",
    "# create lists to contain total rewards and steps per episode\n",
    "# movesList = []\n",
    "lastReward = 0\n",
    "total_steps = 0\n",
    "\n",
    "# make path for model to be saved in\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print(\"Loading Model...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "\n",
    "    # set target network to be equal to primary network\n",
    "    updateTarget(targetOps, sess)\n",
    "\n",
    "    try:\n",
    "        for i in range(num_episodes):\n",
    "            episodeBuffer = experience_buffer()\n",
    "\n",
    "            # reset environment and get first new observation\n",
    "            env.reset()\n",
    "            state = env.renderEnv()\n",
    "            done = False\n",
    "            rewardAll = 0\n",
    "            numMoves = 0\n",
    "\n",
    "            # the Q-Network\n",
    "            while numMoves < 200:\n",
    "                numMoves += 1\n",
    "\n",
    "                # choose action with probability e of being a random action\n",
    "                if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                    action = np.random.randint(0, env.actions) # 2 = num different actions\n",
    "                else:\n",
    "                    action = sess.run(mainQN.predict, feed_dict={mainQN.rgb_array: [state]})\n",
    "\n",
    "                observation, reward, done = env.step(action)\n",
    "#                 if i % 100 == 0 and i != 0:\n",
    "#                     env.render()\n",
    "                state1 = env.renderEnv()\n",
    "                total_steps += 1\n",
    "\n",
    "                # save experience to episode buffer\n",
    "                episodeBuffer.add([state], [action], [reward], [state1], [done])\n",
    "\n",
    "                if total_steps > pre_train_steps:\n",
    "                    if e > endE:\n",
    "                        e -= stepDrop\n",
    "\n",
    "                    if total_steps % update_freq == 0:\n",
    "                        # random sample of experiences\n",
    "                        states_t, actions_t, rewards_t, state1_t, dones_t = myBuffer.sample(batch_size)\n",
    "                        \n",
    "\n",
    "#                         states_t = np.reshape(states_t, [-1, 210, 160, 1])\n",
    "\n",
    "                        # Double-DQN update to the target Q-values\n",
    "                        Q1 = sess.run(mainQN.predict, feed_dict={\n",
    "                                mainQN.rgb_array: state1_t})\n",
    "\n",
    "                        Q2 = sess.run(targetQN.Qout, feed_dict={\n",
    "                                targetQN.rgb_array: state1_t})\n",
    "                        \n",
    "                        # If resulting state is DONE, Q-Target = r\n",
    "                        # If True: 0. If False: 1.\n",
    "                        end_multiplier = -(np.array(dones_t) - 1)\n",
    "\n",
    "                        # The Q values for predicted actions\n",
    "                        doubleQ = np.array([Q2[i, j] for i, j in zip(range(len(Q1)), Q1)])\n",
    "                        \n",
    "                        targetQ = np.array(rewards_t) + (y * doubleQ * end_multiplier)\n",
    "                        \n",
    "                        # update network with target values\n",
    "                        _  = sess.run(mainQN.updateModel,\n",
    "                                     feed_dict={mainQN.rgb_array: states_t,\n",
    "                                               mainQN.targetQ: targetQ,\n",
    "                                               mainQN.actions: actions_t})\n",
    "                        \n",
    "                        updateTarget(targetOps, sess)\n",
    "\n",
    "                rewardAll += reward\n",
    "                state = state1\n",
    "\n",
    "                if done == True:\n",
    "                    break\n",
    "\n",
    "            # get all experiences from this episode\n",
    "            episodeRewards = np.array(episodeBuffer.rewards)\n",
    "\n",
    "            # discount all rewards\n",
    "            discountRewards = discount_rewards(episodeRewards)\n",
    "            episodeBuffer.rewards = discountRewards\n",
    "\n",
    "            # add discounted experiences to our experience buffer\n",
    "            # state, action, reward, state1, done\n",
    "            myBuffer.add(episodeBuffer.states,\n",
    "                         episodeBuffer.actions,\n",
    "                         episodeBuffer.rewards,\n",
    "                         episodeBuffer.states_,\n",
    "                         episodeBuffer.dones)\n",
    "\n",
    "            lastReward = rewardAll\n",
    "\n",
    "            # periodically save model\n",
    "            if i % 100 == 0:\n",
    "                saver.save(sess, path+'/model-gridWorld-'+str(i)+'.cptk')\n",
    "                print(\"Saved Model model-gridWorld-\" +str(i))\n",
    "                print(i, total_steps, rewardAll, e)\n",
    "                \n",
    "\n",
    "        saver.save(sess, path+'/model-gridWorld-'+str(i)+'.cptk')\n",
    "    except Exception as e:\n",
    "        # if frames are still rendering, stop it\n",
    "#         if env.viewer is not None:\n",
    "#             env.viewer.close()\n",
    "#             env.viewer = None\n",
    "        \n",
    "        saver.save(sess, path+'/model-gridWorld-'+str(i)+'.cptk')\n",
    "        print(\"Saved Model model-gridWorld-\" +str(i))\n",
    "        raise e\n",
    "\n",
    "print(\"Reward of last episode: \" + str(lastReward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(path)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "    for i in range(20):\n",
    "        complete = False\n",
    "        iter = 0\n",
    "        env = gameEnv(partial=False,size=5)\n",
    "        plt.imshow(env.a)\n",
    "        time.sleep(2)\n",
    "        while((complete == False) & (iter < 20)):\n",
    "            action = sess.run(mainQN.predict, feed_dict={mainQN.rgb_array: [env.a]})\n",
    "            observation, reward, done = env.step(action)\n",
    "            plt.imshow(env.a)\n",
    "#             display.clear_output(wait=True)\n",
    "#             display.display(plt.gcf())\n",
    "            \n",
    "            print(iter)\n",
    "            print(\"reward: \", reward)\n",
    "            iter += 1\n",
    "            if(done):\n",
    "                complete = True\n",
    "                print(\"Complete\")\n",
    "            time.sleep(.5)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./dqn/gridWorld/model-gridWorld-3825.cptk'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt.model_checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
