{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "from IPython import display\n",
    "import time\n",
    "import math\n",
    "import cProfile\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_color(value):\n",
    "    if value <= 0:\n",
    "        return '#cdc1b4'\n",
    "    if value <= 2:\n",
    "        return '#eee4da'\n",
    "    if value <= 4:\n",
    "        return '#ede0c8'\n",
    "    if value <= 8:\n",
    "        return '#f2b179'\n",
    "    if value <= 16:\n",
    "        return '#f59563'\n",
    "    if value <= 32:\n",
    "        return '#f67c5f'\n",
    "    if value <= 64:\n",
    "        return '#f65e3b'\n",
    "    if value <= 128:\n",
    "        return '#edcf72'\n",
    "    if value <= 256:\n",
    "        return '#edcc61'\n",
    "    if value <= 512:\n",
    "        return '#edc850'\n",
    "    if value <= 1024:\n",
    "        return '#edc53f'\n",
    "    return '#ecc02e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class gameEnv():\n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.actions = 4\n",
    "        self.a = []\n",
    "        self.rendered = []\n",
    "        self.currReward = 0\n",
    "        self.cumReward = 0\n",
    "        self.reset()\n",
    "        self.won = False\n",
    "        self.movement = False\n",
    "        self.last_action = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        arr = np.array([0 for i in range(16)])\n",
    "        self.a = arr.reshape([4,4])\n",
    "        self.addRandomOpen()\n",
    "        self.renderEnv()\n",
    "        self.currReward = 0\n",
    "        self.cumReward = 0\n",
    "        return self.a\n",
    "        \n",
    "    def addRandomOpen(self):\n",
    "        indices = np.asarray(np.where(self.a == 0)).T\n",
    "        choice = random.choice(indices)\n",
    "        if(np.random.rand(1)<.1):\n",
    "            self.a[choice[0]][choice[1]] = 4\n",
    "        else:\n",
    "            self.a[choice[0]][choice[1]] = 2\n",
    "    \n",
    "    def renderEnv(self):\n",
    "        arr = np.ones([4,4])\n",
    "        for i in range(16):\n",
    "            arr[i//4][i%4] = self.getColor(self.a[i//4][i%4])\n",
    "        return scipy.misc.imresize(arr[:,:],[84,84,1],interp='nearest')\n",
    "        \n",
    "    def getColor(self, val):\n",
    "        if(val == 0):\n",
    "            return 0\n",
    "        return math.log2(val) * 23\n",
    "    \n",
    "    def checkEnd(self):\n",
    "        if(len(np.where(self.a == 0)[0]) == 0):\n",
    "            temp = self.a.flatten()\n",
    "            for i in range(len(temp)):\n",
    "                if(i + 4 < len(temp)):\n",
    "                    if(temp[i] == temp[i+4]):\n",
    "                        return False\n",
    "                if((i+1)%4 != 0):\n",
    "                    if(temp[i] == temp[i+1]):\n",
    "                        return False\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    # 0 - up, 1 - down, 2 - left, 3 - right\n",
    "    def move(self, action):\n",
    "        if(action == 0):\n",
    "            self.a = np.rot90(self.a,1)\n",
    "            self.a = self.mergeLeft(self.a)\n",
    "            self.a = np.rot90(self.a,3)\n",
    "        elif(action ==1):\n",
    "            self.a = np.rot90(self.a,3)\n",
    "            self.a = self.mergeLeft(self.a)\n",
    "            self.a = np.rot90(self.a,1)\n",
    "        elif(action == 2):\n",
    "            self.a = self.mergeLeft(self.a)\n",
    "        elif(action == 3):\n",
    "            self.a = np.rot90(self.a,2)\n",
    "            self.a = self.mergeLeft(self.a)\n",
    "            self.a = np.rot90(self.a,2)        \n",
    "        \n",
    "        if(self.movement == True):\n",
    "            self.addRandomOpen()\n",
    "        else:\n",
    "            self.currReward = -10\n",
    "            \n",
    "        return self.checkEnd()\n",
    "                        \n",
    "    def step(self, action):\n",
    "        self.last_action = action\n",
    "        done = self.move(action)\n",
    "        return self.renderEnv(),self.currReward,done\n",
    "    \n",
    "    def checkMove(self):\n",
    "        return self.movement\n",
    "        \n",
    "    def mergeLeft(self, arr):\n",
    "        self.currReward = 0\n",
    "        self.movement = False\n",
    "        for i in range(4):\n",
    "            stop = 0\n",
    "            for j in range(1, 4):\n",
    "                if arr[i][j] != 0:\n",
    "                    for k in range(stop, j):\n",
    "                        if(arr[i][k] == arr[i][j]):\n",
    "                            arr[i][k] = arr[i][k] * 2\n",
    "                            arr[i][j] = 0\n",
    "                            self.currReward += arr[i][k]\n",
    "                            self.movement = True\n",
    "                            stop = k + 1\n",
    "                            break\n",
    "                        elif(arr[i][k] == 0):\n",
    "                            arr[i][k] = arr[i][j]\n",
    "                            arr[i][j] = 0\n",
    "                            self.movement = True\n",
    "                            break\n",
    "                        else:\n",
    "                            stop = k + 1\n",
    "                            \n",
    "        self.cumReward += self.currReward\n",
    "        return arr\n",
    "    \n",
    "    def renderDisplay(self):\n",
    "        size_of_image = 8\n",
    "        font_size = 20\n",
    "        fig, ax = plt.subplots(figsize=(size_of_image+2, size_of_image))\n",
    "\n",
    "        for x in range(0, 4):\n",
    "            for y in range(0, 4):\n",
    "                ax.broken_barh([(x, 1)],\n",
    "                               (y, 1),\n",
    "                               facecolors=get_color(self.a[y][x]))\n",
    "                ax.annotate(str(self.a[y][x]), (x, y),\n",
    "                            xytext=((2*x + 1)/2, (2*y + 1) / 2),\n",
    "                            fontsize=font_size,\n",
    "                            horizontalalignment='center',\n",
    "                            verticalalignment='center')\n",
    "\n",
    "        # set range of plot grid\n",
    "        ax.set_ylim(4, 0)\n",
    "        ax.set_xlim(0, 5)\n",
    "\n",
    "        if self.last_action == 0:\n",
    "            # up\n",
    "            plt.arrow(4.5, 2.2, 0.0, -0.4, fc=\"k\", ec=\"k\", head_width=0.1, head_length=0.1)\n",
    "        elif self.last_action == 1:\n",
    "            # down\n",
    "            plt.arrow(4.5, 1.8, 0.0, 0.4, fc=\"k\", ec=\"k\", head_width=0.1, head_length=0.1)\n",
    "        elif self.last_action == 2:\n",
    "            # left\n",
    "            plt.arrow(4.8, 2.0, -0.4, 0.0, fc=\"k\", ec=\"k\", head_width=0.1, head_length=0.1)\n",
    "        elif self.last_action == 3:\n",
    "            # right\n",
    "            plt.arrow(4.3, 2.0, 0.4, 0.0, fc=\"k\", ec=\"k\", head_width=0.1, head_length=0.1)\n",
    "\n",
    "        ax.annotate(\"Score:\", (1, 1), xytext=(4.5, 0.5), fontsize=20,\n",
    "                   horizontalalignment='center', verticalalignment='center')\n",
    "        ax.annotate(str(self.cumReward), (1, 1), xytext=(4.5, 0.9), fontsize=20,\n",
    "                    horizontalalignment='center', verticalalignment='center')    \n",
    "\n",
    "        plt.axis('off')\n",
    "        ax.grid(True)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size,prev_states):        \n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.imageIn = tf.placeholder(shape=[None,84,84,1],dtype=tf.float32)\n",
    "        self.conv1 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv3,num_outputs=512,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        #We take the output from the final convolutional layer and split it into separate advantage and value streams.\n",
    "        self.streamAC,self.streamVC = tf.split(3,2,self.conv4)\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2,env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.sub(self.Advantage,tf.reduce_mean(self.Advantage,reduction_indices=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.mul(self.Qout, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    \"\"\"Used to store experiences and samples randomly to train the network.\"\"\"\n",
    "    def __init__(self, buffer_size=50000):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        \n",
    "    def add(self, states, actions, rewards, dones):\n",
    "        if len(self.actions) == self.buffer_size:\n",
    "            self.states = self.states[1:]\n",
    "            self.actions = self.actions[1:]\n",
    "            self.rewards = self.rewards[1:]\n",
    "            self.dones = self.dones[1:]\n",
    "\n",
    "        self.states.append(states)\n",
    "        self.actions.append(actions)\n",
    "        self.rewards.append(rewards)\n",
    "        self.dones.append(dones)\n",
    "        \n",
    "    def sample(self, size, previous_states):\n",
    "        samples = np.random.permutation(len(self.actions)-(previous_states-1)) + (previous_states-1)\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        states_ = []\n",
    "        dones = []\n",
    "        for i in samples[:size]:\n",
    "            temp = []\n",
    "            for j in range(previous_states):\n",
    "                temp.append(self.states[i - previous_states + j + 1])\n",
    "            states.append(np.dstack(temp))\n",
    "            actions.append(self.actions[i])\n",
    "            rewards.append(self.rewards[i])\n",
    "            states_.append(self.states[i+1])\n",
    "            dones.append(self.dones[i])\n",
    "                \n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100 #How many experiences to use for each training step.\n",
    "update_freq = 8 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "anneling_steps = 1 #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 100003 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 0 #How many steps of random actions before training begins.\n",
    "pre_train_steps_from_Q = False #If true, initialize buffer with steps from Q instead of random actions\n",
    "max_epLength = 5000 #The max allowed length of our episode.\n",
    "load_model = True #Whether to load a saved model.\n",
    "path = \"./Deep-Reinforcement-Learning-Project/dqn/save_data/2048/1/cont/\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.0001 #Rate to update target network toward primary network\n",
    "previous_states=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gameEnv()\n",
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size,previous_states)\n",
    "targetQN = Qnetwork(h_size,previous_states)\n",
    "sess = tf.Session()\n",
    "load = './Deep-Reinforcement-Learning-Project/dqn/save_data/2048/1/model.cptk'\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess,load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "New Highscore:  1304\n",
      "New Highscore:  2804\n",
      "New Highscore:  3052\n",
      "New Highscore:  3064\n",
      "New Highscore:  3172\n",
      "New Highscore:  3316\n"
     ]
    }
   ],
   "source": [
    "env = gameEnv()\n",
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size,previous_states)\n",
    "targetQN = Qnetwork(h_size,previous_states)\n",
    "t_start = time.time()\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "#arrays to save\n",
    "eps_arr = []\n",
    "time_arr = []\n",
    "err_arr = []\n",
    "annel_arr = []\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "step_list = []\n",
    "reward_list = []\n",
    "total_steps = 0\n",
    "hero_x = 0\n",
    "hero_y = 0\n",
    "least_distance = 100\n",
    "last_time = t_start\n",
    "last_move_no_move = False\n",
    "high_score = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        load = './Deep-Reinforcement-Learning-Project/dqn/save_data/2048/1/cont/model.cptk'\n",
    "        saver.restore(sess,load)\n",
    "        data = np.load('./Deep-Reinforcement-Learning-Project/dqn/save_data/2048/1/cont/data.npz')\n",
    "        eps_arr = data['episode'].tolist()\n",
    "        time_arr = data['ti'].tolist()\n",
    "        err_arr = data['error'].tolist()\n",
    "        annel_arr = data['anneling'].tolist()\n",
    "    else:\n",
    "        sess.run(init)\n",
    "#     updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "    for i in range(1, num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        env.reset()\n",
    "        s = env.renderEnv()\n",
    "        s = s.reshape([84, 84, 1])\n",
    "        if(i==1):\n",
    "            myBuffer.states.append(s)\n",
    "        d = False\n",
    "        reward_sum = 0\n",
    "        step = 0\n",
    "        #The Q-Network\n",
    "        while step < max_epLength: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "            step+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            \n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:       \n",
    "                a = random.randint(0,3)\n",
    "            elif(last_move_no_move == True):\n",
    "                a = random.randint(0,3)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.imageIn:[s]})[0]\n",
    "            \n",
    "            s1,r,d = env.step(a)\n",
    "            \n",
    "            last_move_no_move = False\n",
    "            if(r < 0):\n",
    "                last_move_no_move = True\n",
    "            \n",
    "            \n",
    "            \n",
    "            s1 = s1.reshape([84, 84, 1])\n",
    "            total_steps += 1\n",
    "            myBuffer.add(s1,a,r,d) #Save the experience to our episode buffer.\n",
    "\n",
    "            if total_steps > pre_train_steps and i > 1:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    states, actions, rewards, state_, done = myBuffer.sample(batch_size,previous_states) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.imageIn:state_})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.imageIn:state_})\n",
    "                    end_multiplier = -(np.array(done) - 1)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    targetQ = rewards + (y*doubleQ * end_multiplier)\n",
    "                    #Update the network with our target values.\n",
    "                    _ = sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.imageIn:states,mainQN.targetQ:targetQ, mainQN.actions:actions})\n",
    "\n",
    "                    updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "            reward_sum += r\n",
    "            s = s1\n",
    "\n",
    "            if d == True:\n",
    "                break\n",
    "\n",
    "        #Get all experiences from this episode and discount their rewards. \n",
    "        step_list.append(step)\n",
    "        reward_list.append(env.cumReward)\n",
    "        if(env.cumReward > high_score):\n",
    "            print('New Highscore: ', env.cumReward)\n",
    "            high_score = env.cumReward\n",
    "            \n",
    "        #Periodically save the model. \n",
    "        if len(reward_list) % 100 == 0:\n",
    "            ti = time.time()\n",
    "            eps_arr.append(i)\n",
    "            time_arr.append(ti - t_start)\n",
    "            err_arr.append(np.mean(reward_list[-100:]))\n",
    "            annel_arr.append(e)\n",
    "            if(e > .1):\n",
    "                print(i, total_steps,np.mean(step_list[-100:]),np.mean(reward_list[-100:]), (ti - last_time)/60, e)\n",
    "            else:\n",
    "                print(i, total_steps,np.mean(step_list[-100:]),np.mean(reward_list[-100:]), (ti - last_time)/60)\n",
    "            saver.save(sess,path+'model.cptk')\n",
    "            np.savez(path+'data.npz', episode=eps_arr, ti=time_arr, error = err_arr, anneling = annel_arr)\n",
    "            last_time = ti\n",
    "        if i % 1000 == 0:\n",
    "            print('----last 1000', np.mean(reward_list[-1000:]),'-----')\n",
    "    saver.save(sess,path+'model-'+str(i)+'.cptk')\n",
    "    np.savez(path+'data' + str(i)+ '.npz', episode=eps_arr, time=time_arr, error = err_arr, anneling = annel_arr)\n",
    "print(\"Percent of succesful episodes: \" + str(sum(reward_list)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(env.renderEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    complete = False\n",
    "    iter = 0\n",
    "    env = gameEnv()\n",
    "    env.renderEnv()\n",
    "    r_all = 0\n",
    "    while(complete == False):\n",
    "        action = sess.run(mainQN.predict, feed_dict={mainQN.imageIn:[env.renderEnv().reshape([84, 84, 1])]})\n",
    "        observation, reward, done = env.step(action)\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(env.renderDisplay())\n",
    "        r_all += reward\n",
    "        iter += 1\n",
    "        if(done):\n",
    "            complete = True\n",
    "            print(iter)\n",
    "            print(\"Complete\")\n",
    "            print(\"reward: \", env.cumReward)\n",
    "        time.sleep(.1)\n",
    "#           plt.show()\n",
    "    print(\"\\n \\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gameEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAHfCAYAAAB0213WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdbb812a588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    713\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m                 \u001b[0mident\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    715\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[1;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m             \u001b[0mmsg_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[1;34m(self, flags, copy, track)\u001b[0m\n\u001b[0;32m    357\u001b[0m         \"\"\"\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[0mparts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m         \u001b[1;31m# have first part already, only loop while more to receive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:6971)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:6763)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy (zmq/backend/cython/socket.c:1931)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:7222)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-e2c61de9c75b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckEnd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrenderDisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    687\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m         )\n\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    717\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while(env.checkEnd() ==False):\n",
    "    a= int(input())\n",
    "    env.step(a)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(env.renderDisplay())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "stop = 0\n",
    "for k in range(stop, 5):\n",
    "    print(k)\n",
    "    stop = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
