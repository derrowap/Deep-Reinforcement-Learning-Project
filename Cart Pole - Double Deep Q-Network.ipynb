{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole - Double Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the CartPole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-19 21:54:27,002] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self, h_size, frameShape, batch_size):\n",
    "        # The network recieves a frame from the game, flattened into an array.\n",
    "        # It then resizes it and processes it through four convolutional layers.\n",
    "        \n",
    "        # the number of pixels in a frame, number of inputs to network\n",
    "        numInputs = 1\n",
    "        for i in frameShape:\n",
    "            numInputs *= i\n",
    "        \n",
    "        # raw rgb values from game\n",
    "        self.rgb_array = tf.placeholder(shape=[None, frameShape[0], frameShape[1], frameShape[2]], dtype=tf.float32)\n",
    "        \n",
    "        # tf input: a 4-D tensor [batch_size, height, width, channels]\n",
    "        self.imageIn = tf.reshape(self.rgb_array,shape=[-1, frameShape[0], frameShape[1], frameShape[2]])\n",
    "        \n",
    "        self.conv1 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.imageIn,\n",
    "            num_outputs = 32,\n",
    "            kernel_size = [8, 8],\n",
    "            stride = [4, 4],\n",
    "            padding = 'VALID',\n",
    "            biases_initializer = None)\n",
    "        \n",
    "        self.conv2 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv1,\n",
    "            num_outputs = 64,\n",
    "            kernel_size = [4, 4],\n",
    "            stride = [2, 2],\n",
    "            padding = 'VALID',\n",
    "            biases_initializer = None)\n",
    "        \n",
    "        self.conv3 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv2,\n",
    "            num_outputs = 64,\n",
    "            kernel_size = [3, 3],\n",
    "            stride = [1, 1],\n",
    "            padding = 'VALID',\n",
    "            biases_initializer = None)\n",
    "        \n",
    "        self.conv4 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv3,\n",
    "            num_outputs = 512,\n",
    "            kernel_size = [7, 7],\n",
    "            stride = [1, 1],\n",
    "            padding = 'VALID',\n",
    "            biases_initializer = None)\n",
    "        \n",
    "        # We take the output from the final convolutional layer and split it\n",
    "        # into separate advantage and value streams.\n",
    "        \n",
    "        # TODO: figure out what shape of self.conv4 is\n",
    "        # split on the 3rd dimension into 2 different parts\n",
    "        self.streamAC, self.streamVC = tf.split(3, 2, self.conv4)\n",
    "        \n",
    "        # flatten to [batch_size, k]\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        \n",
    "        # weights for advantage and value stream layer\n",
    "        self.AW = tf.Variable(tf.random_normal([665600, env.action_space.n]))\n",
    "        self.VW = tf.Variable(tf.random_normal([665600, 1]))\n",
    "        \n",
    "        # output of advantage and value layer\n",
    "        self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "        self.Value = tf.matmul(self.streamV, self.VW)\n",
    "        \n",
    "        # Then combine them together to get our final Q-values.\n",
    "        # Q(s, a) = V(s) + A(a)\n",
    "        self.Qout = self.Value + tf.sub(\n",
    "            self.Advantage,\n",
    "            tf.reduce_mean( # TODO: understand how this A(a) is calculated\n",
    "                self.Advantage,\n",
    "                reduction_indices=1,\n",
    "                keep_dims=True))\n",
    "        \n",
    "        # index of max value across 1st dimension\n",
    "        self.predict = tf.argmax(self.Qout, 1)\n",
    "        \n",
    "        # Below we obtain the loss by taking the sum of squares difference between\n",
    "        # the target and prediction Q values.\n",
    "        \n",
    "        # target Q value\n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        \n",
    "        # possible actions\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, env.action_space.n, dtype=tf.float32)\n",
    "        \n",
    "        # predicted Q values\n",
    "        self.Q = tf.reduce_sum(tf.mul(self.Qout, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        # error = sum( (target - actual)^2 ) / batch_size\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        \n",
    "        # define trainer\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        \n",
    "        # define optimizer\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    \"\"\"Used to store experiences and samples randomly to train the network.\"\"\"\n",
    "    def __init__(self, buffer_size=500):\n",
    "        self.buffer_size = buffer_size\n",
    "        # state, action, reward, state1, done\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.states_ = []\n",
    "        self.dones = []\n",
    "        \n",
    "    def add(self, states, actions, rewards, states_, dones):\n",
    "        if len(self.states) + len(states) >= self.buffer_size:\n",
    "            self.states = self.states[:(self.buffer_size - len(states))]\n",
    "            self.actions = self.actions[:(self.buffer_size - len(actions))]\n",
    "            self.rewards = self.rewards[:(self.buffer_size - len(rewards))]\n",
    "            self.states_ = self.states_[:(self.buffer_size - len(states_))]\n",
    "            self.dones = self.dones[:(self.buffer_size - len(dones))]\n",
    "\n",
    "        self.states.extend(states)\n",
    "        self.actions.extend(actions)\n",
    "        self.rewards.extend(rewards)\n",
    "        self.states_.extend(states_)\n",
    "        self.dones.extend(dones)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        samples = random.sample(range(len(self.actions)), size)\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        states_ = []\n",
    "        dones = []\n",
    "        for i in samples:\n",
    "            states.append(self.states[i])\n",
    "            actions.append(self.actions[i])\n",
    "            rewards.append(self.rewards[i])\n",
    "            states_.append(self.states_[i])\n",
    "            dones.append(self.dones[i])\n",
    "                \n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTarget(tfVars, sess):\n",
    "    \"\"\"Updates the parameters of our target network with those of the primary network.\"\"\"\n",
    "    total_vars = len(tfVars)\n",
    "    for idx, var in enumerate(tfVars[:int(total_vars / 2)]):\n",
    "        sess.run(tfVars[int(idx + total_vars / 2)].assign(var.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\"Take 1D float array of rewards and compute discounted reward.\n",
    "    \n",
    "    Discounts rewards for a given episode.\n",
    "    This is the Monte-Carlo method since we apply it to all rewards\n",
    "    in a given episode.\n",
    "    \n",
    "    Provides more robust reward signal to DQN.\n",
    "    \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of experiences to use for each training step\n",
    "batch_size = 5\n",
    "\n",
    "# how often to execute training step\n",
    "update_freq = 1\n",
    "\n",
    "# discount factor on target Q-values\n",
    "y = 0.99\n",
    "\n",
    "# starting chance of random action\n",
    "startE = 1\n",
    "\n",
    "# final chance of random action\n",
    "endE = 0.1\n",
    "\n",
    "# how many training steps required to fully reduce startE to endE\n",
    "anneling_steps = 1000\n",
    "\n",
    "# number of episodes of env to train network with\n",
    "num_episodes = 10000\n",
    "\n",
    "# number of random actions before training begins\n",
    "pre_train_steps = 500\n",
    "\n",
    "# load saved model?\n",
    "load_model = False\n",
    "\n",
    "# path to save model to\n",
    "path = \"./dqn\"\n",
    "\n",
    "# size of final convolutional layer before\n",
    "# splitting it into Advantage and Value streams\n",
    "h_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get frame shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.render()\n",
    "pixels = env.viewer.get_array()\n",
    "frameShape = pixels.shape\n",
    "env.viewer.close()\n",
    "env.viewer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "0 9 9.0 1\n",
      "10 224 17.0 1\n",
      "Saved Model\n",
      "Reward of last episode: 15.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size, frameShape, batch_size)\n",
    "targetQN = Qnetwork(h_size, frameShape, batch_size)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "# set rate of random action decrease\n",
    "e = startE\n",
    "stepDrop = (startE - endE) / anneling_steps\n",
    "\n",
    "# create lists to contain total rewards and steps per episode\n",
    "movesList = []\n",
    "rewardList = []\n",
    "total_steps = 0\n",
    "\n",
    "# make path for model to be saved in\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print(\"Loading Model...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "\n",
    "    # set target network to be equal to primary network\n",
    "    updateTarget(trainables, sess)\n",
    "\n",
    "    try:\n",
    "        for i in range(num_episodes):\n",
    "            episodeBuffer = experience_buffer()\n",
    "\n",
    "            # reset environment and get first new observation\n",
    "            env.reset()\n",
    "            state = env.render(mode='rgb_array')\n",
    "            done = False\n",
    "            rewardAll = 0\n",
    "            numMoves = 0\n",
    "\n",
    "            # the Q-Network\n",
    "            while numMoves < 200:\n",
    "                numMoves += 1\n",
    "\n",
    "                # choose action with probability e of being a random action\n",
    "                if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                    action = np.random.randint(0, 2) # 2 = num different actions\n",
    "                else:\n",
    "                    # select [0] position because returns in the form [action]\n",
    "                    action = sess.run(mainQN.predict, feed_dict={mainQN.rgb_array: [state]})[0]\n",
    "\n",
    "                observation, reward, done, _ = env.step(action)\n",
    "                state1 = env.render(mode='rgb_array')\n",
    "                total_steps += 1\n",
    "\n",
    "                # save experience to episode buffer\n",
    "                episodeBuffer.add([state], [action], [reward], [state1], [done])\n",
    "\n",
    "                if total_steps > pre_train_steps:\n",
    "                    if e > endE:\n",
    "                        e -= stepDrop\n",
    "\n",
    "                    if total_steps % (update_freq * 1000) == 0:\n",
    "                        print(\"Target network updated.\")\n",
    "                        updateTarget(trainables, sess)\n",
    "\n",
    "                    if total_steps % (update_freq * 100) == 0:\n",
    "                        # random sample of experiences\n",
    "                        states_t, actions_t, rewards_t, state1_t, dones_t = myBuffer.sample(batch_size)\n",
    "\n",
    "                        # Double-DQN update to the target Q-values\n",
    "                        Q1 = sess.run(mainQN.predict, feed_dict={\n",
    "                                mainQN.rgb_array: state1_t})\n",
    "\n",
    "                        Q2 = sess.run(targetQN.Qout, feed_dict={\n",
    "                                targetQN.rgb_array: state1_t})\n",
    "                        \n",
    "                        # If resulting state is DONE, Q-Target = r\n",
    "                        # If True: 0. If False: 1.\n",
    "                        end_multiplier = -(np.array(dones_t) - 1)\n",
    "\n",
    "                        # The Q values for predicted actions\n",
    "                        doubleQ = np.array([Q2[i, j] for i, j in zip(range(len(Q1)), Q1)])\n",
    "                        \n",
    "                        targetQ = np.array(rewards_t) + (y * doubleQ * end_multiplier)\n",
    "                        \n",
    "                        # update network with target values\n",
    "                        _  = sess.run(mainQN.updateModel,\n",
    "                                     feed_dict={mainQN.rgb_array: np.array(states_t),\n",
    "                                               mainQN.targetQ: targetQ,\n",
    "                                               mainQN.actions: actions_t})\n",
    "\n",
    "                rewardAll += reward\n",
    "                state = state1\n",
    "\n",
    "                if done == True:\n",
    "                    break\n",
    "\n",
    "            # get all experiences from this episode\n",
    "            episodeRewards = np.array(episodeBuffer.rewards)\n",
    "\n",
    "            # discount all rewards\n",
    "            discountRewards = discount_rewards(episodeRewards)\n",
    "            episodeBuffer.rewards = discountRewards\n",
    "\n",
    "            # add discounted experiences to our experience buffer\n",
    "            # state, action, reward, state1, done\n",
    "            myBuffer.add(episodeBuffer.states,\n",
    "                         episodeBuffer.actions,\n",
    "                         episodeBuffer.rewards,\n",
    "                         episodeBuffer.states_,\n",
    "                         episodeBuffer.dones)\n",
    "\n",
    "            movesList.append(numMoves)\n",
    "            rewardList.append(rewardAll)\n",
    "\n",
    "            # periodically save model\n",
    "            if i % 1000 == 0:\n",
    "                saver.save(sess, path+'/model-'+str(i)+'.cptk')\n",
    "                print(\"Saved Model\")\n",
    "            if i % 10 == 0:\n",
    "                print(i, total_steps, rewardAll, e)\n",
    "\n",
    "        saver.save(sess, path+'/model-'+str(i)+'.cptk')\n",
    "    except:\n",
    "        # if frames are still rendering, stop it\n",
    "        if env.viewer is not None:\n",
    "            env.viewer.close()\n",
    "            env.viewer = None\n",
    "        \n",
    "        saver.save(sess, path+'/model-'+str(i)+'.cptk')\n",
    "        print(\"Saved Model\")\n",
    "\n",
    "print(\"Reward of last episode: \" + str(rewardList[len(rewardList)-1]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
